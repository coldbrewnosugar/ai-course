<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Context Mode: Slash Your MCP Context Burn by 98% — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(59,130,246,0.25);
  background: rgba(59,130,246,0.04);
}
.callout-warning {
  border-color: rgba(239,68,68,0.4);
  background: rgba(239,68,68,0.04);
}
.callout-api-key-note {
  border-color: rgba(234,179,8,0.3);
  background: rgba(234,179,8,0.05);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--blue); }
.callout-warning .callout-label { color: var(--red); }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: var(--green);
  background: rgba(34,197,94,0.06);
  color: #15803D;
}
.decision-feedback.incorrect {
  border-color: var(--red);
  background: rgba(239,68,68,0.05);
  color: #DC2626;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(34,197,94,0.1);
  border-color: var(--green);
  color: var(--green);
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(239,68,68,0.08);
  border-color: var(--red);
  color: var(--red);
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--surface);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-general">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Your AI Agent Has a Memory Leak</h1>
      <div class="hero-subtitle">Every MCP tool call silently eats your context window. After 30 minutes, 40% is gone. Let&#x27;s fix that.</div>
      <div class="hero-meta">
        <span>35 min</span>
        <span> <span class="tag">MCP</span> <span class="tag">context-window</span> <span class="tag">claude-code</span> <span class="tag">developer-tools</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Here&#x27;s what dropped today: a team published <a href="https://mksg.lu/blog/context-mode">Context Mode</a>, an MCP server that sits between Claude Code and your tool outputs, compressing them by up to 98%. It hit 185 points on Hacker News with 46 comments, and the numbers are honestly wild — a single Playwright snapshot goes from 56 KB down to 299 <em>bytes</em>.</p>

<p>But the real story isn&#x27;t about one tool. It&#x27;s about a fundamental tension in how AI agents work right now: <strong>every time your agent uses a tool, the raw output gets dumped straight into the context window.</strong> That&#x27;s like copying an entire book into your notepad just to remember one paragraph. MCP made tool use standardized and beautiful, but nobody optimized the return trip.</p>

<p>Cloudflare already showed you can compress tool <em>definitions</em> by 99.9% (that&#x27;s the outbound side). Context Mode tackles the inbound side — the data coming back. And it does it with two surprisingly elegant ideas: sandboxed execution and a local search index. Let&#x27;s dig in.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Understand the Problem: Where Your Tokens Actually Go</h2>
      </div>
      <div class="step-body">
        <p>Before we install anything, let&#x27;s make sure we actually <em>feel</em> this problem. Most people have no idea how fast their context window drains.</p>

<p>Think of your context window like a whiteboard in a meeting room. You&#x27;ve got 200K tokens of space — sounds huge, right? But here&#x27;s the catch: every tool your agent has access to needs its definition written on the board <em>before the meeting starts</em>. With 81+ MCP tools active, that&#x27;s 143K tokens consumed before you even say hello. You&#x27;re starting the meeting with 72% of the whiteboard already full.</p>

<p>Then your agent starts <em>using</em> those tools. Each response gets written on the whiteboard too. A GitHub issue list? 59 KB scribbled across the board. A log file? 45 KB. A Playwright snapshot of a webpage? 56 KB — that&#x27;s like taping a poster over the remaining space.</p>

<p>After 30 minutes, you&#x27;ve got maybe 10% of the whiteboard left. Your agent starts forgetting earlier instructions, losing context, and giving worse answers. Not because it got dumber — because it literally ran out of room to think.</p>

<p>Let&#x27;s get your agent to help you see this concretely.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to calculate how fast a typical Claude Code session burns through context with common MCP tools
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s the total context window size, and how much do tool definitions consume upfront?</li><li>If each tool call returns 40-60 KB on average, how many calls until you&#x27;re out?</li><li>What does &#x27;session degradation&#x27; actually look like to the user?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>Your agent should walk you through the math: 200K tokens total, minus ~143K for tool definitions = ~57K remaining. At ~50 KB per tool response, you&#x27;re looking at maybe 1-2 raw tool calls before things get tight. It should explain that the agent doesn&#x27;t crash — it just starts dropping earlier context, which means it forgets your instructions, repeats questions, and gives inconsistent answers. Think of it like a goldfish whose bowl keeps shrinking.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Token counts aren&#x27;t the same as byte counts — a token is roughly ¾ of a word, or about 4 characters. But for back-of-napkin math, the proportions are what matter.
      </div>
        
      <details class="reveal">
        <summary>Why don&#x27;t MCP servers just send less data?</summary>
        <div class="reveal-body"><p>Great question! Most MCP servers are designed to be <strong>general-purpose</strong> — they don&#x27;t know what you&#x27;ll need from the data, so they send everything. A GitHub MCP server returns the full issue body, all labels, all comments, timestamps, user objects... because <em>some</em> user <em>might</em> need any of those fields. The compression has to happen at a layer above the individual tool — which is exactly where Context Mode sits.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Install Context Mode and See the Difference</h2>
      </div>
      <div class="step-body">
        <p>Alright, let&#x27;s actually set this up. Context Mode can be installed two ways: as a full plugin (with auto-routing hooks and slash commands) or as a standalone MCP server. We&#x27;ll go with MCP-only since it&#x27;s simpler and shows us exactly what&#x27;s happening.</p>

<p>The install is refreshingly boring — which is exactly what you want from infrastructure tooling. No build steps, no config files to hand-edit, no dependency nightmares.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to walk you through installing Context Mode as an MCP server in Claude Code
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>Where does Claude Code store its MCP server configuration?</li><li>What&#x27;s the difference between the plugin install and the MCP-only install?</li><li>How will you verify it&#x27;s actually running after installation?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should tell you to install the npm package and add it to your Claude Code MCP config. The key command is just <code>npx context-mode</code> as the MCP server command in your settings. After restarting Claude Code, you should see new tools available: <code>execute</code>, <code>index</code>, <code>search</code>, <code>fetch_and_index</code>, and <code>batch_execute</code>. The agent might also mention the plugin marketplace as an alternative one-click install.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        After adding the MCP server config, you MUST restart Claude Code for it to pick up the new tools. This trips up almost everyone the first time.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Context Mode auto-detects Bun for 3-5x faster JavaScript/TypeScript execution. If you have Bun installed, you get the speed boost for free.
      </div>
        
      <details class="reveal">
        <summary>What does credential passthrough mean?</summary>
        <div class="reveal-body"><p>When Context Mode spawns a sandbox to run your code, it inherits your environment variables and config paths. So if you&#x27;re logged into <code>gh</code> (GitHub CLI), <code>aws</code>, <code>gcloud</code>, or <code>kubectl</code>, the sandbox can use those credentials too — without ever exposing the actual tokens or keys to the conversation context. The credentials stay in the process environment, never written to the whiteboard. This is critical for real-world usage: you want to query your prod Kubernetes cluster without your API key becoming part of the chat history.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point, you should have Context Mode installed as an MCP server in Claude Code, and after a restart, you should see its tools (execute, index, search, etc.) available. If you don&#x27;t see them, check your MCP config path and restart again.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>The Sandbox: How Raw Data Stays Off the Whiteboard</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s where Context Mode gets clever. Remember our whiteboard analogy? Instead of scribbling raw tool output directly onto the whiteboard, Context Mode does something different: it hands the data to an assistant in a <strong>separate room</strong> (the sandbox), who processes it and slides a sticky note with just the key findings back under the door.</p>

<p>Each <code>execute</code> call spawns an isolated subprocess — its own little bubble with its own process boundary. Your script runs inside that bubble, crunches whatever data it needs, and only what it <code>print</code>s to stdout comes back into the conversation. The raw data — the 56 KB Playwright snapshot, the 59 KB of GitHub issues — never leaves the sandbox.</p>

<p>Ten language runtimes are supported: JavaScript, TypeScript, Python, Shell, Ruby, Go, Rust, PHP, Perl, and R. So you&#x27;re not locked into one ecosystem.</p>

<p>Let&#x27;s see this in action with a concrete example.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to demonstrate Context Mode&#x27;s sandbox by fetching 20 GitHub issues from any popular repo and comparing the raw output size vs. the sandboxed output size
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What repo would give us meaty issues with lots of text? (Think popular open-source projects)</li><li>What information do you actually NEED from 20 issues vs. what `gh issue list` dumps?</li><li>How would you measure the &#x27;before and after&#x27; to see the compression?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should use Context Mode&#x27;s <code>execute</code> tool to run a shell script that calls <code>gh issue list</code> inside the sandbox, then extracts just the issue numbers, titles, labels, and maybe dates — printing a compact summary to stdout. The raw <code>gh</code> output would be ~59 KB, but the sandbox output should be around 1.1 KB. The key insight: the agent wrote a tiny script that runs inside the sandbox, and only the printed summary enters the context. The full API response with all those nested JSON objects, user avatars, and timeline events stayed in the sandbox and got garbage collected.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The sandbox isn&#x27;t doing magic summarization — it&#x27;s running YOUR code (or code the agent writes) that extracts what matters. The intelligence is in deciding what to keep, and that decision is made by the agent, not by a compression algorithm.
      </div>
        
      <details class="reveal">
        <summary>How is this different from just asking the agent to summarize?</summary>
        <div class="reveal-body"><p>If you ask the agent to summarize raw output, the damage is already done — the full 59 KB is already in your context window. The agent reads it, summarizes it, and now you have 59 KB of raw data <em>plus</em> the summary taking up space. The raw data doesn&#x27;t disappear from context just because the agent summarized it.</p>

<p>The sandbox approach is fundamentally different: the raw data <strong>never enters the context at all</strong>. It exists only inside an ephemeral subprocess. The subprocess does the filtering/extraction, and only the compact result (the stdout) makes it into the conversation. It&#x27;s the difference between photocopying an entire book into your notebook and then highlighting the good parts... vs. having someone read the book in another room and hand you just the highlights.</p></div>
      </details>
      <details class="reveal">
        <summary>What about scripts that need to share state?</summary>
        <div class="reveal-body"><p>Each sandbox is isolated — scripts can&#x27;t access each other&#x27;s memory. This is a feature, not a bug. It means one runaway script can&#x27;t corrupt another&#x27;s data. If you need to share results between steps, you pass data through the conversation context (the sticky notes) or use the knowledge base (which we&#x27;ll cover next). Think of it like microservices vs. a monolith — isolation keeps things clean.</p></div>
      </details>
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You have a 500-line access log file and need to find the top 10 IPs by request count. Which approach burns less context?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt0">
          <label for="decision_6_opt0">Use Context Mode&#x27;s execute tool to run an awk/Python script in the sandbox that counts IPs and prints only the top 10</label>
          <div class="decision-feedback correct">&#10003; Correct! The raw log never enters context — only the 10-line summary does. The article showed 45 KB → 155 bytes for exactly this kind of task. The processing happens entirely inside the sandbox.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt1">
          <label for="decision_6_opt1">Have the agent read the log file directly, then ask it to identify the top 10 IPs</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This dumps the entire 45 KB log into context first. Even though the agent CAN find the answer, you&#x27;ve burned 45 KB of your whiteboard space to get a result that could fit in 155 bytes. And that 45 KB stays in context for the rest of the session.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt2">
          <label for="decision_6_opt2">Pipe the log through grep and wc outside of Context Mode, then paste the result</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Better than reading the raw file, but any bash command output in Claude Code still enters the context window. Context Mode&#x27;s sandbox is the only path where the raw data truly stays out of the conversation.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>The Knowledge Base: Your Agent&#x27;s Personal Search Engine</h2>
      </div>
      <div class="step-body">
        <p>The sandbox handles ephemeral data — stuff you process once and move on. But what about reference material you need to come back to? Documentation, API specs, long guides?</p>

<p>Context Mode&#x27;s second trick is a <strong>local knowledge base</strong> built on SQLite FTS5 (Full-Text Search 5). Think of it like building your agent a personal search engine that lives on your machine.</p>

<p>Here&#x27;s the mental model: instead of pasting an entire API doc into the conversation (killing your context), you feed it into the knowledge base once. It gets chunked by headings — each section becomes a searchable unit, with code blocks kept intact. Then when you need something specific, your agent searches the index and pulls back just the relevant chunks.</p>

<p>The search uses <strong>BM25 ranking</strong> — this is the same algorithm that powered search engines before neural retrieval took over. It scores results based on three things: how often your search terms appear in a chunk (term frequency), how rare those terms are across all chunks (inverse document frequency), and how long the chunk is (shorter chunks with your terms get boosted). It also applies <strong>Porter stemming</strong>, so searching for &quot;running&quot; also matches &quot;runs&quot; and &quot;ran&quot;.</p>

<p>The key thing: you get back the <strong>actual indexed content</strong>, not a summary or approximation. Real code blocks, real text, with their heading hierarchy preserved.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to index a documentation page from a library you use frequently, then search it to answer a specific question
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What documentation do you find yourself re-reading most often? (React hooks? Express middleware? A specific API?)</li><li>What&#x27;s a specific question you&#x27;d normally Ctrl+F for in those docs?</li><li>How does `fetch_and_index` differ from just reading the page?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should use <code>fetch_and_index</code> with a URL to pull a docs page, convert it to markdown, chunk it, and store it in the FTS5 index. Then when you search, it returns just the relevant section(s) — maybe 200-500 bytes instead of the full page&#x27;s 50+ KB. The crucial detail: the raw HTML page never enters context. It goes straight from fetch → markdown → chunks → SQLite. Only your search results touch the conversation.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The knowledge base persists across your session. Index something once at the start, and your agent can search it throughout the entire conversation without re-fetching or re-reading.
      </div>
        
      <details class="reveal">
        <summary>Why BM25 instead of vector embeddings?</summary>
        <div class="reveal-body"><p>You might be thinking — wait, isn&#x27;t everyone using embeddings and vector databases for search now? Why is Context Mode using BM25, an algorithm from the &#x27;90s?</p>

<p>A few reasons:</p>

<ol>
<li><strong>Zero dependencies.</strong> BM25 with FTS5 runs in SQLite — no embedding model to download, no vector DB to spin up, no API calls to OpenAI. It&#x27;s instant and local.</li>
</ol>

<ol>
<li><strong>Exact matching matters here.</strong> When you&#x27;re searching docs for <code>useEffect cleanup</code> or <code>kubectl rollout</code>, you want exact term matching, not semantic similarity. BM25 is excellent at this.</li>
</ol>

<ol>
<li><strong>Speed.</strong> BM25 scoring is just arithmetic on precomputed indexes. Embedding-based search requires running a neural network. For a local dev tool, the speed difference matters.</li>
</ol>

<ol>
<li><strong>Transparency.</strong> You can reason about why BM25 returned a result (the terms matched!). Embedding similarity is harder to debug when it gives you weird results.</li>
</ol>

<p>That said, for truly semantic queries (&quot;how do I handle errors gracefully?&quot;), embeddings would win. It&#x27;s a trade-off, and for the code/docs use case, BM25 is the pragmatic choice.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now understand both core mechanisms: the **sandbox** (ephemeral data processing where raw output never enters context) and the **knowledge base** (persistent FTS5 search index for reference material). The sandbox is for data you process and discard. The knowledge base is for data you want to query repeatedly.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Stress Test: Does the 98% Claim Hold Up?</h2>
      </div>
      <div class="step-body">
        <p>Alright, the marketing numbers sound great — 315 KB → 5.4 KB, 98% reduction. But we&#x27;re the kind of people who check receipts. Let&#x27;s actually stress-test this with real workflows and see what happens.</p>

<p>The article validated across 11 scenarios. The standout numbers:</p>
<ul>
<li>Playwright snapshot: <strong>56 KB → 299 B</strong></li>
<li>20 GitHub issues: <strong>59 KB → 1.1 KB</strong></li>
<li>500-line access log: <strong>45 KB → 155 B</strong></li>
<li>500-row CSV analytics: <strong>85 KB → 222 B</strong></li>
<li>153 git commits: <strong>11.6 KB → 107 B</strong></li>
</ul>

<p>The most interesting one is the &quot;repo research&quot; scenario: <strong>986 KB → 62 KB</strong> using a subagent that makes 5 calls instead of 37. That&#x27;s still a 94% reduction, but notice it&#x27;s not 98% — because the subagent needs to return richer context to be useful.</p>

<p>Honestly, the individual numbers almost don&#x27;t matter. What matters is the <em>session-level</em> impact: going from ~30 minutes before degradation to ~3 hours. That&#x27;s the difference between an agent that forgets your project structure halfway through a task and one that stays coherent for an entire work session.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Pick a real task from your daily workflow — something that generates a lot of tool output — and run it with and without Context Mode to compare
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s the most data-heavy thing your agent does regularly? (Reading logs? Reviewing PRs? Analyzing test output?)</li><li>How would you measure the &#x27;before&#x27; and &#x27;after&#x27; fairly?</li><li>What would you look at besides raw size — does the compressed output still contain everything you need?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>This one&#x27;s personal to your workflow, but the agent should help you design a fair comparison: run the task once with raw tool output (note the context consumed), then again using Context Mode&#x27;s execute/batch_execute. You should see a dramatic reduction in context usage while still getting the information you need. The key validation: does the compressed output actually let you make the same decisions? If Context Mode strips something you needed, that&#x27;s a real problem — compression is only useful if it preserves the signal.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        Context Mode&#x27;s PreToolUse hook auto-routes outputs through the sandbox. This means you might see different behavior than expected if tools you rely on get intercepted. Keep an eye on whether any of your existing workflows break after installation.
      </div>
        
      <details class="reveal">
        <summary>What about the subagent upgrade?</summary>
        <div class="reveal-body"><p>One subtle but powerful detail: Context Mode upgrades Bash subagents to <code>general-purpose</code> subagents so they can access MCP tools, including <code>batch_execute</code>. This means when Claude Code spawns a subagent to research something, that subagent uses the sandbox too — compounding the context savings.</p>

<p>The article&#x27;s repo research example dropped from 37 tool calls to 5 and from 986 KB to 62 KB. The subagent was doing the same work — it was just doing it <em>smarter</em>, batching operations inside sandboxes instead of making individual raw calls. This is where the session-level impact really comes from: not just compressing individual outputs, but reducing the total number of context-consuming interactions.</p></div>
      </details>
      </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Build a prompt that gets your agent to create a &#x27;context budget dashboard&#x27; — a quick way to monitor how much context you&#x27;ve consumed in a session and what consumed it</div>
      <div class="your-turn-context">Now that you understand the problem (context drain) and the solution (sandbox + knowledge base), the natural next step is visibility. You can&#x27;t optimize what you can&#x27;t measure. Imagine a lightweight dashboard that shows you a breakdown: how many tokens went to tool definitions, how many to tool outputs, and how much headroom you have left.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What information would you need to calculate context usage? (Think: what goes IN to the context and how to track it)</li><li>How should the dashboard present this? A simple text summary? A bar chart? Percentages?</li><li>Should it run inside Context Mode&#x27;s sandbox to avoid... burning context to measure context? (Meta, right?)</li><li>What thresholds would trigger a warning? When should you worry?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>I want to build a context budget monitor for Claude Code sessions. Create a script that I can run inside Context Mode&#x27;s execute sandbox that: (1) estimates token usage based on conversation length, (2) breaks down usage into categories — tool definitions, tool outputs, user messages, and agent responses, (3) shows a simple ASCII bar chart of the breakdown, and (4) flags a warning if remaining context drops below 20%. The output should be compact — under 500 bytes — since the whole point is not burning context to measure context. Give me the script and explain how I&#x27;d integrate it as a periodic check during a session.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You need to reference a 200-page API specification throughout a long coding session. What&#x27;s the most context-efficient approach?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt0">
          <label for="decision_11_opt0">Use fetch_and_index to load the spec into the knowledge base, then search as needed</label>
          <div class="decision-feedback correct">&#10003; Correct! The full spec gets chunked and indexed in SQLite — never entering context. Each search returns only the relevant section (maybe 200-500 bytes). Over a session with 20 lookups, you might use 10 KB instead of loading 200+ KB upfront.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt1">
          <label for="decision_11_opt1">Use the sandbox to process the spec once and extract all the endpoints into a summary</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This works for a one-time extraction, but you lose the ability to go deep on specific endpoints later. The knowledge base lets you query different parts at different times, which is what you need for ongoing reference. The sandbox is better for ephemeral processing.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt2">
          <label for="decision_11_opt2">Paste the most important 20% of the spec directly into the conversation</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Even 20% of a 200-page spec is a LOT of context. And you&#x27;re guessing which 20% you&#x27;ll need. The knowledge base approach means you don&#x27;t have to predict — you search when the need arises, and only pay the context cost for what you actually use.</div>
        </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>Let&#x27;s zoom out. We started with a deceptively simple problem: MCP tool calls dump raw data into your AI agent&#x27;s context window, and after 30 minutes, your agent basically has amnesia.</p>

<p>Context Mode solves this with two complementary ideas:</p>

<p><strong>The Sandbox</strong> — a separate room where data gets processed. Raw output never enters the conversation. Your agent writes a little script, the sandbox runs it, and only the printed results (the sticky note) come back. This is perfect for ephemeral tasks: log analysis, API responses, test output, data crunching.</p>

<p><strong>The Knowledge Base</strong> — a local search engine powered by SQLite FTS5 and BM25 ranking. Feed in docs, specs, or any reference material once, then query it repeatedly throughout your session. Porter stemming means your searches are forgiving. Heading-aware chunking means you get properly scoped results with code blocks intact.</p>

<p>The practical impact? Sessions that used to degrade after 30 minutes now stay coherent for 3+ hours. Context remaining after 45 minutes: 99% instead of 60%.</p>

<p>But here&#x27;s the deeper insight: <strong>this is a pattern, not just a tool.</strong> The idea of &#x27;process data in isolation and only surface the results&#x27; applies everywhere in AI agent design. Whether you&#x27;re building your own MCP servers, designing agent workflows, or just prompting more effectively — thinking about what <em>needs</em> to be in context vs. what can stay outside of it is a superpower.</p></div>
      <ul class="takeaways-list"><li>Every MCP tool call has a hidden cost — raw output consumes context that your agent needs for reasoning. Awareness of this cost changes how you design agent workflows.</li><li>Sandboxed execution is the key pattern: process data in an isolated subprocess and only return the extracted signal to context. The raw data never touches the conversation.</li><li>A local FTS5 knowledge base with BM25 ranking gives you &#x27;search, don&#x27;t paste&#x27; semantics for reference material — no embedding model, no API calls, just fast local search with stemming.</li><li>The biggest wins are at the session level: not just compressing individual outputs, but reducing the total number of context-consuming interactions through batching and smarter subagent routing.</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Try Context Mode on your heaviest daily workflow — the one where your agent usually starts forgetting things after 30 minutes — and compare the before/after experience</li><li>Explore building your own MCP server that applies the sandbox pattern to a domain-specific tool you use frequently</li><li>Look into Cloudflare&#x27;s Code Mode for tool definition compression — combine it with Context Mode for optimization on both sides of the MCP pipeline</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://mksg.lu/blog/context-mode" target="_blank" rel="noopener">Stop Burning Your Context Window – How We Cut MCP Output by 98% in Claude Code</a> <span class="source-name">(Hacker News AI)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">VectifyAI/PageIndex</div>
            <div class="oa-summary">A RAG framework that retrieves document pages using reasoning instead of vector databases or chunking.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese tutorial teaching how to build AI agents from scratch, covering theory through practical implementation.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">ruvnet/wifi-densepose</div>
            <div class="oa-summary">Estimates human body poses through walls in real time using standard WiFi routers.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">4thfever/cultivation-world-simulator</div>
            <div class="oa-summary">AI agent-driven simulator that generates a dynamic Chinese fantasy (Xianxia) cultivation world.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">muratcankoylan/Agent-Skills-for-Context-Engineering</div>
            <div class="oa-summary">Collection of reusable agent skills for context engineering, multi-agent architectures, and production agent systems.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of agent skills that extend Claude&#x27;s capabilities with specialized tools.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">QwenLM/Qwen-Agent</div>
            <div class="oa-summary">Agent framework built on Qwen models with function calling, MCP, code interpreter, and RAG support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">NevaMind-AI/memU</div>
            <div class="oa-summary">Memory framework for always-on AI agents that need persistent, proactive recall across long-running sessions.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">modelscope/ms-swift</div>
            <div class="oa-summary">Toolkit for fine-tuning 600+ LLMs and 300+ multimodal models using PEFT, SFT, DPO, and GRPO.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">agentscope-ai/agentscope</div>
            <div class="oa-summary">Production-ready framework for building transparent, trustworthy AI agents you can inspect and understand.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "VectifyAI/PageIndex", "tags": "agents,tools,rag", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "ruvnet/wifi-densepose", "tags": "tools,rag", "source": "GitHub Trending Python"}, {"title": "4thfever/cultivation-world-simulator", "tags": "agents,open-source", "source": "GitHub Trending Python"}, {"title": "muratcankoylan/Agent-Skills-for-Context-Engineering", "tags": "agents,tools,technique", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "QwenLM/Qwen-Agent", "tags": "agents,open-source,tools,coding,rag", "source": "GitHub Trending Python"}, {"title": "NevaMind-AI/memU", "tags": "agents,tools", "source": "GitHub Trending Python"}, {"title": "modelscope/ms-swift", "tags": "tools,fine-tuning", "source": "GitHub Trending Python"}, {"title": "agentscope-ai/agentscope", "tags": "agents,tools,technique,audio,rag", "source": "GitHub Trending Python"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "general";
      var date = "2026-02-28";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>