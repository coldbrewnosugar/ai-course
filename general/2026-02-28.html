<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Build an LLM Prompt Energy-Cost Estimator ‚Äî Tinker
  <style>
@import url('https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=DM+Sans:wght@300;400;500;600;700&display=swap');
:root {
  --bg: #ffffff;
  --ink: #0a0a0a;
  --ink-secondary: #333;
  --muted: #888;
  --red: #E63226;
  --blue: #1B3F8B;
  --yellow: #F5B731;
  --green: #1a8754;
  --light-gray: #f6f6f6;
  --border-gray: #e0e0e0;
  --mono: 'Space Mono', monospace;
  --sans: 'DM Sans', system-ui, sans-serif;
  --border: 2px solid var(--ink);
  --max-w: 720px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
}

/* ‚îÄ‚îÄ Layout ‚îÄ‚îÄ */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 4rem;
}

/* ‚îÄ‚îÄ Back link ‚îÄ‚îÄ */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.4rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  text-decoration: none;
  padding: 1.5rem 0 1rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--ink); }

/* ‚îÄ‚îÄ Hero ‚îÄ‚îÄ */
.session-hero {
  border-bottom: 3px solid var(--ink);
  padding: 2.5rem 0 2rem;
  margin-bottom: 2.5rem;
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: #fff;
  background: var(--red);
  padding: 0.25rem 0.75rem;
  margin-bottom: 1rem;
}
.session-hero h1 {
  font-family: var(--mono);
  font-size: 2rem;
  font-weight: 700;
  line-height: 1.15;
  letter-spacing: -0.02em;
  margin-bottom: 0.5rem;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.5rem;
  margin-top: 1rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.04em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--light-gray);
  border: 1px solid var(--border-gray);
  padding: 0.15rem 0.5rem;
  font-size: 0.65rem;
}

/* ‚îÄ‚îÄ Section divider ‚îÄ‚îÄ */
.section-divider {
  border: none;
  border-top: 3px solid var(--ink);
  margin: 3rem 0;
}

/* ‚îÄ‚îÄ Context block ‚îÄ‚îÄ */
.context-block {
  background: var(--light-gray);
  border-left: 4px solid var(--ink);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ‚îÄ‚îÄ Steps ‚îÄ‚îÄ */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.5rem;
}
.step-number {
  flex-shrink: 0;
  width: 48px; height: 48px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 1.2rem;
  font-weight: 700;
  display: flex;
  align-items: center;
  justify-content: center;
}
.step-header h2 {
  font-family: var(--mono);
  font-size: 1.25rem;
  font-weight: 700;
  line-height: 1.2;
  padding-top: 0.3rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.3rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--blue); }

/* ‚îÄ‚îÄ Code blocks ‚îÄ‚îÄ */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--light-gray);
  border: 1px solid var(--border-gray);
  border-left: 4px solid var(--blue);
}
.code-caption {
  display: block;
  padding: 0.5rem 1rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  color: var(--muted);
  border-bottom: 1px solid var(--border-gray);
  letter-spacing: 0.04em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.85rem;
  line-height: 1.5;
  color: var(--ink);
}
.copy-btn {
  position: absolute;
  top: 0.4rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 700;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  background: var(--ink);
  color: #fff;
  border: none;
  padding: 0.3rem 0.6rem;
  cursor: pointer;
  transition: background 0.15s;
}
.copy-btn:hover { background: var(--blue); }
.copy-btn.copied { background: var(--green); }

/* ‚îÄ‚îÄ Callouts ‚îÄ‚îÄ */
.callout {
  border-left: 4px solid;
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.95rem;
}
.callout-tip {
  border-color: var(--blue);
  background: rgba(27,63,139,0.05);
}
.callout-warning {
  border-color: var(--red);
  background: rgba(230,50,38,0.05);
}
.callout-api-key-note {
  border-color: var(--yellow);
  background: rgba(245,183,49,0.1);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.3rem;
}
.callout-tip .callout-label { color: var(--blue); }
.callout-warning .callout-label { color: var(--red); }
.callout-api-key-note .callout-label { color: #b8860b; }

/* ‚îÄ‚îÄ Reveals (details/summary) ‚îÄ‚îÄ */
.reveal {
  margin: 1rem 0;
  border: 1px solid var(--border-gray);
}
.reveal summary {
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--light-gray);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: #eee; }
.reveal summary::before {
  content: "\25B6";
  font-size: 0.6rem;
  transition: transform 0.2s;
}
.reveal[open] summary::before {
  transform: rotate(90deg);
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border-gray);
  font-size: 0.95rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ‚îÄ‚îÄ Checkpoint ‚îÄ‚îÄ */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 1rem;
  padding: 1.25rem 1.5rem;
  background: var(--ink);
  color: #fff;
  margin: 2rem 0;
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
  letter-spacing: 0.02em;
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 32px; height: 32px;
  border: 2px solid #fff;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 1rem;
}

/* ‚îÄ‚îÄ Decision point ‚îÄ‚îÄ */
.decision-point {
  border: 2px solid var(--ink);
  margin: 2rem 0;
  padding: 1.5rem;
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
}
.decision-option {
  margin-bottom: 0.75rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.75rem 1rem;
  border: 1px solid var(--border-gray);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
}
.decision-option label:hover {
  border-color: var(--ink);
  background: var(--light-gray);
}
.decision-option input:checked + label {
  border-color: var(--ink);
  border-width: 2px;
  background: var(--light-gray);
}
.decision-feedback {
  display: none;
  padding: 0.75rem 1rem;
  margin-top: 0.25rem;
  font-size: 0.9rem;
  border-left: 3px solid;
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: var(--green);
  background: rgba(26,135,84,0.05);
  color: var(--green);
}
.decision-feedback.incorrect {
  border-color: var(--red);
  background: rgba(230,50,38,0.05);
  color: var(--red);
}

/* ‚îÄ‚îÄ Agent interaction ‚îÄ‚îÄ */
.agent-interaction {
  margin: 1.5rem 0;
  border: 2px solid var(--ink);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
}
.agent-goal-label {
  font-size: 0.6rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: rgba(255,255,255,0.6);
  margin-bottom: 0.3rem;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: rgba(27,63,139,0.04);
  border-bottom: 1px solid var(--border-gray);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--blue);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
  font-size: 0.95rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "?";
  position: absolute;
  left: 0;
  color: var(--blue);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* ‚îÄ‚îÄ Your turn ‚îÄ‚îÄ */
.your-turn {
  border: 2px solid var(--blue);
  padding: 1.5rem;
  margin: 2rem 0;
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--blue);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
}
.your-turn .your-turn-context {
  font-size: 0.95rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ‚îÄ‚îÄ Recap ‚îÄ‚îÄ */
.recap-section {
  border-top: 3px solid var(--ink);
  padding-top: 2.5rem;
  margin-top: 3rem;
}
.recap-section h2 {
  font-family: var(--mono);
  font-size: 1.25rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.5rem 0 0.5rem 1.5rem;
  position: relative;
  border-bottom: 1px solid var(--border-gray);
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--blue);
  font-weight: 700;
}

/* ‚îÄ‚îÄ Sources ‚îÄ‚îÄ */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border-gray);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--blue);
  text-decoration: none;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration: underline; }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  margin-left: 0.5rem;
}

/* ‚îÄ‚îÄ Footer ‚îÄ‚îÄ */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-top: 3rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 3px solid var(--ink);
}
.session-footer span { color: var(--ink); font-weight: 700; }

/* ‚îÄ‚îÄ Responsive ‚îÄ‚îÄ */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.5rem; }
  .step-number { width: 36px; height: 36px; font-size: 1rem; }
  .session-container { padding: 0 1rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
}
</style>
</head>
<body>
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>What&#x27;s the Real Cost of &quot;Hey ChatGPT&quot;?</h1>
      <div class="hero-subtitle">We&#x27;re building a CLI that turns token counts into watts, water, and carbon ‚Äî so you can see the invisible footprint of every prompt you send.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">sustainability</span> <span class="tag">LLM-ops</span> <span class="tag">python</span> <span class="tag">CLI</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>MIT Technology Review just got nominated for a National Magazine Award for their <em>Power Hungry</em> investigation into AI&#x27;s energy footprint. The TL;DR? AI companies have kept energy numbers under wraps for years, and when journalists finally did the math, the results were eye-opening.</p>

<p>A single ChatGPT query burns roughly <strong>10x the energy</strong> of a Google search. Scale that to hundreds of millions of daily users and you&#x27;re talking about power consumption rivaling small countries. After the investigation dropped, OpenAI, Mistral, and Google actually started publishing energy and water figures for their models ‚Äî which tells you how much pressure good reporting can create.</p>

<p>But here&#x27;s what&#x27;s cool for us: the math behind estimating a prompt&#x27;s energy cost isn&#x27;t magic. It&#x27;s GPU power draw √ó time √ó overhead factors. Today we&#x27;re going to build a tool that does exactly that calculation ‚Äî and we&#x27;re going to let an AI agent write most of it for us. Meta, right? Using AI to build a tool that measures AI&#x27;s environmental cost.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>The Core Energy Model ‚Äî From Tokens to Watts</h2>
      </div>
      <div class="step-body">
        <p>Before we touch any code, let&#x27;s get the mental model straight. Every token an LLM processes requires floating-point operations (FLOPs). Those FLOPs run on GPUs that have a known power draw (TDP ‚Äî thermal design power). If we know the model size, we can estimate FLOPs per token, and from there estimate energy.</p>

<p>The formula chain looks like this:</p>

<p><strong>Tokens ‚Üí FLOPs ‚Üí GPU-seconds ‚Üí Watt-hours</strong></p>

<p>A rough rule of thumb: for a forward pass, FLOPs per token ‚âà 2 √ó model parameters. An H100 GPU does about 990 TFLOPS at FP16. So a 70B parameter model needs ~140 billion FLOPs per token, which takes a fraction of a millisecond on modern hardware.</p>

<p>Let&#x27;s get our agent to build the foundational data structures and energy calculation.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to create a Python module with GPU specs, model profiles, and a function that estimates watt-hours from a token count and model name.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What data does the function need? Think: model size (parameters), which GPU runs it, and how efficiently that GPU is utilized...</li><li>GPUs have a theoretical max throughput (TFLOPS) but real utilization is usually 30-60%. How should we model that?</li><li>Should GPU specs and model profiles be hardcoded or configurable? What&#x27;s easier to extend later?</li><li>What&#x27;s the output unit? Watt-hours makes sense for a single prompt ‚Äî why not kilowatt-hours?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>See what the agent gives back</summary>
        <div class="reveal-body">
      <div class="code-block">
        <span class="code-caption">Your agent should give back something like this ‚Äî a clean module with dataclasses for GPU/model profiles and an energy estimation function that shows its math.</span>
        <button class="copy-btn">COPY</button>
        <pre><code class="language-python">&quot;&quot;&quot;energy_model.py ‚Äî core energy estimation from tokens to watt-hours.&quot;&quot;&quot;
from dataclasses import dataclass


@dataclass
class GPUProfile:
    name: str
    tdp_watts: float            # thermal design power
    fp16_tflops: float          # peak FP16 throughput
    typical_utilization: float  # real-world utilization factor (0-1)


@dataclass
class ModelProfile:
    name: str
    params_billion: float
    gpu: GPUProfile
    num_gpus: int = 1  # how many GPUs serve this model


# --- GPU catalog ---
GPUS = {
    &quot;H100&quot;: GPUProfile(&quot;NVIDIA H100 SXM&quot;, tdp_watts=700, fp16_tflops=990, typical_utilization=0.40),
    &quot;A100&quot;: GPUProfile(&quot;NVIDIA A100 80GB&quot;, tdp_watts=400, fp16_tflops=312, typical_utilization=0.35),
    &quot;H200&quot;: GPUProfile(&quot;NVIDIA H200&quot;,      tdp_watts=700, fp16_tflops=990, typical_utilization=0.42),
}

# --- Model catalog (best-guess public configurations) ---
MODELS = {
    &quot;gpt-4o&quot;:        ModelProfile(&quot;GPT-4o&quot;,        params_billion=200, gpu=GPUS[&quot;H100&quot;], num_gpus=8),
    &quot;gpt-4o-mini&quot;:   ModelProfile(&quot;GPT-4o mini&quot;,   params_billion=8,   gpu=GPUS[&quot;H100&quot;], num_gpus=1),
    &quot;claude-sonnet&quot;: ModelProfile(&quot;Claude Sonnet&quot;,  params_billion=70,  gpu=GPUS[&quot;H100&quot;], num_gpus=4),
    &quot;llama-70b&quot;:     ModelProfile(&quot;Llama 3 70B&quot;,    params_billion=70,  gpu=GPUS[&quot;A100&quot;], num_gpus=4),
    &quot;llama-8b&quot;:      ModelProfile(&quot;Llama 3 8B&quot;,     params_billion=8,   gpu=GPUS[&quot;A100&quot;], num_gpus=1),
    &quot;mistral-large&quot;: ModelProfile(&quot;Mistral Large&quot;,  params_billion=123, gpu=GPUS[&quot;H100&quot;], num_gpus=4),
}


def estimate_energy_wh(
    model_name: str,
    prompt_tokens: int,
    completion_tokens: int,
    pue: float = 1.2,  # power usage effectiveness (data-center overhead)
) -&gt; dict:
    &quot;&quot;&quot;Estimate energy in watt-hours for a single LLM API call.

    Returns a dict with intermediate values so callers can inspect the math.
    &quot;&quot;&quot;
    model = MODELS[model_name]
    gpu = model.gpu
    total_tokens = prompt_tokens + completion_tokens

    # Step 1: FLOPs estimate  (forward pass ‚âà 2 * params per token)
    flops_per_token = 2 * model.params_billion * 1e9
    total_flops = flops_per_token * total_tokens

    # Step 2: GPU-seconds  (using effective throughput)
    effective_tflops = gpu.fp16_tflops * gpu.typical_utilization
    effective_flops_per_sec = effective_tflops * 1e12
    # Spread across all GPUs serving the model
    gpu_seconds = total_flops / (effective_flops_per_sec * model.num_gpus)

    # Step 3: Energy  (all GPUs drawing power for that duration)
    total_gpu_watts = gpu.tdp_watts * model.num_gpus
    energy_joules = total_gpu_watts * gpu_seconds
    energy_wh = (energy_joules / 3600) * pue  # apply data-center overhead

    return {
        &quot;model&quot;: model.name,
        &quot;total_tokens&quot;: total_tokens,
        &quot;total_flops&quot;: total_flops,
        &quot;gpu_seconds&quot;: round(gpu_seconds, 6),
        &quot;energy_wh&quot;: round(energy_wh, 6),
        &quot;pue&quot;: pue,
    }</code></pre>
      </div></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The parameter counts for proprietary models (GPT-4o, Claude) are estimates based on public reporting and leaks. That&#x27;s fine ‚Äî we&#x27;re building an <em>estimator</em>, not an audit tool. The point is order-of-magnitude awareness.
      </div>
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        Don&#x27;t confuse training energy with inference energy. Training GPT-4 reportedly consumed ~50 GWh. But inference ‚Äî the per-prompt cost ‚Äî is what matters for day-to-day impact. Our tool focuses on inference.
      </div>
        
      <details class="reveal">
        <summary>Why 2√ó parameters for FLOPs per token?</summary>
        <div class="reveal-body"><p>In a transformer forward pass, each parameter participates in roughly one multiply and one add per token ‚Äî that&#x27;s 2 floating-point operations per parameter. For a 70B model, that&#x27;s ~140 billion FLOPs per token. This is a simplification (attention is slightly more complex), but it&#x27;s the standard back-of-envelope estimate used by researchers at DeepMind, Meta, and Epoch AI.</p></div>
      </details>
      <details class="reveal">
        <summary>What&#x27;s PUE and why 1.2?</summary>
        <div class="reveal-body"><p>Power Usage Effectiveness measures how much <em>extra</em> energy a data center uses beyond just the compute hardware ‚Äî cooling, lighting, networking, storage. A PUE of 1.2 means for every 1 watt of GPU power, the facility uses 1.2 watts total. Google reports PUE of ~1.1, while the industry average is closer to 1.3-1.6. We use 1.2 as a reasonable middle ground for hyperscaler facilities.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have an `energy_model.py` file with GPU profiles, model profiles, and an `estimate_energy_wh()` function. Try running it mentally: 1000 tokens on GPT-4o should give you something in the range of 0.001‚Äì0.01 Wh. If your agent gave you numbers way outside that, ask it to double-check the unit conversions.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Adding Water and Carbon ‚Äî The Environmental Layer</h2>
      </div>
      <div class="step-body">
        <p>Energy is only part of the story. Data centers use enormous amounts of water for cooling, and the carbon impact depends entirely on <em>where</em> the electricity comes from. A prompt served from a data center in Iowa (wind-heavy grid) has a very different carbon footprint than one served from West Virginia (coal-heavy grid).</p>

<p>We need two more calculations:</p>

<ul>
<li><strong>Water</strong>: roughly 0.5‚Äì1 liter per kWh of data center energy (varies by cooling method)</li>
<li><strong>Carbon</strong>: grams of CO‚ÇÇ per kWh, which varies from ~20 (Norway hydro) to ~800+ (coal-heavy regions)</li>
</ul>

<p>The electricityMap API gives real-time carbon intensity by region. Let&#x27;s get our agent to add environmental impact calculations on top of our energy model.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to create an environmental impact module that takes watt-hours and returns water consumption (liters) and carbon emissions (grams CO‚ÇÇ), with support for different grid regions.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What data structure makes sense for representing different regions and their carbon intensity?</li><li>Should we call the electricityMap API live, or start with a static lookup table? What are the trade-offs?</li><li>Water usage depends on the cooling method (evaporative vs. air-cooled). How might we model that simply?</li><li>What&#x27;s a good default region for someone who doesn&#x27;t know where their API provider&#x27;s data center is?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>See what the agent gives back</summary>
        <div class="reveal-body">
      <div class="code-block">
        <span class="code-caption">A clean environmental module with region-aware carbon intensity and cooling-method-aware water estimates. Notice we start static and can plug in the live API later.</span>
        <button class="copy-btn">COPY</button>
        <pre><code class="language-python">&quot;&quot;&quot;environmental.py ‚Äî water and carbon impact calculations.&quot;&quot;&quot;
from dataclasses import dataclass
from enum import Enum


class CoolingMethod(Enum):
    EVAPORATIVE = &quot;evaporative&quot;   # wet cooling ‚Äî common in hot/dry climates
    AIR_COOLED = &quot;air_cooled&quot;     # less water, more energy
    HYBRID = &quot;hybrid&quot;             # mix of both


# Liters of water per kWh of data-center energy
WATER_INTENSITY = {
    CoolingMethod.EVAPORATIVE: 1.8,
    CoolingMethod.AIR_COOLED: 0.1,
    CoolingMethod.HYBRID: 0.75,
}


@dataclass
class GridRegion:
    code: str
    name: str
    carbon_intensity_gco2_kwh: float  # grams CO‚ÇÇ per kWh
    default_cooling: CoolingMethod = CoolingMethod.HYBRID


# Static lookup ‚Äî a curated subset from electricityMap averages
GRID_REGIONS = {
    &quot;US-CAL&quot;:  GridRegion(&quot;US-CAL&quot;,  &quot;California&quot;,           220, CoolingMethod.AIR_COOLED),
    &quot;US-VA&quot;:   GridRegion(&quot;US-VA&quot;,   &quot;Virginia (us-east-1)&quot;, 310, CoolingMethod.HYBRID),
    &quot;US-IA&quot;:   GridRegion(&quot;US-IA&quot;,   &quot;Iowa (us-east-2)&quot;,     380, CoolingMethod.EVAPORATIVE),
    &quot;US-OR&quot;:   GridRegion(&quot;US-OR&quot;,   &quot;Oregon (us-west-2)&quot;,    90, CoolingMethod.AIR_COOLED),
    &quot;EU-IE&quot;:   GridRegion(&quot;EU-IE&quot;,   &quot;Ireland (eu-west-1)&quot;,  300, CoolingMethod.HYBRID),
    &quot;EU-SE&quot;:   GridRegion(&quot;EU-SE&quot;,   &quot;Sweden&quot;,                25, CoolingMethod.AIR_COOLED),
    &quot;EU-DE&quot;:   GridRegion(&quot;EU-DE&quot;,   &quot;Germany&quot;,              350, CoolingMethod.HYBRID),
    &quot;EU-FR&quot;:   GridRegion(&quot;EU-FR&quot;,   &quot;France&quot;,                55, CoolingMethod.AIR_COOLED),
    &quot;AP-SG&quot;:   GridRegion(&quot;AP-SG&quot;,   &quot;Singapore&quot;,            410, CoolingMethod.EVAPORATIVE),
    &quot;AP-JP&quot;:   GridRegion(&quot;AP-JP&quot;,   &quot;Japan (Tokyo)&quot;,        460, CoolingMethod.HYBRID),
    &quot;WORLD&quot;:   GridRegion(&quot;WORLD&quot;,   &quot;Global Average&quot;,       436, CoolingMethod.HYBRID),
}


def estimate_environmental_impact(
    energy_wh: float,
    region_code: str = &quot;WORLD&quot;,
) -&gt; dict:
    &quot;&quot;&quot;Convert watt-hours into water (liters) and carbon (gCO‚ÇÇ).&quot;&quot;&quot;
    region = GRID_REGIONS.get(region_code, GRID_REGIONS[&quot;WORLD&quot;])
    energy_kwh = energy_wh / 1000

    water_liters = energy_kwh * WATER_INTENSITY[region.default_cooling]
    carbon_grams = energy_kwh * region.carbon_intensity_gco2_kwh

    return {
        &quot;region&quot;: region.name,
        &quot;energy_wh&quot;: round(energy_wh, 6),
        &quot;energy_kwh&quot;: round(energy_kwh, 8),
        &quot;water_liters&quot;: round(water_liters, 6),
        &quot;carbon_grams_co2&quot;: round(carbon_grams, 6),
        &quot;carbon_intensity_source&quot;: &quot;static (electricityMap averages)&quot;,
    }</code></pre>
      </div></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Starting with a static lookup table instead of a live API call is a great pattern. It means your tool works offline, tests are deterministic, and you can add the live API as an <em>enhancement</em> later ‚Äî not a hard dependency.
      </div>
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        If you want to add live electricityMap data later, you&#x27;ll need a free API key from app.electricitymap.org. Their free tier gives 100 requests/month ‚Äî plenty for personal use.
      </div>
        
      <details class="reveal">
        <summary>Why does Virginia matter so much?</summary>
        <div class="reveal-body"><p>Northern Virginia (the us-east-1 AWS region) hosts the largest concentration of data centers on Earth ‚Äî roughly 70% of the world&#x27;s internet traffic passes through &quot;Data Center Alley&quot; in Loudoun County. When you call the OpenAI or Anthropic API from the eastern US, there&#x27;s a decent chance your prompt is processed there. Virginia&#x27;s grid is a mix of natural gas, nuclear, and some renewables ‚Äî landing around 300-350 gCO‚ÇÇ/kWh.</p></div>
      </details>
      <details class="reveal">
        <summary>How accurate are these water numbers?</summary>
        <div class="reveal-body"><p>Honestly? They&#x27;re rough. Microsoft disclosed that their global water consumption rose 34% in 2022 largely due to AI training. A 2024 UC Riverside study estimated ChatGPT &quot;drinks&quot; about 500ml of water per 20-50 responses. Our per-kWh multipliers are simplifications ‚Äî real water usage depends on ambient temperature, humidity, time of year, and the specific cooling system. But for awareness purposes, they&#x27;re in the right ballpark.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>The CLI ‚Äî Bringing It All Together</h2>
      </div>
      <div class="step-body">
        <p>Now let&#x27;s wire everything into a command-line tool. We want something you can actually <em>use</em> ‚Äî pipe it into your workflow, run it after an API call, maybe even integrate it as middleware later.</p>

<p>The UX we&#x27;re going for:</p>

<p>```</p>
<p>$ python estimator.py --model gpt-4o --prompt-tokens 500 --completion-tokens 1500 --region US-VA</p>
<p>```</p>

<p>And it should spit out a nicely formatted summary of energy, water, and carbon. Honestly, this is the kind of thing where AI agents shine ‚Äî CLIs with argument parsing and formatted output are well-trodden ground.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to build a CLI entry point that uses argparse, calls both our modules, and prints a clean, readable summary.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What arguments does the CLI need? Think about required vs optional, and what sensible defaults look like.</li><li>How should the output look? Table format? Key-value pairs? Should there be a --json flag for machine-readable output?</li><li>What happens if someone passes a model name we don&#x27;t recognize? How should we handle that gracefully?</li><li>Should we list available models and regions if someone passes --help?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>See what the agent gives back</summary>
        <div class="reveal-body">
      <div class="code-block">
        <span class="code-caption">A polished CLI with argparse, emoji-decorated output, and a --json flag for piping into other tools. The --help even lists available models and regions.</span>
        <button class="copy-btn">COPY</button>
        <pre><code class="language-python">#!/usr/bin/env python3
&quot;&quot;&quot;estimator.py ‚Äî CLI entry point for the LLM energy-cost estimator.&quot;&quot;&quot;
import argparse
import json
import sys

from energy_model import MODELS, estimate_energy_wh
from environmental import GRID_REGIONS, estimate_environmental_impact


def format_number(value: float, unit: str) -&gt; str:
    &quot;&quot;&quot;Human-friendly number formatting.&quot;&quot;&quot;
    if value &lt; 0.001:
        return f&quot;{value:.2e} {unit}&quot;
    elif value &lt; 1:
        return f&quot;{value:.4f} {unit}&quot;
    else:
        return f&quot;{value:.2f} {unit}&quot;


def print_report(energy: dict, env: dict) -&gt; None:
    &quot;&quot;&quot;Print a formatted human-readable report.&quot;&quot;&quot;
    bar = &quot;‚ïê&quot; * 52
    print(f&quot;\n{bar}&quot;)
    print(f&quot;  ‚ö° LLM Prompt Energy Estimate&quot;)
    print(f&quot;{bar}&quot;)
    print(f&quot;  Model:            {energy[&#x27;model&#x27;]}&quot;)
    print(f&quot;  Tokens:           {energy[&#x27;total_tokens&#x27;]:,} &quot;
          f&quot;(prompt + completion)&quot;)
    print(f&quot;  GPU time:         {format_number(energy[&#x27;gpu_seconds&#x27;], &#x27;sec&#x27;)}&quot;)
    print(f&quot;  PUE:              {energy[&#x27;pue&#x27;]}&quot;)
    print(f&quot;{&#x27;‚îÄ&#x27; * 52}&quot;)
    print(f&quot;  üîã Energy:         {format_number(energy[&#x27;energy_wh&#x27;], &#x27;Wh&#x27;)}&quot;)
    print(f&quot;  üíß Water:          {format_number(env[&#x27;water_liters&#x27;], &#x27;liters&#x27;)}&quot;)
    print(f&quot;  üåç Carbon:         {format_number(env[&#x27;carbon_grams_co2&#x27;], &#x27;g CO‚ÇÇ&#x27;)}&quot;)
    print(f&quot;{&#x27;‚îÄ&#x27; * 52}&quot;)
    print(f&quot;  Grid region:      {env[&#x27;region&#x27;]}&quot;)
    print(f&quot;  Carbon source:    {env[&#x27;carbon_intensity_source&#x27;]}&quot;)
    print(f&quot;{bar}\n&quot;)


def main() -&gt; None:
    parser = argparse.ArgumentParser(
        description=&quot;Estimate the energy, water, and carbon cost of an LLM prompt.&quot;,
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            &quot;Available models:\n  &quot;
            + &quot;\n  &quot;.join(sorted(MODELS.keys()))
            + &quot;\n\nAvailable regions:\n  &quot;
            + &quot;\n  &quot;.join(
                f&quot;{k:8s} ‚Äî {v.name}&quot; for k, v in sorted(GRID_REGIONS.items())
            )
        ),
    )
    parser.add_argument(
        &quot;--model&quot;, &quot;-m&quot;, required=True, choices=sorted(MODELS.keys()),
        help=&quot;LLM model name&quot;,
    )
    parser.add_argument(
        &quot;--prompt-tokens&quot;, &quot;-p&quot;, type=int, required=True,
        help=&quot;Number of prompt (input) tokens&quot;,
    )
    parser.add_argument(
        &quot;--completion-tokens&quot;, &quot;-c&quot;, type=int, required=True,
        help=&quot;Number of completion (output) tokens&quot;,
    )
    parser.add_argument(
        &quot;--region&quot;, &quot;-r&quot;, default=&quot;WORLD&quot;,
        choices=sorted(GRID_REGIONS.keys()),
        help=&quot;Grid region for carbon intensity (default: WORLD)&quot;,
    )
    parser.add_argument(
        &quot;--pue&quot;, type=float, default=1.2,
        help=&quot;Power Usage Effectiveness (default: 1.2)&quot;,
    )
    parser.add_argument(
        &quot;--json&quot;, dest=&quot;output_json&quot;, action=&quot;store_true&quot;,
        help=&quot;Output raw JSON instead of formatted report&quot;,
    )

    args = parser.parse_args()

    energy = estimate_energy_wh(
        model_name=args.model,
        prompt_tokens=args.prompt_tokens,
        completion_tokens=args.completion_tokens,
        pue=args.pue,
    )
    env = estimate_environmental_impact(
        energy_wh=energy[&quot;energy_wh&quot;],
        region_code=args.region,
    )

    if args.output_json:
        combined = {**energy, **env}
        print(json.dumps(combined, indent=2))
    else:
        print_report(energy, env)


if __name__ == &quot;__main__&quot;:
    main()</code></pre>
      </div></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The --json flag is a small detail that makes a huge difference. It means you can do <code>python estimator.py --model gpt-4o -p 500 -c 1500 --json | jq .carbon_grams_co2</code> and pipe the output into dashboards, logging systems, or middleware.
      </div>
        
      <details class="reveal">
        <summary>What would middleware integration look like?</summary>
        <div class="reveal-body"><p>Imagine wrapping your OpenAI API calls with a decorator that automatically logs energy estimates:</p>

<p>```python</p>
<p>@track_energy(model=&quot;gpt-4o&quot;, region=&quot;US-VA&quot;)</p>
<p>def ask_gpt(prompt):</p>
<p>return openai.chat.completions.create(...)</p>
<p>```</p>

<p>The decorator would read the token counts from the API response and call our estimator behind the scenes. You could aggregate this into a daily/weekly energy report for your team. Some companies are already building this kind of observability into their LLM platforms.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have three files: `energy_model.py`, `environmental.py`, and `estimator.py`. Try running: `python estimator.py --model gpt-4o -p 500 -c 1500 --region US-VA`. You should see a formatted report showing energy in the range of 0.001‚Äì0.01 Wh, a tiny fraction of a liter of water, and a small number of milligrams of CO‚ÇÇ. Those numbers look small per-prompt ‚Äî but multiply by millions of daily requests and you start to see the MIT Tech Review headline.</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">Your estimator currently uses a static table for carbon intensity. A teammate suggests calling the electricityMap API for every single estimation. What&#x27;s the right approach?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_7" id="decision_7_opt0">
          <label for="decision_7_opt0">Static table as default, with an optional --live flag that hits the API</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the best of both worlds. The tool works offline and fast by default (great for CI/CD, testing, and places without internet). When someone wants real-time data, they opt in with --live. This follows the principle of progressive enhancement ‚Äî start simple, add complexity as an option.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_7" id="decision_7_opt1">
          <label for="decision_7_opt1">Always call the API for accurate data</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This makes your tool dependent on network access, slower (API latency on every call), and ironically ‚Äî uses more energy itself. Plus you&#x27;d burn through the free API tier quickly. Real-time data is nice but shouldn&#x27;t be a hard requirement.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_7" id="decision_7_opt2">
          <label for="decision_7_opt2">Just use the static table forever, APIs are overkill</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Carbon intensity actually varies significantly by time of day (solar peaks during daytime, wind varies with weather). A static annual average misses these swings. Having the *option* for live data is genuinely valuable ‚Äî you just shouldn&#x27;t force it.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Batch Mode ‚Äî Estimating a Full Conversation</h2>
      </div>
      <div class="step-body">
        <p>A single prompt is interesting, but real workloads involve multi-turn conversations, RAG pipelines, or batch processing. Let&#x27;s add a mode where you can feed in a JSON file of API calls and get a cumulative report.</p>

<p>This is the kind of feature that separates a toy script from a tool people actually use. Imagine running this against your API logs at the end of the month: &quot;Your team&#x27;s LLM usage this February consumed an estimated 4.2 kWh, 3 liters of water, and 1.4 kg of CO‚ÇÇ ‚Äî equivalent to driving 5.6 km in a gas car.&quot;</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to add a batch estimation mode that reads a JSON array of API calls and outputs an aggregated summary with per-call breakdown and totals, including relatable comparisons.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What should the input JSON format look like? Think about what fields each entry needs.</li><li>What makes the output *memorable*? Raw numbers are forgettable ‚Äî comparisons to everyday things (driving a car, charging a phone) stick in people&#x27;s minds.</li><li>Should the batch mode be a separate command, a flag on the existing CLI, or a subcommand?</li><li>How do you handle a batch where different calls use different models or regions?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>See what the agent gives back</summary>
        <div class="reveal-body">
      <div class="code-block">
        <span class="code-caption">Batch mode with relatable comparisons ‚Äî because &#x27;0.034 Wh&#x27; means nothing to most people, but &#x27;11 Google searches&#x27; clicks instantly.</span>
        <button class="copy-btn">COPY</button>
        <pre><code class="language-python">&quot;&quot;&quot;batch.py ‚Äî batch estimation mode for multiple API calls.&quot;&quot;&quot;
import json
import sys
from pathlib import Path

from energy_model import MODELS, estimate_energy_wh
from environmental import GRID_REGIONS, estimate_environmental_impact


# Relatable comparisons
COMPARISONS = {
    &quot;phone_charges&quot;: {&quot;kwh&quot;: 0.015, &quot;label&quot;: &quot;smartphone charges&quot;},
    &quot;led_bulb_hours&quot;: {&quot;kwh&quot;: 0.01, &quot;label&quot;: &quot;hours of LED bulb&quot;},
    &quot;km_driven&quot;: {&quot;gco2&quot;: 121, &quot;label&quot;: &quot;km driven (avg gas car)&quot;},
    &quot;google_searches&quot;: {&quot;wh&quot;: 0.3, &quot;label&quot;: &quot;Google searches equivalent&quot;},
}


def make_comparisons(total_wh: float, total_co2_g: float) -&gt; list[str]:
    &quot;&quot;&quot;Generate relatable comparisons for the totals.&quot;&quot;&quot;
    results = []
    kwh = total_wh / 1000

    charges = kwh / COMPARISONS[&quot;phone_charges&quot;][&quot;kwh&quot;]
    if charges &gt;= 0.01:
        results.append(f&quot;üì± {charges:.1f} smartphone charges&quot;)

    led_hours = kwh / COMPARISONS[&quot;led_bulb_hours&quot;][&quot;kwh&quot;]
    if led_hours &gt;= 0.1:
        results.append(f&quot;üí° {led_hours:.1f} hours of an LED bulb&quot;)

    km = total_co2_g / COMPARISONS[&quot;km_driven&quot;][&quot;gco2&quot;]
    if km &gt;= 0.001:
        results.append(f&quot;üöó {km:.2f} km driven in a gas car&quot;)

    searches = total_wh / COMPARISONS[&quot;google_searches&quot;][&quot;wh&quot;]
    results.append(f&quot;üîç {searches:.0f} Google searches&quot;)

    return results


def run_batch(input_path: str, default_region: str = &quot;WORLD&quot;) -&gt; dict:
    &quot;&quot;&quot;Process a JSON batch file and return aggregated results.&quot;&quot;&quot;
    data = json.loads(Path(input_path).read_text())

    calls = []
    total_wh = 0.0
    total_water = 0.0
    total_co2 = 0.0

    for i, entry in enumerate(data):
        model = entry[&quot;model&quot;]
        prompt_tokens = entry[&quot;prompt_tokens&quot;]
        completion_tokens = entry[&quot;completion_tokens&quot;]
        region = entry.get(&quot;region&quot;, default_region)

        if model not in MODELS:
            print(f&quot;‚ö† Skipping entry {i}: unknown model &#x27;{model}&#x27;&quot;,
                  file=sys.stderr)
            continue

        energy = estimate_energy_wh(model, prompt_tokens, completion_tokens)
        env = estimate_environmental_impact(energy[&quot;energy_wh&quot;], region)

        call_summary = {
            &quot;index&quot;: i,
            &quot;model&quot;: model,
            &quot;tokens&quot;: energy[&quot;total_tokens&quot;],
            &quot;energy_wh&quot;: energy[&quot;energy_wh&quot;],
            &quot;water_liters&quot;: env[&quot;water_liters&quot;],
            &quot;carbon_g&quot;: env[&quot;carbon_grams_co2&quot;],
            &quot;region&quot;: region,
        }
        calls.append(call_summary)

        total_wh += energy[&quot;energy_wh&quot;]
        total_water += env[&quot;water_liters&quot;]
        total_co2 += env[&quot;carbon_grams_co2&quot;]

    comparisons = make_comparisons(total_wh, total_co2)

    return {
        &quot;total_calls&quot;: len(calls),
        &quot;total_energy_wh&quot;: round(total_wh, 4),
        &quot;total_water_liters&quot;: round(total_water, 6),
        &quot;total_carbon_grams_co2&quot;: round(total_co2, 4),
        &quot;comparisons&quot;: comparisons,
        &quot;calls&quot;: calls,
    }


if __name__ == &quot;__main__&quot;:
    if len(sys.argv) &lt; 2:
        print(&quot;Usage: python batch.py &lt;input.json&gt; [region]&quot;)
        sys.exit(1)

    region = sys.argv[2] if len(sys.argv) &gt; 2 else &quot;WORLD&quot;
    result = run_batch(sys.argv[1], region)

    print(f&quot;\n{&#x27;‚ïê&#x27; * 52}&quot;)
    print(f&quot;  üìä Batch Energy Report ‚Äî {result[&#x27;total_calls&#x27;]} API calls&quot;)
    print(f&quot;{&#x27;‚ïê&#x27; * 52}&quot;)
    print(f&quot;  üîã Total energy:    {result[&#x27;total_energy_wh&#x27;]:.4f} Wh&quot;)
    print(f&quot;  üíß Total water:     {result[&#x27;total_water_liters&#x27;]:.4f} liters&quot;)
    print(f&quot;  üåç Total carbon:    {result[&#x27;total_carbon_grams_co2&#x27;]:.4f} g CO‚ÇÇ&quot;)
    print(f&quot;{&#x27;‚îÄ&#x27; * 52}&quot;)
    print(f&quot;  That&#x27;s roughly:&quot;)
    for comp in result[&quot;comparisons&quot;]:
        print(f&quot;    {comp}&quot;)
    print(f&quot;{&#x27;‚ïê&#x27; * 52}\n&quot;)</code></pre>
      </div></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Want real data to test with? Ask your agent: &#x27;Generate a sample JSON batch file with 20 realistic API calls across different models, varying prompt and completion token counts.&#x27; Agents are great at generating test data.
      </div>
        
      <details class="reveal">
        <summary>Making the comparisons more accurate</summary>
        <div class="reveal-body"><p>Our comparison numbers (121 gCO‚ÇÇ/km for a car, 0.3 Wh per Google search) are widely-cited averages but they come with asterisks. The car figure is the EU fleet average ‚Äî a Tesla in Norway is basically zero, a pickup in Texas is much higher. The Google search figure is from 2009 and Google&#x27;s efficiency has improved. For a production tool, you might want to cite your sources in the output and let users configure their own comparison baselines.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Quick Test Suite ‚Äî Trusting the Numbers</h2>
      </div>
      <div class="step-body">
        <p>We&#x27;ve got a working tool, but how do we know the numbers are reasonable? This is actually a really interesting testing challenge ‚Äî we can&#x27;t easily verify against ground truth (the real energy numbers are proprietary). But we <em>can</em> test:</p>

<ol>
<li><strong>Sanity bounds</strong>: A single GPT-4o prompt should not estimate more than 1 Wh</li>
<li><strong>Proportionality</strong>: More tokens should always mean more energy</li>
<li><strong>Region ordering</strong>: Sweden should always be lower-carbon than Germany</li>
<li><strong>Unit consistency</strong>: The math chain from FLOPs ‚Üí Joules ‚Üí Wh should be dimensionally correct</li>
</ol>

<p>Let&#x27;s get our agent to write tests that validate the <em>reasonableness</em> of our estimates.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to create a pytest test suite that validates sanity bounds, proportionality, and region comparisons ‚Äî not exact values, but reasonable ranges.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>How do you test something when you don&#x27;t know the exact right answer? Think about bounds and relationships...</li><li>What invariants should ALWAYS hold? (e.g., more tokens = more energy, carbon in Sweden &lt; carbon in Germany)</li><li>What&#x27;s a reasonable upper bound for energy per prompt? Think about what would be obviously wrong.</li></ul>
        </div>
        
      <details class="reveal">
        <summary>See what the agent gives back</summary>
        <div class="reveal-body">
      <div class="code-block">
        <span class="code-caption">Tests that validate reasonableness rather than exact values. The double-tokens test is particularly clever ‚Äî it checks that our math is truly linear.</span>
        <button class="copy-btn">COPY</button>
        <pre><code class="language-python">&quot;&quot;&quot;test_estimator.py ‚Äî sanity and proportionality tests.&quot;&quot;&quot;
import pytest
from energy_model import MODELS, estimate_energy_wh
from environmental import estimate_environmental_impact, GRID_REGIONS


class TestEnergySanity:
    &quot;&quot;&quot;Verify estimates fall within reasonable bounds.&quot;&quot;&quot;

    def test_single_prompt_under_1wh(self):
        &quot;&quot;&quot;A single prompt should never exceed 1 Wh.&quot;&quot;&quot;
        for model_name in MODELS:
            result = estimate_energy_wh(model_name, 1000, 500)
            assert result[&quot;energy_wh&quot;] &lt; 1.0, (
                f&quot;{model_name}: {result[&#x27;energy_wh&#x27;]} Wh exceeds 1 Wh sanity bound&quot;
            )

    def test_single_prompt_above_zero(self):
        &quot;&quot;&quot;Every prompt should consume *some* energy.&quot;&quot;&quot;
        for model_name in MODELS:
            result = estimate_energy_wh(model_name, 1, 1)
            assert result[&quot;energy_wh&quot;] &gt; 0

    def test_small_model_cheaper_than_large(self):
        &quot;&quot;&quot;An 8B model should use less energy than a 70B model.&quot;&quot;&quot;
        small = estimate_energy_wh(&quot;llama-8b&quot;, 500, 500)
        large = estimate_energy_wh(&quot;llama-70b&quot;, 500, 500)
        assert small[&quot;energy_wh&quot;] &lt; large[&quot;energy_wh&quot;]


class TestProportionality:
    &quot;&quot;&quot;More tokens should always mean more energy.&quot;&quot;&quot;

    @pytest.mark.parametrize(&quot;model&quot;, list(MODELS.keys()))
    def test_more_tokens_more_energy(self, model):
        small = estimate_energy_wh(model, 100, 100)
        large = estimate_energy_wh(model, 1000, 1000)
        assert large[&quot;energy_wh&quot;] &gt; small[&quot;energy_wh&quot;]

    def test_double_tokens_double_energy(self):
        &quot;&quot;&quot;Energy should scale linearly with token count.&quot;&quot;&quot;
        base = estimate_energy_wh(&quot;gpt-4o&quot;, 500, 500)
        double = estimate_energy_wh(&quot;gpt-4o&quot;, 1000, 1000)
        ratio = double[&quot;energy_wh&quot;] / base[&quot;energy_wh&quot;]
        assert 1.9 &lt; ratio &lt; 2.1, f&quot;Expected ~2x scaling, got {ratio:.2f}x&quot;


class TestEnvironmental:
    &quot;&quot;&quot;Carbon and water should vary by region as expected.&quot;&quot;&quot;

    def test_sweden_lower_carbon_than_germany(self):
        env_se = estimate_environmental_impact(1.0, &quot;EU-SE&quot;)
        env_de = estimate_environmental_impact(1.0, &quot;EU-DE&quot;)
        assert env_se[&quot;carbon_grams_co2&quot;] &lt; env_de[&quot;carbon_grams_co2&quot;]

    def test_oregon_lower_carbon_than_virginia(self):
        env_or = estimate_environmental_impact(1.0, &quot;US-OR&quot;)
        env_va = estimate_environmental_impact(1.0, &quot;US-VA&quot;)
        assert env_or[&quot;carbon_grams_co2&quot;] &lt; env_va[&quot;carbon_grams_co2&quot;]

    def test_water_always_positive(self):
        for region_code in GRID_REGIONS:
            env = estimate_environmental_impact(1.0, region_code)
            assert env[&quot;water_liters&quot;] &gt; 0

    def test_zero_energy_zero_impact(self):
        env = estimate_environmental_impact(0.0, &quot;WORLD&quot;)
        assert env[&quot;water_liters&quot;] == 0
        assert env[&quot;carbon_grams_co2&quot;] == 0</code></pre>
      </div></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        This &quot;property-based&quot; testing style ‚Äî checking invariants and relationships instead of exact values ‚Äî is incredibly useful when building estimation tools. You&#x27;re testing the <em>shape</em> of the math, not specific numbers that might change as you refine your model.
      </div>
        
      </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Add a comparison mode to the CLI that lets users compare the energy cost of the same prompt across different models. Something like: `python estimator.py compare -p 500 -c 1500 --models gpt-4o llama-8b claude-sonnet --region US-VA` that outputs a side-by-side table showing which model is most energy-efficient for that workload.</div>
      <div class="your-turn-context">This is a practical feature: if you&#x27;re choosing between models for a project and sustainability matters to your team, you want to see the energy trade-offs at a glance. Smaller models are almost always more energy-efficient, but by how much? The comparison table makes that tangible.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What should the columns be in the comparison table? Model name, tokens, energy, water, carbon ‚Äî anything else useful like a relative percentage?</li><li>How should the models be sorted in the output? By energy consumption? Alphabetically?</li><li>What if someone passes a model name that doesn&#x27;t exist in our catalog? Should we skip it, error out, or warn and continue?</li><li>Could you add a &#x27;winner&#x27; indicator that highlights the most efficient option?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Add a &#x27;compare&#x27; subcommand to my estimator CLI. It should accept --models (a list of 2+ model names), --prompt-tokens, --completion-tokens, and --region. Output a side-by-side table sorted by energy consumption (lowest first), with columns for: model name, energy (Wh), water (liters), carbon (g CO‚ÇÇ), and a relative efficiency column showing percentage vs the most expensive model. Mark the most efficient model with a green checkmark. If a model name isn&#x27;t in our catalog, print a warning to stderr and skip it. Support both human-readable table and --json output.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>Look at what we just built ‚Äî a real, working CLI tool that turns invisible LLM costs into visible numbers. And we did it by <em>directing</em> an AI agent, not by writing every line ourselves. That&#x27;s the meta-skill here: knowing what to ask for, evaluating what you get back, and iterating.</p>

<p>Our estimator does the full chain: <strong>tokens ‚Üí FLOPs ‚Üí GPU-seconds ‚Üí watt-hours ‚Üí water and carbon</strong>, with region-aware carbon intensity and relatable real-world comparisons. It supports single prompts, batch files, JSON output for pipelines, and it&#x27;s tested with property-based sanity checks.</p>

<p>Is it perfectly accurate? No ‚Äî and it can&#x27;t be, because the real numbers are proprietary. But it&#x27;s <em>reasonably</em> accurate, and more importantly, it makes the invisible <em>visible</em>. That&#x27;s exactly what the MIT Tech Review investigation was about: shining a light on hidden costs so we can make informed decisions.</p></div>
      <ul class="takeaways-list"><li>Every LLM prompt has a physical cost ‚Äî energy, water, and carbon ‚Äî that scales with model size and token count. A rough formula: FLOPs per token ‚âà 2 √ó model parameters, then divide by GPU throughput to get time, multiply by power draw to get energy.</li><li>Where your prompt is processed matters enormously for carbon impact. The same API call can be 10x cleaner in Sweden (hydro) vs Singapore (gas). Region-aware estimation isn&#x27;t a nice-to-have ‚Äî it&#x27;s essential for honest accounting.</li><li>When building estimation tools, test <em>properties and relationships</em> (more tokens = more energy, smaller models &lt; bigger models) rather than exact values. You&#x27;re validating the shape of your math, not memorizing answers.</li><li>The agent-first workflow ‚Äî goal ‚Üí hints ‚Üí prompt ‚Üí evaluate ‚Üí iterate ‚Äî is how professional developers increasingly work. The skill isn&#x27;t writing Python; it&#x27;s knowing what to ask for and spotting when something&#x27;s wrong.</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Wire the estimator into a real API pipeline as middleware ‚Äî intercept OpenAI/Anthropic responses and automatically log energy estimates alongside your existing observability</li><li>Add the live electricityMap API behind a --live flag for real-time carbon intensity that changes by time of day</li><li>Build a simple web dashboard (ask your agent for a Flask or Streamlit app) that visualizes your team&#x27;s cumulative LLM energy usage over time</li><li>Extend the model catalog with newer architectures (Mixture of Experts models like Mixtral are interesting ‚Äî they activate fewer parameters per token, so energy per token can be lower despite a large total parameter count)</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://www.technologyreview.com/2026/02/27/1133769/asme-finalist-reporting/" target="_blank" rel="noopener">MIT Technology Review is a 2026 ASME finalist in reporting</a> <span class="source-name">(MIT Tech Review)</span></li></ul>
    </div>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>