<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Context Mode: Slash Your MCP Context Window Burn by 98% — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Space+Mono:wght@400;700&family=DM+Sans:wght@300;400;500;600;700&display=swap');
:root {
  --bg: #ffffff;
  --ink: #0a0a0a;
  --ink-secondary: #333;
  --muted: #888;
  --red: #E63226;
  --blue: #1B3F8B;
  --yellow: #F5B731;
  --green: #1a8754;
  --light-gray: #f6f6f6;
  --border-gray: #e0e0e0;
  --mono: 'Space Mono', monospace;
  --sans: 'DM Sans', system-ui, sans-serif;
  --border: 2px solid var(--ink);
  --max-w: 720px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
}

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 4rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.4rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  text-decoration: none;
  padding: 1.5rem 0 1rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--ink); }

/* ── Hero ── */
.session-hero {
  border-bottom: 3px solid var(--ink);
  padding: 2.5rem 0 2rem;
  margin-bottom: 2.5rem;
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: #fff;
  background: var(--red);
  padding: 0.25rem 0.75rem;
  margin-bottom: 1rem;
}
.session-hero h1 {
  font-family: var(--mono);
  font-size: 2rem;
  font-weight: 700;
  line-height: 1.15;
  letter-spacing: -0.02em;
  margin-bottom: 0.5rem;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.5rem;
  margin-top: 1rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.04em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--light-gray);
  border: 1px solid var(--border-gray);
  padding: 0.15rem 0.5rem;
  font-size: 0.65rem;
}

/* ── Section divider ── */
.section-divider {
  border: none;
  border-top: 3px solid var(--ink);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--light-gray);
  border-left: 4px solid var(--ink);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.5rem;
}
.step-number {
  flex-shrink: 0;
  width: 48px; height: 48px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 1.2rem;
  font-weight: 700;
  display: flex;
  align-items: center;
  justify-content: center;
}
.step-header h2 {
  font-family: var(--mono);
  font-size: 1.25rem;
  font-weight: 700;
  line-height: 1.2;
  padding-top: 0.3rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.3rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--blue); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--light-gray);
  border: 1px solid var(--border-gray);
  border-left: 4px solid var(--blue);
}
.code-caption {
  display: block;
  padding: 0.5rem 1rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  color: var(--muted);
  border-bottom: 1px solid var(--border-gray);
  letter-spacing: 0.04em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.85rem;
  line-height: 1.5;
  color: var(--ink);
}
.copy-btn {
  position: absolute;
  top: 0.4rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 700;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  background: var(--ink);
  color: #fff;
  border: none;
  padding: 0.3rem 0.6rem;
  cursor: pointer;
  transition: background 0.15s;
}
.copy-btn:hover { background: var(--blue); }
.copy-btn.copied { background: var(--green); }

/* ── Callouts ── */
.callout {
  border-left: 4px solid;
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.95rem;
}
.callout-tip {
  border-color: var(--blue);
  background: rgba(27,63,139,0.05);
}
.callout-warning {
  border-color: var(--red);
  background: rgba(230,50,38,0.05);
}
.callout-api-key-note {
  border-color: var(--yellow);
  background: rgba(245,183,49,0.1);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.3rem;
}
.callout-tip .callout-label { color: var(--blue); }
.callout-warning .callout-label { color: var(--red); }
.callout-api-key-note .callout-label { color: #b8860b; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border: 1px solid var(--border-gray);
}
.reveal summary {
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--light-gray);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: #eee; }
.reveal summary::before {
  content: "\25B6";
  font-size: 0.6rem;
  transition: transform 0.2s;
}
.reveal[open] summary::before {
  transform: rotate(90deg);
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border-gray);
  font-size: 0.95rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 1rem;
  padding: 1.25rem 1.5rem;
  background: var(--ink);
  color: #fff;
  margin: 2rem 0;
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
  letter-spacing: 0.02em;
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 32px; height: 32px;
  border: 2px solid #fff;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 1rem;
}

/* ── Decision point ── */
.decision-point {
  border: 2px solid var(--ink);
  margin: 2rem 0;
  padding: 1.5rem;
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
}
.decision-option {
  margin-bottom: 0.75rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.75rem 1rem;
  border: 1px solid var(--border-gray);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
}
.decision-option label:hover {
  border-color: var(--ink);
  background: var(--light-gray);
}
.decision-option input:checked + label {
  border-color: var(--ink);
  border-width: 2px;
  background: var(--light-gray);
}
.decision-feedback {
  display: none;
  padding: 0.75rem 1rem;
  margin-top: 0.25rem;
  font-size: 0.9rem;
  border-left: 3px solid;
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: var(--green);
  background: rgba(26,135,84,0.05);
  color: var(--green);
}
.decision-feedback.incorrect {
  border-color: var(--red);
  background: rgba(230,50,38,0.05);
  color: var(--red);
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem 0;
  border: 2px solid var(--ink);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 700;
}
.agent-goal-label {
  font-size: 0.6rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: rgba(255,255,255,0.6);
  margin-bottom: 0.3rem;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: rgba(27,63,139,0.04);
  border-bottom: 1px solid var(--border-gray);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--blue);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
  font-size: 0.95rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "?";
  position: absolute;
  left: 0;
  color: var(--blue);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* ── Your turn ── */
.your-turn {
  border: 2px solid var(--blue);
  padding: 1.5rem;
  margin: 2rem 0;
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--blue);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
}
.your-turn .your-turn-context {
  font-size: 0.95rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  border-top: 3px solid var(--ink);
  padding-top: 2.5rem;
  margin-top: 3rem;
}
.recap-section h2 {
  font-family: var(--mono);
  font-size: 1.25rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.5rem 0 0.5rem 1.5rem;
  position: relative;
  border-bottom: 1px solid var(--border-gray);
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--blue);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border-gray);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--blue);
  text-decoration: none;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration: underline; }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  margin-left: 0.5rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border-gray);
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.7rem;
  font-weight: 700;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.85rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  border: 1px solid var(--border-gray);
  padding: 1rem 1.25rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.other-article-card:hover {
  border-color: var(--ink);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.95rem;
  margin-bottom: 0.15rem;
}
.oa-summary {
  font-size: 0.85rem;
  color: var(--ink-secondary);
  margin: 0.15rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  letter-spacing: 0.04em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 36px; height: 36px;
  font-size: 1.1rem;
  border: 1px solid var(--border-gray);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--light-gray);
  user-select: none;
}
.oa-toggle-label:hover {
  border-color: var(--ink);
  background: #fff;
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  border-color: var(--green);
  background: rgba(26,135,84,0.1);
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  border-color: var(--red);
  background: rgba(230,50,38,0.1);
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--mono);
  font-size: 0.75rem;
  font-weight: 700;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  padding: 0.6rem 1.5rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: background 0.15s;
}
.oa-submit-btn:hover { background: var(--blue); }
.oa-submit-btn:disabled {
  background: var(--border-gray);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-top: 3rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 3px solid var(--ink);
}
.session-footer span { color: var(--ink); font-weight: 700; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.5rem; }
  .step-number { width: 36px; height: 36px; font-size: 1rem; }
  .session-container { padding: 0 1rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
}
</style>
</head>
<body>
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Your Context Window Has a Silent Leak</h1>
      <div class="hero-subtitle">Every MCP tool call dumps raw data into your conversation. A single Playwright snapshot costs 56 KB. Twenty GitHub issues cost 59 KB. After 30 minutes, 40% of your context is just... tool output. Let&#x27;s fix that.</div>
      <div class="hero-meta">
        <span>35 min</span>
        <span> <span class="tag">MCP</span> <span class="tag">context-window</span> <span class="tag">claude-code</span> <span class="tag">productivity</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Here&#x27;s what hit Hacker News today (180 points, 46 comments, people are <em>feeling</em> this one): a team built an MCP server called <strong>Context Mode</strong> that intercepts tool outputs before they flood your conversation. The numbers are wild — 315 KB of raw output compressed to 5.4 KB. That&#x27;s a 98% reduction.</p>

<p>Why should you care? If you&#x27;ve ever had a Claude Code session start getting sluggish after 30 minutes, or watched your agent lose track of earlier context because the window filled up with raw git diffs and API responses — this is exactly that problem. Your agent doesn&#x27;t <em>need</em> the full 56 KB Playwright snapshot sitting in the conversation. It needs a compact summary of what matters.</p>

<p>The clever bit: Context Mode doesn&#x27;t just truncate things. It runs your tool outputs through isolated subprocesses (sandboxes), processes the data there, and only sends back the distilled result. The raw data never enters your context window. It&#x27;s like having a research assistant who reads the whole report and hands you a one-page brief instead of dropping the full PDF on your desk.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Understand the Problem: Where Your Context Actually Goes</h2>
      </div>
      <div class="step-body">
        <p>Before we install anything, let&#x27;s build an intuition for <em>why</em> this matters. Most people think their context window is spent on their messages and the agent&#x27;s responses. That&#x27;s like thinking your phone battery drains from phone calls — technically true, but background apps are the real culprit.</p>

<p>Here&#x27;s the mental model: imagine your 200K context window is a 200-page notebook. Before you even write your first message, MCP tool <em>definitions</em> have already filled 70+ pages (if you have 80+ tools active). Then every time a tool runs — fetching GitHub issues, reading a log file, taking a browser snapshot — the raw output gets copied into more pages. After 30 minutes of active tool use, you&#x27;re writing on the last 20 pages and the agent has forgotten what was on page 1.</p>

<p>Let&#x27;s get your agent to help you visualize this problem with your own setup.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to estimate how much context your current MCP tools consume before you even start working
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What MCP servers do you currently have configured? Where does Claude Code store that config?</li><li>How would you estimate the token cost of tool definitions vs. tool outputs?</li><li>What&#x27;s the biggest tool output you&#x27;ve seen in a session — a test suite? A git diff? An API response?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should look at your <code>~/.claude/settings.json</code> or <code>.mcp.json</code> files and list your active MCP servers. It&#x27;ll estimate that each tool definition costs roughly 200-500 tokens, so 80 tools = ~16K-40K tokens just in definitions. Then it&#x27;ll point out that typical tool outputs range from 1 KB (simple commands) to 85 KB (CSV data or browser snapshots). The key insight it should surface: tool definitions are a fixed tax, but tool outputs are the variable cost that kills your session over time.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Run <code>/mcp</code> in Claude Code to see your active MCP servers. Count the tools — you might be surprised. The Playwright MCP server alone registers 15+ tools.
      </div>
        
      <details class="reveal">
        <summary>The 72% stat — is it really that bad?</summary>
        <div class="reveal-body"><p>The article claims 143K tokens (72%) consumed before your first message with 81+ tools active. That&#x27;s an extreme case, but not unrealistic. Each MCP server registers multiple tools, and each tool has a name, description, and input schema. The Cloudflare team showed tool definitions can be compressed 99.9% with their &#x27;Code Mode&#x27; approach — that&#x27;s the <em>input</em> side. Context Mode tackles the <em>output</em> side. They&#x27;re complementary solutions to the same problem: MCP was designed for capability, not context efficiency.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Install Context Mode and Understand the Sandbox Architecture</h2>
      </div>
      <div class="step-body">
        <p>Time to install. Context Mode gives you two options: the full plugin (with auto-routing hooks that intercept tool outputs automatically) or just the MCP server (manual, more control). We&#x27;ll go with the plugin because honestly, the whole point is to <em>not</em> think about it.</p>

<p>But before we blindly install, let&#x27;s understand what&#x27;s actually happening under the hood. The sandbox model is the clever part:</p>

<ol>
<li><strong>A tool runs</strong> — say, <code>gh issue list</code> returns 59 KB of JSON</li>
<li><strong>Context Mode intercepts</strong> the output via a PreToolUse hook</li>
<li><strong>A subprocess spawns</strong> — an isolated process with its own memory boundary</li>
<li><strong>The subprocess processes the data</strong> — extracts what matters, summarizes, computes stats</li>
<li><strong>Only stdout from the subprocess</strong> enters your conversation — maybe 1 KB</li>
<li><strong>The raw 59 KB never touches your context</strong></li>
</ol>

<p>Think of it like a mail room. Instead of delivering every package directly to your desk (cluttering your workspace), there&#x27;s a mail room that opens packages, pulls out the important documents, and delivers just those. The boxes stay in the mail room.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to install Context Mode as a Claude Code plugin and verify it&#x27;s working
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>The article mentions two install methods — plugin marketplace vs MCP-only. Which gives you auto-routing?</li><li>After installing, how would you verify the MCP server is running and the hooks are active?</li><li>What should change in your Claude Code configuration files?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should walk you through running the plugin install command (it&#x27;s a one-liner via the Claude Code plugin marketplace — <code>/install-plugin context-mode</code> or adding it to your MCP config). After install, you should see a new entry in your MCP settings and a PreToolUse hook registered. The agent should verify by running <code>/mcp</code> to confirm the <code>context-mode</code> server appears with tools like <code>execute</code>, <code>batch_execute</code>, <code>index</code>, <code>search</code>, and <code>fetch_and_index</code>. Restart Claude Code if needed.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        Context Mode spawns subprocesses that inherit your environment variables for credential passthrough. This means <code>gh</code>, <code>aws</code>, <code>gcloud</code>, etc. work inside the sandbox — but it also means the sandbox has access to your credentials. This is by design (tools need auth to work), but worth understanding.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Context Mode auto-detects Bun for 3-5x faster JavaScript/TypeScript execution in the sandbox. If you have Bun installed, you&#x27;ll get faster processing for free.
      </div>
        
      <details class="reveal">
        <summary>How does credential passthrough actually work?</summary>
        <div class="reveal-body"><p>When Context Mode spawns a subprocess, it inherits the parent process&#x27;s environment variables — things like <code>GITHUB_TOKEN</code>, <code>AWS_ACCESS_KEY_ID</code>, your <code>~/.config/gh</code> directory, <code>~/.kube/config</code>, etc. The subprocess can use these to authenticate CLI calls (like <code>gh issue list</code>), but the credentials themselves never appear in the stdout that enters your conversation. It&#x27;s the same mechanism as running a script in your terminal — the script can use your auth, but it doesn&#x27;t print your tokens. The key security boundary: scripts in the sandbox can&#x27;t access each other&#x27;s memory or state, and only stdout crosses back into the conversation.</p></div>
      </details>
      <details class="reveal">
        <summary>What are the 10 supported language runtimes?</summary>
        <div class="reveal-body"><p>JavaScript, TypeScript, Python, Shell, Ruby, Go, Rust, PHP, Perl, and R. The sandbox detects which runtime to use based on the script you pass to <code>execute</code>. For JS/TS, it prefers Bun if available (significantly faster startup than Node). For Python, it uses whatever <code>python3</code> is on your PATH. Shell scripts run in Bash. The compiled languages (Go, Rust) actually compile and run in the subprocess — which is wild but works because the subprocess lifecycle is managed by Context Mode.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point you should have Context Mode installed as a Claude Code plugin, with the MCP server showing up when you run `/mcp`. You should see tools like `execute`, `batch_execute`, `index`, `search`, and `fetch_and_index` available. If not, try restarting Claude Code — MCP servers sometimes need a fresh start to register.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>See the Compression in Action: Raw vs. Sandboxed Output</h2>
      </div>
      <div class="step-body">
        <p>Alright, let&#x27;s actually <em>see</em> the difference. This is the part that sold me.</p>

<p>The best way to internalize what Context Mode does is to run the same operation twice — once letting the raw output flood your context, once with Context Mode intercepting it. We&#x27;ll use a real-world task: looking at GitHub issues.</p>

<p>Without Context Mode, <code>gh issue list --limit 20</code> dumps the full JSON for 20 issues into your conversation. That&#x27;s titles, bodies, labels, assignees, timestamps, URLs — roughly 59 KB. Your agent reads all 59 KB, but probably only needs a handful of fields.</p>

<p>With Context Mode, the same command runs inside a sandbox. A script processes the output, extracts what&#x27;s relevant (title, status, key labels), and returns maybe 1.1 KB. The agent gets everything it needs to answer your question, and your context window barely notices.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to demonstrate the before/after by running a GitHub issue listing (or similar data-heavy command) and comparing the context impact
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>How would you measure the &#x27;size&#x27; of a tool output in your context? Token count? Character count?</li><li>What&#x27;s a command you run regularly that returns a lot of data? Git log? Test results? API responses?</li><li>Ask the agent to run the same task with and without the sandbox — what should you compare?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should run something like <code>gh issue list</code> or <code>git log --oneline -50</code> both ways. Without Context Mode, it&#x27;ll note the raw output size (e.g., &#x27;59 KB of JSON just entered our conversation&#x27;). With Context Mode&#x27;s <code>execute</code> tool, the same data gets processed in the sandbox and only a compact summary comes back — maybe a clean table of issue titles and statuses in under 2 KB. The agent should explicitly call out the size difference: &#x27;That was 59 KB raw → 1.1 KB processed. Your context window just saved 57.9 KB on a single command.&#x27;</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The auto-routing hook means you don&#x27;t have to manually choose between raw and sandboxed execution. Once installed, Context Mode intercepts tool outputs automatically. But for this demo, it&#x27;s worth seeing both modes to appreciate what&#x27;s happening.
      </div>
        
      <details class="reveal">
        <summary>The full compression numbers from the article</summary>
        <div class="reveal-body"><p>Here&#x27;s the validated data across 11 real-world scenarios:</p>

<ul>
<li><strong>Playwright snapshot:</strong> 56 KB → 299 bytes (99.5% reduction)</li>
<li><strong>GitHub issues (20):</strong> 59 KB → 1.1 KB (98.1%)</li>
<li><strong>Access log (500 requests):</strong> 45 KB → 155 bytes (99.7%)</li>
<li><strong>Analytics CSV (500 rows):</strong> 85 KB → 222 bytes (99.7%)</li>
<li><strong>Git log (153 commits):</strong> 11.6 KB → 107 bytes (99.1%)</li>
<li><strong>Repo research (subagent):</strong> 986 KB → 62 KB with 5 calls vs 37 (93.7%)</li>
</ul>

<p>The repo research case is interesting — it&#x27;s not just about compressing output, it&#x27;s about reducing the <em>number</em> of tool calls too. The subagent uses <code>batch_execute</code> to do multiple operations in one sandbox call instead of making 37 separate tool invocations.</p></div>
      </details>
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">A Playwright browser snapshot is 56 KB of raw HTML/accessibility tree data. Context Mode compresses this to 299 bytes. What&#x27;s the most likely way it achieves this?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt0">
          <label for="decision_6_opt0">It runs a script in the sandbox that extracts only the relevant elements (visible text, interactive elements, key structure) and returns a compact summary</label>
          <div class="decision-feedback correct">&#10003; Correct! Exactly right. The sandbox runs a processing script that analyzes the 56 KB snapshot, pulls out what the agent actually needs to reason about (page title, main content, button labels, form fields, error messages), and returns just that. The full DOM never enters the context. This is the core insight — your agent doesn&#x27;t need raw data, it needs processed intelligence.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt1">
          <label for="decision_6_opt1">It uses gzip compression on the raw output before passing it to the context window</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Nope — the context window works with tokens (text), not compressed binary data. You can&#x27;t gzip a string and pass it to an LLM. The compression here is semantic, not algorithmic. A script in the sandbox reads the 56 KB, understands what matters, and produces a 299-byte text summary. It&#x27;s closer to a human reading a report and writing a one-line takeaway than it is to zipping a file.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt2">
          <label for="decision_6_opt2">It truncates the output to the first 299 bytes</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Truncation would give you the first 299 bytes of raw HTML — probably just the DOCTYPE declaration and opening tags. Completely useless. Context Mode is smarter: it processes the *entire* 56 KB in the sandbox, understands the structure, and returns a curated 299-byte summary of what actually matters. The processing happens outside your context; only the result enters it.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>The Knowledge Base: Index Once, Search Forever</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s the part that goes beyond just compressing outputs — Context Mode has a built-in knowledge base powered by SQLite FTS5.</p>

<p>Let me break this down with an analogy. Imagine you&#x27;re researching a topic and you keep opening browser tabs. Without a system, you re-read the same articles every time you need a fact. With a personal wiki, you read each article once, jot down the key points, and search your wiki later.</p>

<p>Context Mode&#x27;s knowledge base works the same way:</p>

<ol>
<li><strong><code>index</code></strong> — Takes markdown content, splits it into chunks by headings (keeping code blocks intact), and stores them in a SQLite FTS5 table</li>
<li><strong><code>search</code></strong> — Queries the index using BM25 ranking and returns exact matching chunks</li>
<li><strong><code>fetch_and_index</code></strong> — Fetches a URL, converts HTML to markdown, and indexes it — the raw page never enters your context</li>
</ol>

<p><strong>BM25 ranking</strong> is worth understanding. It&#x27;s the same algorithm that powered early Google. It scores each chunk based on three things: how often your search terms appear in the chunk (term frequency), how rare those terms are across all chunks (inverse document frequency), and how long the chunk is (shorter chunks with your terms score higher). It also uses <strong>Porter stemming</strong>, so searching for &quot;running&quot; also matches &quot;runs&quot; and &quot;ran&quot;.</p>

<p>The killer use case: index your project&#x27;s documentation at the start of a session. Every time the agent needs to reference docs, it searches the knowledge base instead of re-fetching and re-reading the full docs. Zero context cost for repeated lookups.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to index some documentation (a README, API docs, or a wiki page) into Context Mode&#x27;s knowledge base and then search it
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What documentation do you reference repeatedly in your projects? README? API docs? Style guides?</li><li>How would you use `fetch_and_index` for a URL vs. `index` for local content?</li><li>What search queries would test whether the indexing actually works — try something that needs stemming?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should use <code>fetch_and_index</code> with a documentation URL (or <code>index</code> with local markdown content) and confirm the content was chunked and indexed. Then it should run a <code>search</code> query and show that it returns the exact relevant chunk — with its heading hierarchy preserved. For example, searching &#x27;authentication&#x27; in your project docs might return a chunk under &#x27;API Reference &gt; Auth Endpoints&#x27; with the exact code example from the docs. The key thing to notice: the full docs page (maybe 50 KB) was fetched and indexed <em>outside</em> your context, and the search result is maybe 500 bytes of exactly what you needed.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The knowledge base persists across the session. Index your most-referenced docs once at the start, then search freely. Think of it as giving your agent a personal reference library that doesn&#x27;t cost context to consult.
      </div>
        
      <details class="reveal">
        <summary>BM25 vs. vector/embedding search — why not use embeddings?</summary>
        <div class="reveal-body"><p>You might be wondering: why BM25 (a keyword algorithm from the &#x27;90s) instead of fancy vector embeddings? A few reasons:</p>

<ol>
<li><strong>No external API needed</strong> — BM25 runs entirely in SQLite. No embedding model, no API calls, no latency.</li>
<li><strong>Exact matching</strong> — When you search for a function name like <code>handleAuthCallback</code>, you want exact matches, not semantically similar results. BM25 excels at this.</li>
<li><strong>Transparent scoring</strong> — You can understand <em>why</em> a result ranked high (the terms matched, they&#x27;re rare, the chunk is focused). Embedding similarity is a black box.</li>
<li><strong>Speed</strong> — FTS5 queries are sub-millisecond on typical knowledge bases.</li>
</ol>

<p>For a context-window optimization tool, these tradeoffs make perfect sense. You&#x27;re not doing open-ended semantic search — you&#x27;re looking up specific documentation chunks. BM25 is the right tool for this job.</p></div>
      </details>
      <details class="reveal">
        <summary>How does the chunking work?</summary>
        <div class="reveal-body"><p>The <code>index</code> tool splits markdown by headings (H1, H2, H3, etc.) so each chunk is a logical section of the document. Crucially, code blocks are kept intact — a code example is never split across chunks. Each chunk stores its heading hierarchy (e.g., &#x27;Getting Started &gt; Installation &gt; npm&#x27;) so when you get a search result, you know exactly where in the document it came from. This is way better than naive fixed-size chunking (splitting every 500 characters) because you get semantically coherent results.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You&#x27;ve now seen three capabilities: sandboxed execution (compress tool outputs), knowledge base indexing (store docs outside context), and search (retrieve just what you need). Together, these turn your 30-minute agent sessions into 3-hour sessions. The context window that used to be 60% full after 45 minutes is now 99% free.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>The Invisible Part: Auto-Routing and Subagent Upgrades</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s what makes Context Mode practical instead of just cool: you don&#x27;t have to change how you work.</p>

<p>When you install the full plugin (not just the MCP server), it registers a <strong>PreToolUse hook</strong>. This hook intercepts tool outputs <em>before</em> they enter your context and routes them through the sandbox automatically. You don&#x27;t have to remember to use <code>execute</code> instead of <code>bash</code>. You just work normally, and the compression happens behind the scenes.</p>

<p>The subagent upgrade is the other subtle win. Normally, when Claude Code spawns a subagent (like a Bash-only agent for running commands), that subagent can only use bash tools. Context Mode upgrades these to <code>general-purpose</code> agents that can access MCP tools — meaning subagents also get the context compression benefits. They use <code>batch_execute</code> to do multiple operations in a single sandbox call, which is why the repo research scenario dropped from 37 tool calls to 5.</p>

<p>Think of it like installing an ad blocker. After setup, you just browse normally. The blocking happens in the background, and you occasionally notice pages load faster. Same energy here — you just code normally, and your sessions last 6x longer.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain what hooks are registered and do a real task that would normally burn a lot of context — like reviewing a git diff or running a test suite — to see auto-routing in action
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s a task you regularly do that generates lots of output? Test suites? Build logs? Code reviews?</li><li>How would you verify that the output was actually routed through the sandbox vs. dumped raw?</li><li>Ask the agent to compare how much context this task would have cost without Context Mode</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should run a data-heavy task — a test suite, a large git diff review, or a multi-file code analysis — and you&#x27;ll notice the output is compact and focused. The agent should confirm that the PreToolUse hook intercepted the output (you might see it in the tool call metadata or the agent can explain what happened). The key metric: &#x27;This git diff was 23 KB raw. The sandbox extracted the 4 changed functions, summarized the modifications, and returned 800 bytes. Your context saved 22.2 KB on this one operation.&#x27; Over a session with 20-30 tool calls, those savings compound to hundreds of KB.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The <code>batch_execute</code> tool is especially powerful for subagents doing research. Instead of 37 separate tool calls (each adding overhead to the context), a single <code>batch_execute</code> runs all operations in one sandbox and returns one compact result. Fewer calls = less context overhead = longer sessions.
      </div>
        
      <details class="reveal">
        <summary>What exactly is a PreToolUse hook?</summary>
        <div class="reveal-body"><p>In Claude Code&#x27;s plugin system, hooks are functions that run at specific points in the tool execution lifecycle. A <strong>PreToolUse</strong> hook runs <em>before</em> a tool&#x27;s output enters the conversation context. It can intercept, modify, or redirect the output. Context Mode&#x27;s hook checks: &#x27;Is this tool output larger than a threshold? If so, route it through the sandbox for compression before it enters context.&#x27; Other hook points exist too (PostToolUse, PreMessage, etc.), but PreToolUse is the critical one for context optimization because it catches the data before it&#x27;s too late.</p></div>
      </details>
      </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Set up a &#x27;session starter&#x27; prompt that indexes your project&#x27;s key documentation and configures Context Mode for maximum efficiency at the beginning of every coding session</div>
      <div class="your-turn-context">The biggest win with Context Mode isn&#x27;t any single compressed output — it&#x27;s the compound effect over a full session. If you index your project docs, API references, and style guides at session start, every subsequent lookup is free. Combined with auto-routing, your sessions go from 30 minutes to 3 hours before slowdown. The challenge: create a reusable prompt that sets this up for your specific project.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What documentation does your agent reference most often? README? API docs? Contributing guide? Architecture docs?</li><li>Should you index remote URLs (fetch_and_index) or local files (index)? What about both?</li><li>How would you verify the knowledge base is working — what&#x27;s a good test query?</li><li>Could you save this as a Claude Code slash command or custom instruction so it runs automatically?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>At the start of this session, please set up my knowledge base for efficient context usage:

1. Use fetch_and_index to index our API documentation at [docs-url]
2. Use index to add the content of our README.md and CONTRIBUTING.md to the knowledge base
3. Index the architecture decision records in docs/adr/
4. Run a test search for &#x27;authentication&#x27; to verify the knowledge base is working
5. Confirm how many chunks were indexed and estimate the context savings vs. re-reading these docs every time

For the rest of this session, whenever I ask about our API or project conventions, search the knowledge base first instead of re-reading the source files.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re about to start a long debugging session. Your project has 50 KB of API docs, a 30 KB changelog, and you&#x27;ll be running test suites that output ~40 KB each. Without Context Mode, roughly how much context would be consumed by tool outputs after 5 test runs and 3 doc lookups?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt0">
          <label for="decision_11_opt0">~350 KB — likely exceeding your context window</label>
          <div class="decision-feedback correct">&#10003; Correct! Let&#x27;s do the math: 5 test runs × 40 KB = 200 KB, plus 3 doc lookups × 50 KB = 150 KB (assuming the full docs are re-read each time). That&#x27;s 350 KB of tool output alone — well past the 200K token limit (roughly 150 KB of text). Your session would be crippled long before the 5th test run. With Context Mode, those same operations might cost ~5 KB total. That&#x27;s the difference between a session that dies after 20 minutes and one that lasts all afternoon.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt1">
          <label for="decision_11_opt1">~50 KB — plenty of room left</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This would only be true if each tool call&#x27;s output magically disappeared after use. It doesn&#x27;t — in the context window model, every tool output stays in the conversation for the entire session. Five 40 KB test runs plus repeated doc reads compounds fast. This is exactly the sneaky problem that makes people wonder why their agent &#x27;got dumber&#x27; halfway through a session.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt2">
          <label for="decision_11_opt2">~120 KB — tight but manageable</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. You might get this number if you assumed each doc lookup doesn&#x27;t re-read the full docs, but in practice, without a knowledge base, the agent re-reads the source each time it needs to reference documentation. And test suite outputs are fully dumped into context on every run. The compound effect is worse than most people expect — which is exactly why Context Mode&#x27;s numbers feel so dramatic.</div>
        </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>Let&#x27;s zoom out. We just installed a tool that fundamentally changes the economics of AI-assisted coding sessions.</p>

<p>The core insight isn&#x27;t about compression algorithms or SQLite tricks — it&#x27;s about <strong>where data lives</strong>. Your context window is precious real estate. Every byte of raw tool output that sits there is a byte your agent can&#x27;t use for reasoning about your actual problem. Context Mode introduces a simple but powerful boundary: raw data stays in the sandbox, processed intelligence enters the conversation.</p>

<p>Three capabilities, one philosophy:</p>
<ul>
<li><strong>Sandbox execution</strong> — tool outputs get processed in isolated subprocesses; only compact results enter context</li>
<li><strong>Knowledge base</strong> — index docs once, search for free forever; no repeated context cost for lookups</li>
<li><strong>Auto-routing</strong> — hooks intercept outputs automatically so you don&#x27;t change how you work</li>
</ul>

<p>The result: sessions that last 6x longer, agents that stay sharp because they&#x27;re not wading through 200 KB of raw git diffs, and a workflow that feels exactly the same from your perspective. That&#x27;s the best kind of infrastructure — invisible until you notice everything just... works better.</p></div>
      <ul class="takeaways-list"><li>Every MCP tool output stays in your context window for the entire session — they compound, and 30 minutes of tool use can consume 40%+ of your context</li><li>Context Mode&#x27;s sandbox processes raw data in isolated subprocesses and only returns compact results — 315 KB becomes 5.4 KB (98% reduction)</li><li>The SQLite FTS5 knowledge base with BM25 ranking lets you index documentation once and search it for free — no embeddings, no API calls, sub-millisecond queries</li><li>Auto-routing via PreToolUse hooks means you don&#x27;t change how you work — the compression happens invisibly behind every tool call</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Index your most-referenced project docs at session start and measure how much context you save over a typical session</li><li>Try <code>batch_execute</code> for multi-step research tasks — fewer tool calls means less context overhead and faster results</li><li>Explore combining Context Mode with Cloudflare&#x27;s Code Mode for tool <em>definition</em> compression — together they optimize both sides of the MCP context equation</li><li>Watch the Context Mode GitHub repo for new processing scripts — the community is adding domain-specific compressors for common tool outputs</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://mksg.lu/blog/context-mode" target="_blank" rel="noopener">Stop Burning Your Context Window – How We Cut MCP Output by 98% in Claude Code</a> <span class="source-name">(Hacker News AI)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">VectifyAI/PageIndex</div>
            <div class="oa-summary">📑 PageIndex: Document Index for Vectorless, Reasoning-based RAG PageIndex: Vectorless, Reasoning-based RAG...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">📚 《从零开始构建智能体》——从零开始的智能体原理与实践教程 English | 中文 Hello-Agents 🤖 《从零开始构建智能体》 从基础理论到实际应用，全面掌握智能体系统的设计与实现 🎯 项目介绍 如果说 2024...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">ruvnet/wifi-densepose</div>
            <div class="oa-summary">Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">4thfever/cultivation-world-simulator</div>
            <div class="oa-summary">基于 AI Agent 工作流的修仙世界模拟器，旨在还原智能、开放的仙侠世界。| An open-source Cultivation World Simulator using Agentic Workflow to create...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">muratcankoylan/Agent-Skills-for-Context-Engineering</div>
            <div class="oa-summary">A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Public repository for Agent Skills Note: This repository contains Anthropic&#x27;s implementation of skills for Claude....</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">QwenLM/Qwen-Agent</div>
            <div class="oa-summary">Agent framework and applications built upon Qwen&gt;=3.0, featuring Function Calling, MCP, Code Interpreter, RAG,...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">NevaMind-AI/memU</div>
            <div class="oa-summary">Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot). memU 24/7 Always-On Proactive Memory for AI...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">modelscope/ms-swift</div>
            <div class="oa-summary">Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3.5, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">agentscope-ai/agentscope</div>
            <div class="oa-summary">Build and run agents you can see, understand and trust. 中文主页 | Tutorial | Roadmap (Jan 2026 -) | FAQ What is...</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x1F44D;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x1F44E;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "VectifyAI/PageIndex", "tags": "agents,tools,rag", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "ruvnet/wifi-densepose", "tags": "tools,rag", "source": "GitHub Trending Python"}, {"title": "4thfever/cultivation-world-simulator", "tags": "agents,open-source", "source": "GitHub Trending Python"}, {"title": "muratcankoylan/Agent-Skills-for-Context-Engineering", "tags": "agents,tools,technique", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "QwenLM/Qwen-Agent", "tags": "agents,open-source,tools,coding,rag", "source": "GitHub Trending Python"}, {"title": "NevaMind-AI/memU", "tags": "agents,tools", "source": "GitHub Trending Python"}, {"title": "modelscope/ms-swift", "tags": "tools,fine-tuning", "source": "GitHub Trending Python"}, {"title": "agentscope-ai/agentscope", "tags": "agents,tools,technique,audio,rag", "source": "GitHub Trending Python"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "general";
      var date = "2026-02-28";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>