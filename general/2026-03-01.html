<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Vectorless RAG with PageIndex: Ditch Your Vector DB — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #FAFAFA;
  --bg-subtle: #F4F4F5;
  --bg-elevated: #FFFFFF;
  --ink: #18181B;
  --ink-secondary: #52525B;
  --muted: #A1A1AA;
  --accent: #E5484D;
  --accent-hover: #CD2B31;
  --accent-light: rgba(229,72,77,0.06);
  --accent-subtle: rgba(229,72,77,0.12);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #F4F4F5;
  --border: #E4E4E7;
  --border-subtle: #F4F4F5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default coral — uses :root values */ }
body.track-image-gen { --accent: #8B5CF6; --accent-hover: #7C3AED; --accent-light: rgba(139,92,246,0.06); --accent-subtle: rgba(139,92,246,0.12); }
body.track-audio { --accent: #F59E0B; --accent-hover: #D97706; --accent-light: rgba(245,158,11,0.06); --accent-subtle: rgba(245,158,11,0.12); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(59,130,246,0.25);
  background: rgba(59,130,246,0.04);
}
.callout-warning {
  border-color: rgba(239,68,68,0.4);
  background: rgba(239,68,68,0.04);
}
.callout-api-key-note {
  border-color: rgba(234,179,8,0.3);
  background: rgba(234,179,8,0.05);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--blue); }
.callout-warning .callout-label { color: var(--red); }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: var(--green);
  background: rgba(34,197,94,0.06);
  color: #15803D;
}
.decision-feedback.incorrect {
  border-color: var(--red);
  background: rgba(239,68,68,0.05);
  color: #DC2626;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(34,197,94,0.1);
  border-color: var(--green);
  color: var(--green);
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(239,68,68,0.08);
  border-color: var(--red);
  color: var(--red);
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--surface);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-general">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Vectorless RAG with PageIndex: Ditch Your Vector DB</h1>
      <div class="hero-subtitle">What if your retrieval system could think instead of just pattern-match? Let&#x27;s build one that navigates documents the way a human expert actually reads them.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">RAG</span> <span class="tag">retrieval</span> <span class="tag">reasoning</span> <span class="tag">document-analysis</span> <span class="tag">PageIndex</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Here&#x27;s something that&#x27;s been bugging a lot of folks building RAG systems: <strong>vector similarity search is a vibe check, not a relevance check.</strong></p>

<p>You embed your documents into vectors, embed your query, and find the closest match by cosine similarity. Sounds elegant. But &quot;closest in meaning-space&quot; and &quot;actually answers the question&quot; are surprisingly different things — especially with professional documents like financial reports, legal contracts, or medical records.</p>

<p>PageIndex, a project that&#x27;s been trending on GitHub, takes a completely different approach. Instead of embedding chunks into a vector database, it builds a <strong>hierarchical tree index</strong> — basically a smart table of contents — and then uses an LLM to <strong>reason</strong> its way to the right sections. Think less &quot;Google search&quot; and more &quot;experienced analyst flipping through a 200-page report.&quot;</p>

<p>The kicker? It hit <strong>98.7% accuracy on FinanceBench</strong>, a benchmark for financial document Q&amp;A. That&#x27;s state-of-the-art, and it did it without a single vector embedding.</p>

<p>Let&#x27;s understand why this works and build one ourselves.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Why Vector RAG Breaks Down (and What to Do Instead)</h2>
      </div>
      <div class="step-body">
        <p>Before we build anything, let&#x27;s really understand the problem we&#x27;re solving. This isn&#x27;t just academic — if you&#x27;ve ever built a RAG system and gotten frustratingly wrong answers, this is probably why.</p>

<p>Imagine you&#x27;re a financial analyst and someone asks: <em>&quot;Did Company X&#x27;s revenue grow faster than their operating expenses in Q3?&quot;</em></p>

<p>With vector RAG, your system embeds that question and hunts for chunks that are <em>semantically similar</em>. It might find a paragraph mentioning &quot;revenue&quot; and another mentioning &quot;expenses&quot; — but maybe from different quarters, or maybe it grabs a forward-looking statement instead of the actuals. The vectors are <em>close</em>, but the answer is <em>wrong</em>.</p>

<p>A human expert would handle this differently. They&#x27;d go to the table of contents, find the income statement section, locate Q3 specifically, and compare two line items. That&#x27;s <strong>reasoning-based retrieval</strong> — and that&#x27;s exactly what PageIndex does.</p>

<p>Let&#x27;s get your agent to help us really nail this distinction.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain the fundamental difference between similarity-based and reasoning-based retrieval, with a concrete example showing where similarity fails.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What kind of document would make a good example — one where similar-sounding passages could be misleading?</li><li>Can you think of a query where the most semantically similar chunk is NOT the most relevant one?</li><li>How would a human expert approach the same question differently from a search engine?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should walk you through a concrete scenario — say, a 100-page annual report where a question about &#x27;net income trends&#x27; could match dozens of paragraphs (CEO letter, risk factors, footnotes) but only the actual income statement table is relevant. It should explain that similarity search treats all these matches roughly equally, while reasoning-based retrieval understands that you need <em>the financial statements section, specifically the comparative income table</em>. The key insight: reasoning can follow multi-step logic (&#x27;I need income → that&#x27;s in financials → specifically the P&amp;L → the year-over-year comparison row&#x27;), while similarity just measures distance in embedding space.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        A great mental model: vector search is like searching your email by keyword — it finds messages that <em>contain</em> the right words. Reasoning-based retrieval is like asking your assistant who <em>understands</em> your inbox to find the right email. Same data, very different approach.
      </div>
        
      <details class="reveal">
        <summary>Wait, what about hybrid search and reranking?</summary>
        <div class="reveal-body"><p>Good question — the RAG community has been patching vector search with layers like hybrid search (combining keyword + vector), reranking models, and query decomposition. These help! But they&#x27;re still fundamentally working with chunks ripped from context. PageIndex&#x27;s argument is: why patch a lossy retrieval method when you can use the document&#x27;s <em>actual structure</em> as your index? It&#x27;s like the difference between improving your Google search skills vs. hiring a librarian who knows the collection.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>The Tree Index: Building a Brain Map of Your Document</h2>
      </div>
      <div class="step-body">
        <p>Okay, so PageIndex doesn&#x27;t use vectors. What <em>does</em> it use?</p>

<p>The core idea is beautifully simple: <strong>build a hierarchical table of contents</strong> that captures the document&#x27;s structure, then let an LLM navigate it like a tree.</p>

<p>Think about how a real book&#x27;s table of contents works. It&#x27;s a tree:</p>

<ul>
<li><strong>Chapter 3: Financial Statements</strong></li>
<li>3.1 Income Statement</li>
<li>3.2 Balance Sheet</li>
<li>3.2.1 Current Assets</li>
<li>3.2.2 Long-term Liabilities</li>
<li>3.3 Cash Flow Statement</li>
</ul>

<p>PageIndex generates something like this automatically from your documents — but richer, with page references and content summaries at each node. No chunking into arbitrary 500-token blocks. No embedding anything. Just... understanding the document&#x27;s natural structure.</p>

<p>This is the first of PageIndex&#x27;s two steps. Let&#x27;s set it up.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to help you set up PageIndex and build a tree index from a PDF document.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What library do we need to install, and what API key does PageIndex require to work?</li><li>What kind of document should we test with — something short and simple, or something that actually has structure?</li><li>What do you think the output of indexing looks like — a flat list? A nested structure?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you the pip install command (<code>pip install pageindex</code>) and explain you&#x27;ll need an OpenAI API key (PageIndex uses LLMs under the hood for the reasoning steps). The core setup is tiny — you create a <code>PageIndex</code> instance with your API key, then call a build method on a PDF file. The agent should suggest using a real structured document like a financial report or technical whitepaper for testing. The output is a tree structure where each node has a title, page range, summary, and children — like a rich, auto-generated table of contents.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        PageIndex uses OpenAI&#x27;s API under the hood for its reasoning steps. You&#x27;ll need an <code>OPENAI_API_KEY</code> set in your environment. The indexing step does consume tokens — a 50-page PDF might use a few thousand tokens to index. After that, each query is relatively cheap.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        No chunking is a bigger deal than it sounds. Traditional RAG forces you to decide chunk size (200 tokens? 500? 1000?) and overlap — and getting this wrong silently destroys retrieval quality. PageIndex uses the document&#x27;s <em>natural</em> sections. A paragraph stays a paragraph. A table stays a table. The document tells you where the boundaries are.
      </div>
        
      <details class="reveal">
        <summary>How does it figure out the document structure automatically?</summary>
        <div class="reveal-body"><p>PageIndex analyzes the document&#x27;s formatting cues — headings, font sizes, numbering patterns, page breaks — to reconstruct a hierarchical structure. Think of it like reverse-engineering a table of contents from the document itself. For well-structured PDFs (reports, papers, manuals), this works remarkably well because those documents were <em>designed</em> to be navigable. For messy documents, there&#x27;s also a vision-based mode that processes page images directly, which can handle scanned documents and complex layouts.</p></div>
      </details>
      <details class="reveal">
        <summary>What does the tree actually look like in practice?</summary>
        <div class="reveal-body"><p>Imagine a JSON tree where each node has: a <code>title</code> (like &#x27;Revenue Recognition Policy&#x27;), a <code>page_range</code> (pages 34-37), a <code>summary</code> (a one-line description of what&#x27;s in that section), and <code>children</code> (sub-sections). The root node represents the whole document, level 1 might be chapters, level 2 sections, level 3 subsections, and so on. When the LLM searches this tree, it reads the summaries to decide which branches to explore — just like scanning a table of contents.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point, you should understand two things: (1) why similarity ≠ relevance in retrieval, and (2) how PageIndex creates a structured tree index from a document instead of chunking it into vectors. If the tree index concept feels fuzzy, re-read the table-of-contents analogy — that&#x27;s literally what&#x27;s happening.</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">A user asks: &#x27;What were the risk factors that could affect revenue growth?&#x27; The document has a Risk Factors section AND a Revenue Discussion section, both mentioning revenue risks. Which retrieval approach handles this better?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt0">
          <label for="decision_5_opt0">Reasoning-based retrieval — it can check both sections and understand which is more relevant to the specific question</label>
          <div class="decision-feedback correct">&#10003; Correct! Exactly right. A reasoning-based system can navigate to both sections, read the summaries, and reason that the Risk Factors section contains the *formal risk disclosures* while the Revenue Discussion mentions risks only in passing. It might even pull from both, understanding the different contexts. Vector search would just return whichever chunks happen to have the highest cosine similarity to &#x27;risk factors revenue growth&#x27; — which could easily be the wrong one.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt1">
          <label for="decision_5_opt1">Vector search — it would find chunks from both sections and return them all</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Vector search *would* find chunks from both sections, but that&#x27;s actually the problem. It returns them ranked by similarity score, not by actual relevance to the question. It can&#x27;t reason about *why* the Risk Factors section is the primary answer while the Revenue Discussion is supplementary context. You&#x27;d get a pile of similar-sounding chunks with no understanding of which ones actually answer the question.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt2">
          <label for="decision_5_opt2">They&#x27;d perform about the same — both would find the right content</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This seems reasonable but underestimates the gap. For simple, single-hop questions (&#x27;What was Q3 revenue?&#x27;), both approaches work fine. But for nuanced questions requiring judgment about which section is *more relevant*, reasoning-based retrieval has a significant edge. The 98.7% vs. lower accuracy on FinanceBench exists precisely because financial questions often require this kind of contextual judgment.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Tree Search: Teaching the LLM to Navigate</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s where the magic happens. You&#x27;ve got your tree index — a hierarchical map of the document. Now you need to <em>search</em> it.</p>

<p>PageIndex&#x27;s tree search works like a game of 20 questions with the document. Starting from the root:</p>

<ol>
<li><strong>Look at the top-level sections</strong> — &quot;Hmm, this question about debt covenants... that&#x27;s probably in Financial Statements or Notes to Financial Statements, not in the Company Overview.&quot;</li>
<li><strong>Drill into the most promising branch</strong> — &quot;Within Notes to Financial Statements, there&#x27;s a Debt Obligations subsection. Let me check that.&quot;</li>
<li><strong>Keep going until you hit the content</strong> — &quot;Found it — Note 12: Long-term Debt, page 87-89. This has the covenant details.&quot;</li>
</ol>

<p>This is <em>exactly</em> how AlphaGo works, by the way — evaluating which branches of a game tree are worth exploring. PageIndex was literally inspired by that approach. Instead of evaluating board positions, it&#x27;s evaluating document sections.</p>

<p>The beautiful part? Each navigation step is an LLM call with reasoning. You can actually <em>read</em> why it chose each branch. Try getting that kind of explainability from a cosine similarity score.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to build a complete query pipeline: take a question, search the tree index, retrieve the relevant pages, and generate an answer with citations.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What are the two main steps after you have the index — finding the right sections and then doing what with them?</li><li>How should the final answer reference where it found the information?</li><li>What happens if the answer spans multiple sections of the document?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a simple pipeline with two stages: (1) use PageIndex&#x27;s search/query method to find relevant sections from the tree index — this returns page numbers and section references, and (2) feed those specific pages to an LLM along with the original question to generate a grounded answer. The key difference from vector RAG is visible here: instead of &#x27;here are the top-5 chunks by similarity,&#x27; you get &#x27;here are the specific sections the model reasoned its way to, with page numbers.&#x27; The answer should include citations like &#x27;According to Section 3.2 (pages 34-37)...&#x27; rather than vague references.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The explainability angle is huge for regulated industries. If an auditor asks &#x27;why did your system say the company&#x27;s debt ratio is 2.3x?&#x27;, you can show the exact reasoning path: &#x27;It navigated to Financial Statements → Balance Sheet → found total debt on page 45, then to Income Statement → found EBITDA on page 42, and computed the ratio.&#x27; Try doing that with vector similarity scores.
      </div>
        
      <details class="reveal">
        <summary>How many LLM calls does a tree search take?</summary>
        <div class="reveal-body"><p>It depends on the depth of the tree, but typically 2-4 LLM calls per query. Level 1: pick the right chapter (one call). Level 2: pick the right section within that chapter (one call). Level 3: pick the right subsection (one call). Then a final call to generate the answer from the retrieved content. That&#x27;s more calls than a single vector lookup, but each call is small and fast — you&#x27;re just asking the LLM to read a few summaries and pick the best one. The total latency is usually comparable to vector RAG with reranking, and the accuracy is significantly higher.</p></div>
      </details>
      <details class="reveal">
        <summary>What about the AlphaGo connection?</summary>
        <div class="reveal-body"><p>AlphaGo uses Monte Carlo Tree Search (MCTS) to explore a game tree — it doesn&#x27;t evaluate every possible move, just the most promising branches. PageIndex applies the same principle to document retrieval. The tree is the document structure, the &#x27;moves&#x27; are choosing which section to explore, and the &#x27;evaluation function&#x27; is the LLM&#x27;s reasoning about relevance. Just like AlphaGo doesn&#x27;t need to see every board position to play well, PageIndex doesn&#x27;t need to compare against every document chunk to find the right answer.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Putting It All Together: A Complete Vectorless RAG Pipeline</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s zoom out and see the whole picture. We&#x27;ve got all the pieces — now let&#x27;s connect them into something you could actually use.</p>

<p>A traditional RAG pipeline has: <strong>Document → Chunker → Embedder → Vector DB → Retriever → LLM → Answer</strong></p>

<p>That&#x27;s six components, three of which (chunker, embedder, vector DB) are infrastructure you have to host, tune, and maintain.</p>

<p>The PageIndex pipeline is: <strong>Document → Tree Indexer → Tree Searcher → LLM → Answer</strong></p>

<p>Four components. No vector DB to host. No embedding model to choose and fine-tune. No chunk size to agonize over. Honestly, the simplicity is what sold me.</p>

<p>Let&#x27;s get your agent to build the end-to-end thing.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to build a complete end-to-end script: load a PDF, build the tree index, answer a question using tree search, and print the answer with page references.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s the simplest possible version of this — just a script that takes a PDF path and a question?</li><li>Should we save the index so we don&#x27;t rebuild it every time we ask a new question?</li><li>What error cases should we handle — what if the document has no clear structure?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a clean, minimal script. The flow is: (1) initialize PageIndex with your OpenAI API key, (2) call the index-building method on your PDF — and importantly, save the result so you can reuse it, (3) call the search/query method with a user question, which returns relevant sections with page numbers, (4) pass those sections plus the question to an LLM for the final answer. The whole thing should be maybe 15-20 lines. Ask the agent to also show you what the intermediate output looks like — the tree structure and the search results — so you can see the reasoning chain.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        Index building is the expensive step — it reads and analyzes the whole document. Always save/cache your index. Querying against an existing index is fast and cheap. Think of it like: indexing is &#x27;reading the book for the first time,&#x27; querying is &#x27;flipping to the right page because you remember the structure.&#x27;
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Pro move: ask your agent to add a comparison mode that runs the same question through both PageIndex and a basic vector RAG setup (using something like ChromaDB). Seeing the different results side-by-side on the same document is the fastest way to internalize why reasoning-based retrieval matters.
      </div>
        
      <details class="reveal">
        <summary>Can this handle multiple documents?</summary>
        <div class="reveal-body"><p>Yes — you build a separate tree index for each document, then at query time, you can either (a) search all indices and let the LLM decide which document is most relevant, or (b) route the query to the right document first based on metadata (title, type, date). For a small collection (say, 10-20 documents), option (a) works fine. For larger collections, you&#x27;d want a lightweight routing layer — and honestly, this is where a <em>small</em> amount of vector search for document-level routing combined with tree search for within-document retrieval could be a powerful hybrid.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have a working vectorless RAG pipeline: a PDF goes in, a tree index gets built, and you can ask questions that get answered with page-level citations. The key difference from traditional RAG: every retrieval step is traceable reasoning, not opaque similarity scores.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>When to Use This (and When Not To)</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s be honest about trade-offs — no tool is perfect for everything.</p>

<p><strong>PageIndex shines when:</strong></p>
<ul>
<li>Documents have clear structure (reports, papers, manuals, contracts)</li>
<li>Questions require multi-step reasoning (&#x27;compare X in section A with Y in section B&#x27;)</li>
<li>Explainability matters (regulated industries, audit trails)</li>
<li>You want to avoid vector infrastructure overhead</li>
<li>Accuracy on professional documents is critical</li>
</ul>

<p><strong>Traditional vector RAG might still win when:</strong></p>
<ul>
<li>You&#x27;re searching across thousands of short, unstructured documents (chat logs, tweets, support tickets)</li>
<li>Your queries are simple keyword-style lookups</li>
<li>You need sub-second latency on every query (tree search adds LLM calls)</li>
<li>Your documents genuinely lack structure</li>
</ul>

<p>The 98.7% accuracy on FinanceBench is real and impressive — but FinanceBench is specifically designed around structured financial documents. Know your use case.</p>

<p>Let&#x27;s get your agent to help you think through which approach fits <em>your</em> scenario.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to create a decision framework — a set of questions you can answer about your use case that tells you whether PageIndex, vector RAG, or a hybrid approach is the best fit.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What characteristics of the documents matter most for this decision?</li><li>How does the type of questions users will ask affect the choice?</li><li>What about operational concerns — cost, latency, infrastructure?</li><li>Is there a middle ground where you&#x27;d use both approaches together?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a practical decision tree or checklist. Something like: &#x27;If your documents are structured AND your questions require reasoning → PageIndex. If your corpus is large and unstructured AND queries are simple → vector RAG. If you need both → hybrid with vector search for document routing and tree search for within-document retrieval.&#x27; It should also cover practical considerations: PageIndex uses more LLM calls per query (cost), vector RAG needs embedding infrastructure (ops burden), and the hybrid approach gives you the best of both worlds at the cost of complexity.</p></div>
      </details>
      </div>
        
        
        
      <details class="reveal">
        <summary>What about the vision-based mode?</summary>
        <div class="reveal-body"><p>PageIndex also has a vision-based mode that works directly on PDF page images — no OCR, no text extraction. It sends page images to a vision-capable LLM and reasons over them visually. This is killer for documents with complex layouts, tables, charts, or scanned pages where text extraction is lossy. The trade-off is higher token cost (images are expensive) and higher latency, but for documents where layout matters (think: engineering drawings, medical charts, complex financial tables), it can be dramatically more accurate than text-based approaches.</p></div>
      </details>
      </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Build a document Q&amp;A tool that indexes a collection of PDF reports and answers questions with full reasoning traces — showing not just the answer, but the path the system took through the document tree to find it.</div>
      <div class="your-turn-context">You&#x27;ve seen how PageIndex works on a single document. Now imagine you&#x27;re building a tool for a compliance team that needs to query across a set of quarterly reports. They don&#x27;t just want answers — they want to see *how* the system found each answer, so they can verify it themselves. This is about trust and auditability.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>How would you handle multiple documents — separate indices? A combined one?</li><li>What should the &#x27;reasoning trace&#x27; output look like — what would a compliance officer actually want to see?</li><li>How would you let the user drill deeper if the first answer isn&#x27;t specific enough?</li><li>Should the tool compare information across documents (e.g., &#x27;how did this metric change from Q1 to Q3&#x27;)?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Build me a Python tool that takes a folder of PDF reports and creates a queryable document Q&amp;A system using PageIndex. When I ask a question, it should: (1) search across all indexed documents, (2) show me the reasoning path — which document it chose, which sections it navigated through, and why it picked each branch, (3) give me the final answer with specific page and section citations, and (4) let me ask follow-up questions that maintain context from the previous answer. Use PageIndex for the tree indexing and search. Store the indices so I don&#x27;t have to rebuild them each time. Include a simple CLI interface where I can type questions and see the full reasoning trace alongside the answer.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>Let&#x27;s step back and appreciate what just happened. We took a completely different approach to document retrieval — one that doesn&#x27;t use vectors, doesn&#x27;t chunk documents, and doesn&#x27;t rely on similarity search.</p>

<p>Instead, we built a system that <em>reads</em> documents the way a human expert does: understand the structure, navigate to the right section, and reason about relevance. The tree index is the document&#x27;s brain map, and tree search is the expert&#x27;s navigation strategy.</p>

<p>The PageIndex approach isn&#x27;t just academically interesting — it hit 98.7% accuracy on FinanceBench, a real benchmark with real financial questions. And the explainability is a game-changer: every retrieval decision comes with a reasoning trace you can audit.</p>

<p>Is this the death of vector databases? Probably not. But it&#x27;s a powerful reminder that <strong>the best retrieval method depends on the kind of documents and questions you&#x27;re dealing with.</strong> For structured, professional documents where accuracy and explainability matter, reasoning-based retrieval is looking very strong.</p></div>
      <ul class="takeaways-list"><li>Similarity ≠ relevance. Vector search finds semantically close chunks, but reasoning-based retrieval finds actually relevant sections — a crucial distinction for professional documents.</li><li>PageIndex builds a hierarchical tree index (like a smart table of contents) and uses LLM reasoning to navigate it, inspired by how AlphaGo searches game trees.</li><li>No chunking means no chunk-size tuning headaches. Documents are organized by their natural structure — sections, subsections, paragraphs — not arbitrary token windows.</li><li>Explainability is a first-class feature. Every retrieval step is a reasoning decision you can trace and audit, unlike opaque cosine similarity scores.</li><li>The right approach depends on your use case: PageIndex for structured documents with complex queries, vector RAG for large unstructured corpora with simple lookups, or a hybrid for the best of both.</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Try PageIndex on your own documents — start with something well-structured like a technical report or contract, and compare the results to your current RAG setup.</li><li>Explore the vision-based mode for documents with complex layouts, tables, or scanned pages where text extraction loses information.</li><li>Read the PageIndex framework article (linked in the GitHub repo) for the full technical deep-dive on tree construction and search algorithms.</li><li>Experiment with a hybrid approach: use lightweight vector search for document-level routing across a large corpus, then PageIndex for precise within-document retrieval.</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/VectifyAI/PageIndex" target="_blank" rel="noopener">VectifyAI/PageIndex</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">ComposioHQ/awesome-claude-skills</div>
            <div class="oa-summary">Curated collection of ready-made Claude Skills and tools for customizing AI-assisted workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datagouv/datagouv-mcp</div>
            <div class="oa-summary">MCP server letting AI chatbots search and analyze France&#x27;s national open data platform via conversation.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Don&#x27;t trust AI agents</div>
            <div class="oa-summary">Argues AI agents pose security risks and shouldn&#x27;t be trusted with unsupervised access to systems.</div>
            <div class="oa-meta">Hacker News AI · Feb 28</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">521xueweihan/HelloGitHub</div>
            <div class="oa-summary">Monthly curated showcase of interesting, beginner-friendly open source projects on GitHub.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">ruvnet/wifi-densepose</div>
            <div class="oa-summary">Tracks full human body poses through walls in real time using standard WiFi routers.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">OpenFang</div>
            <div class="oa-summary">Open-source operating system designed for building and managing autonomous AI agents.</div>
            <div class="oa-meta">Product Hunt AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a one-trillion-parameter language model locally on a cluster of Ryzen AI laptops.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">What AI coding costs you</div>
            <div class="oa-summary">Explores the hidden tradeoffs of AI-assisted coding and finding the right balance of AI usage.</div>
            <div class="oa-meta">Hacker News AI · Feb 28</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">MCP server that reduces Claude Code context consumption by 98%</div>
            <div class="oa-summary">An MCP server that dramatically shrinks context window usage in Claude Code sessions.</div>
            <div class="oa-meta">Hacker News AI · Feb 28</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Switch to Claude without starting over</div>
            <div class="oa-summary">Claude now lets you import conversation history and memory from other AI assistants.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "ComposioHQ/awesome-claude-skills", "tags": "tools,coding", "source": "GitHub Trending Python"}, {"title": "datagouv/datagouv-mcp", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Don't trust AI agents", "tags": "agents", "source": "Hacker News AI"}, {"title": "521xueweihan/HelloGitHub", "tags": "open-source", "source": "GitHub Trending Python"}, {"title": "ruvnet/wifi-densepose", "tags": "", "source": "GitHub Trending Python"}, {"title": "OpenFang", "tags": "agents,open-source", "source": "Product Hunt AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}, {"title": "What AI coding costs you", "tags": "coding", "source": "Hacker News AI"}, {"title": "MCP server that reduces Claude Code context consumption by 98%", "tags": "coding", "source": "Hacker News AI"}, {"title": "Switch to Claude without starting over", "tags": "", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "general";
      var date = "2026-03-01";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>