<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Build Real-Time Vision AI Agents with Stream&#x27;s Open Vision Agents — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-general">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>AI That Watches, Listens, and Understands — In Real Time</h1>
      <div class="hero-subtitle">Stream just open-sourced Vision Agents: a framework for building AI agents that process live video with sub-30ms latency. Let&#x27;s explore how it works and design our own.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">vision-ai</span> <span class="tag">real-time</span> <span class="tag">multi-modal</span> <span class="tag">yolo</span> <span class="tag">agents</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <h2>What Just Happened</h2>

<p>Stream — the company behind one of the most popular real-time chat and video SDKs — just open-sourced <strong>Vision Agents</strong>, a Python framework for building AI agents that process live video feeds in real time.</p>

<p>This is a big deal for a few reasons:</p>

<p><strong>Speed matters more than smarts (sometimes).</strong> We&#x27;ve had vision models for a while. GPT-4o can describe an image. Gemini can analyze a video clip. But doing it <em>live</em> — at 10-30 frames per second, with under 30ms of latency — that&#x27;s an engineering problem, not just an AI problem. Stream&#x27;s contribution here is their <strong>edge network</strong>, which keeps the video pipeline fast enough that the AI feels responsive.</p>

<p><strong>The &quot;fast model + smart model&quot; pattern is emerging everywhere.</strong> Vision Agents doesn&#x27;t pick one model. It lets you wire together a <em>fast</em> computer vision model (like YOLO for object detection — runs in milliseconds) with a <em>smart</em> reasoning model (like Gemini Realtime or OpenAI). The fast model tells you <em>what&#x27;s there</em>. The smart model tells you <em>what it means</em>. This two-tier pattern is becoming a standard architecture for production AI.</p>

<p><strong>The demos are wild.</strong> A golf coaching agent that tracks your swing in real time using pose estimation, then gives you feedback via Gemini. A security camera that detects faces, tracks packages, identifies theft, generates WANTED posters, and <em>posts them to X automatically</em>. An invisible interview coaching overlay (yes, like Cluely).</p>

<p>Today we&#x27;re going to understand how this architecture works, get comfortable with the mental model, and design our own vision agent — all by working with our AI coding assistant.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>The Three-Layer Cake: Understanding the Architecture</h2>
      </div>
      <div class="step-body">
        <p>Before we build anything, let&#x27;s understand what we&#x27;re working with. Vision Agents has three core layers, and honestly, the naming is so clean it almost explains itself:</p>

<ol>
<li><strong>Edge</strong> — The video highway. This is Stream&#x27;s network that moves video frames and audio between your users and your agent. Think of it like a CDN, but for live video. It&#x27;s what gets you that sub-30ms latency.</li>
</ol>

<ol>
<li><strong>Processors</strong> — The fast eyes. These are computer vision models like YOLO that run on every frame (or every Nth frame). They&#x27;re dumb but <em>fast</em>. They can tell you &quot;there&#x27;s a person at coordinates (x, y)&quot; or &quot;I see a package&quot; in a couple milliseconds. They don&#x27;t understand <em>meaning</em> — they just detect patterns.</li>
</ol>

<ol>
<li><strong>LLM</strong> — The brain. This is your Gemini, OpenAI, or Claude model that takes the processor outputs plus the video context and actually <em>reasons</em> about what&#x27;s happening. &quot;That person picked up the package and is walking away — that might be theft.&quot;</li>
</ol>

<p>The magic is in how these layers compose. The Processor annotates frames (draws bounding boxes, extracts poses), and the LLM sees both the original video AND the annotations. It&#x27;s like giving someone a highlighted document vs. a raw one — the highlights guide their attention.</p>

<p>Let&#x27;s get our agent to explain this architecture back to us and scaffold a minimal example.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to explain the Vision Agents Agent/Processor/LLM pattern and show you the minimal skeleton of a vision agent.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What are the three main components the Agent class needs to be wired up?</li><li>How does data flow between them — what does the Processor output, and what does the LLM receive?</li><li>What&#x27;s the simplest possible configuration — one edge, one processor, one LLM?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that an <code>Agent</code> is basically a composition root — it takes an <code>Edge</code> (video transport), a list of <code>Processors</code> (CV models that run per-frame), and an <code>LLM</code> (the reasoning model). The minimal setup looks something like:</p>

<p>```python</p>
<p>agent = Agent(</p>
<p>edge=getstream.Edge(),</p>
<p>agent_user=User(name=&quot;My Agent&quot;, id=&quot;agent&quot;),</p>
<p>instructions=&quot;You are a helpful video assistant.&quot;,</p>
<p>llm=gemini.Realtime(fps=10),</p>
<p>processors=[ultralytics.YOLOProcessor()],</p>
<p>)</p>
<p>```</p>

<p>The key insight: Processors annotate frames → LLM sees annotated frames + instructions → LLM generates responses. The Agent orchestrates this pipeline.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Think of Processors like a highlighter pen and the LLM like a reader. The highlighter marks up the document (video frames) so the reader can focus on what matters instead of scanning everything.
      </div>
        
      <details class="reveal">
        <summary>Why not just send raw video to the LLM?</summary>
        <div class="reveal-body"><p>You <em>could</em> skip Processors and send raw frames to Gemini or GPT-4o. But there are two problems:</p>

<ol>
<li><strong>Speed</strong>: LLMs take 100-500ms per frame. YOLO takes 2-5ms. If you need to track a golf ball mid-swing, you can&#x27;t wait for GPT to process each frame.</li>
</ol>

<ol>
<li><strong>Cost</strong>: Sending 30 frames per second to an LLM API would bankrupt you in about 4 minutes. Processors are local models running on your GPU — essentially free after the initial setup.</li>
</ol>

<p>The two-tier approach gives you the speed of computer vision AND the intelligence of an LLM, without the cost or latency of either alone.</p></div>
      </details>
      <details class="reveal">
        <summary>What does &#x27;Edge&#x27; actually mean here?</summary>
        <div class="reveal-body"><p>Stream&#x27;s Edge network is essentially a globally distributed set of servers optimized for real-time media. When a user&#x27;s camera sends video, it goes to the nearest edge server (think: a data center in their city), gets processed, and the annotated result streams back.</p>

<p>The 500ms join time and sub-30ms latency numbers come from this infrastructure. You <em>can</em> swap in your own edge network — the framework is designed to be pluggable — but Stream&#x27;s is the default and it&#x27;s what makes the &#x27;real-time&#x27; part actually real-time.</p>

<p>For local development, you can also run everything on localhost, which is what the examples do.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Dissecting the Golf Coach: YOLO Meets Gemini</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s look at the most impressive example — the golf coaching agent. This one is genuinely cool because it shows the full power of the architecture.</p>

<p>Here&#x27;s what happens when you swing a golf club in front of this agent:</p>

<ol>
<li><strong>YOLO Pose Processor</strong> runs on every frame and detects your body&#x27;s skeleton — 17 keypoints (shoulders, elbows, wrists, hips, knees, ankles, etc.) with their (x, y) coordinates.</li>
<li>Those keypoints get drawn as an overlay on the video frame — little dots and lines showing your pose.</li>
<li><strong>Gemini Realtime</strong> receives the annotated video stream at ~10fps and sees both you AND your skeleton overlay.</li>
<li>Combined with instructions from a <code>golf_coach.md</code> prompt file, Gemini gives you real-time coaching: &quot;Your left elbow is breaking down at the top of your backswing — try keeping it straighter.&quot;</li>
</ol>

<p>The really clever part? YOLO doesn&#x27;t know anything about golf. It just detects human poses. All the golf expertise lives in the LLM instructions. This means you could swap the instructions to create a yoga instructor, a physical therapy assistant, or a dance coach — same YOLO model, totally different agent.</p>

<p>Let&#x27;s get our agent to help us understand how the processor pattern works and how instructions shape behavior.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to explain how YOLO pose estimation works at a high level, and how the golf coaching instructions turn a generic pose detector into a domain-specific coach.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What does YOLO actually output for each frame — what data structure represents a &#x27;pose&#x27;?</li><li>How do the instructions in the markdown file shape what Gemini does with that pose data?</li><li>What would you change to turn this into a yoga instructor instead?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that YOLOPoseProcessor outputs 17 keypoints per detected person — each keypoint is an (x, y, confidence) tuple representing joints like &#x27;left_shoulder&#x27;, &#x27;right_elbow&#x27;, etc. These get drawn as a stick figure overlay on the video frame.</p>

<p>The LLM never sees raw keypoint data — it sees the <em>visual</em> overlay on the video. The instructions file (<code>golf_coach.md</code>) is what gives it domain knowledge: what a good swing looks like, common mistakes, how to give feedback. Swapping that file to <code>yoga_instructor.md</code> with yoga-specific knowledge would completely change the agent&#x27;s behavior without touching any code.</p>

<p>This is the power of the pattern: processors are generic, instructions are specific.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        This &#x27;generic processor + specific instructions&#x27; pattern is one of the most reusable ideas here. You&#x27;re essentially building the eyes once and swapping the brain&#x27;s training manual.
      </div>
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        The golf example requires a Google Gemini API key (for the Realtime LLM), a Stream API key (for the edge network), and YOLO weights (downloaded automatically on first run). The example README walks through setup.
      </div>
        
      <details class="reveal">
        <summary>How does YOLO run so fast?</summary>
        <div class="reveal-body"><p>YOLO (You Only Look Once) is a family of object detection models designed for speed. The key insight in YOLO&#x27;s design is right in the name — it processes the entire image in a single pass through the neural network, rather than scanning regions one at a time like older approaches.</p>

<p>YOLO11n-pose (the &#x27;nano&#x27; variant used in the golf example) is specifically optimized for edge devices. It runs at 2-5ms per frame on a decent GPU, which means you can comfortably process 30+ fps with room to spare. The &#x27;pose&#x27; variant adds keypoint detection on top of the standard bounding box detection.</p>

<p>The trade-off? YOLO is less accurate than slower models. It might occasionally miss a keypoint or jitter between frames. But for real-time coaching, speed matters more than pixel-perfect accuracy — the LLM smooths over the noise anyway.</p></div>
      </details>
      <details class="reveal">
        <summary>What&#x27;s in the golf_coach.md instructions file?</summary>
        <div class="reveal-body"><p>We don&#x27;t have the exact contents, but based on the pattern, it likely includes:</p>

<ul>
<li><strong>Role definition</strong>: &#x27;You are an expert golf coach analyzing a student&#x27;s swing in real time.&#x27;</li>
<li><strong>What to look for</strong>: Key checkpoints in a golf swing — address position, backswing, top of swing, downswing, impact, follow-through.</li>
<li><strong>Common mistakes</strong>: Things like &#x27;chicken wing&#x27; (bent left elbow), early extension, over-the-top swing path.</li>
<li><strong>How to communicate</strong>: Be encouraging, give one tip at a time, use visual references (&#x27;notice how your right shoulder is dipping&#x27;).</li>
<li><strong>When to speak</strong>: Don&#x27;t narrate every frame — wait for a complete swing, then give feedback.</li>
</ul>

<p>The instructions file IS the expertise. The models are just the delivery mechanism.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>By now you should understand the three-layer architecture (Edge → Processors → LLM), how YOLO provides fast per-frame detection while the LLM provides reasoning, and why the &#x27;generic processor + specific instructions&#x27; pattern is so powerful. If any of that feels fuzzy, re-read Step 1 before continuing.</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re building a real-time fitness coaching agent. Your Processor detects human poses at 30fps, but your LLM (Gemini Realtime) can only handle ~10fps at a reasonable cost. What&#x27;s the best approach?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt0">
          <label for="decision_5_opt0">Set the LLM to 10fps and let the Processor run at 30fps independently</label>
          <div class="decision-feedback correct">&#10003; Correct! This is exactly how Vision Agents is designed. The Processor and LLM run at different frame rates — the Processor annotates every frame for smooth visual feedback, while the LLM samples at a lower rate for reasoning. The `fps` parameter on the LLM controls this. You get smooth pose overlay AND intelligent coaching without breaking the bank.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt1">
          <label for="decision_5_opt1">Slow the Processor down to 10fps to match the LLM</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This would work but it&#x27;s wasteful. The Processor is fast and cheap — there&#x27;s no reason to throttle it. Running it at 30fps gives you smoother visual annotations (the stick figure overlay), even if the LLM only reasons about every 3rd frame. The whole point of the two-tier architecture is that each tier runs at its own optimal speed.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt2">
          <label for="decision_5_opt2">Buffer 3 frames and send them as a batch to the LLM</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Batching adds latency and complexity. The LLM doesn&#x27;t need every frame to give good coaching advice — it needs a *recent* frame with context. Sending it 10 current frames per second is better than sending 3-frame batches with delays. The streaming architecture is designed for continuous processing, not batch processing.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>The Security Camera: Multi-Processor Composition</h2>
      </div>
      <div class="step-body">
        <p>The golf coach is impressive, but the security camera example is where things get <em>really</em> interesting architecturally. It doesn&#x27;t just use one processor — it composes multiple AI capabilities into a single agent:</p>

<ul>
<li><strong>YOLOv11</strong> (custom-trained) for package detection</li>
<li><strong>Face recognition</strong> for identifying people</li>
<li><strong>Gemini</strong> for reasoning about what&#x27;s happening</li>
<li><strong>ElevenLabs</strong> for text-to-speech (the agent can <em>talk</em>)</li>
<li><strong>Deepgram</strong> for speech-to-text (the agent can <em>listen</em>)</li>
</ul>

<p>This is a full multi-modal agent. It watches your front porch, recognizes who&#x27;s there, tracks packages, detects suspicious behavior, and can have a voice conversation with you about what it sees.</p>

<p>The wildest part? When it detects a package theft, it automatically generates a WANTED poster from the face recognition data and posts it to X (Twitter). That&#x27;s... a lot.</p>

<p>But here&#x27;s the architectural lesson: <strong>the Agent class is a composition root</strong>. It doesn&#x27;t care what processors you plug in or how many. Each processor does its own thing on each frame, and all their outputs get composed together before the LLM sees them. It&#x27;s like a Lego system — snap in the pieces you need.</p>

<p>Let&#x27;s explore how this multi-processor pattern works and how TTS/STT add voice interaction.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to explain how multiple processors compose in a single Vision Agent, and how TTS/STT add voice capabilities.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>How do multiple processors run on the same frame — sequentially or in parallel?</li><li>When the LLM receives the frame, how does it know which annotations came from which processor?</li><li>What role do the `tts` and `stt` parameters play — how does voice interaction change the agent&#x27;s behavior?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that processors run as a pipeline on each frame — each one adds its own annotations (bounding boxes, labels, face tags). The LLM sees the combined annotated frame.</p>

<p>The security example adds two new components: <code>tts=elevenlabs.TTS()</code> gives the agent a voice (it can speak its observations aloud), and <code>stt=deepgram.STT()</code> lets it hear voice commands. This turns a passive observer into an interactive assistant you can talk to.</p>

<p>The key design insight: the Agent class treats processors, LLM, TTS, and STT as pluggable modules. You compose the capabilities you need — same pattern every time.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        The security camera example auto-posts to X/Twitter. If you run it, make sure you either disable that feature or use a test account. Nobody wants to accidentally tweet a WANTED poster of their mail carrier.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Custom YOLO weights (<code>weights_custom.pt</code> in the example) mean someone trained a YOLO model specifically for package detection. You can do this too using tools like Roboflow — train on your specific objects, export YOLO weights, drop them in.
      </div>
        
      <details class="reveal">
        <summary>How would you train custom YOLO weights?</summary>
        <div class="reveal-body"><p>Training a custom YOLO model for your specific use case is surprisingly accessible:</p>

<ol>
<li><strong>Collect images</strong>: Take ~200-500 photos of the objects you want to detect (e.g., packages on porches).</li>
<li><strong>Label them</strong>: Use a tool like Roboflow or Label Studio to draw bounding boxes around your objects.</li>
<li><strong>Train</strong>: Ultralytics makes this a one-liner — point it at your labeled dataset and it fine-tunes a YOLO model.</li>
<li><strong>Export</strong>: Save the weights as a <code>.pt</code> file and pass it to <code>YOLOProcessor(model_path=&#x27;your_weights.pt&#x27;)</code>.</li>
</ol>

<p>The whole process can take a few hours on a decent GPU. Roboflow even offers a hosted training option if you don&#x27;t have a GPU.</p>

<p>This is one of the most powerful aspects of the processor pattern — you can drop in domain-specific detection without changing any other part of your agent.</p></div>
      </details>
      <details class="reveal">
        <summary>Gemini vs Gemini Realtime — what&#x27;s the difference?</summary>
        <div class="reveal-body"><p>In the examples, you&#x27;ll notice two different Gemini configurations:</p>

<ul>
<li><strong><code>gemini.Realtime(fps=10)</code></strong> — This is Gemini&#x27;s real-time streaming API. It maintains a persistent connection, receives frames continuously, and can respond at any time. It&#x27;s designed for live interaction. Used in the golf coach.</li>
</ul>

<ul>
<li><strong><code>gemini.LLM(&#x27;gemini-2.5-flash-lite&#x27;)</code></strong> — This is a standard request/response call to Gemini. You send it a frame (or a few frames), it thinks, it responds. Used in the security camera (where you don&#x27;t need continuous coaching, just alerts).</li>
</ul>

<p>The Realtime variant is more expensive but gives you that conversational, always-on feel. The standard LLM variant is cheaper and better for event-driven use cases where you only need the AI when something happens.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Building a Custom Processor: The Lego Brick Pattern</h2>
      </div>
      <div class="step-body">
        <p>Now let&#x27;s get to the fun part — understanding how you&#x27;d build your own processor. This is where the framework becomes a tool for <em>your</em> ideas, not just Stream&#x27;s demos.</p>

<p>A Processor in Vision Agents is basically a class with one important method: it takes a video frame in, and returns an annotated frame out. That&#x27;s it. Whatever you do in between — run YOLO, run a custom model, call an API, do some OpenCV magic — is up to you.</p>

<p>Think of it like a photo filter on Instagram, except instead of making your food look vintage, it&#x27;s drawing bounding boxes around objects or highlighting regions of interest.</p>

<p>The framework gives you built-in processors for common tasks (YOLO detection, YOLO pose estimation), but the real power is writing your own. Want to detect specific hand gestures? Count cars in a parking lot? Track a ball in a sports game? You write a processor that does that specific thing, and the rest of the pipeline stays the same.</p>

<p>Let&#x27;s get our agent to help us understand the processor interface and sketch one out.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to explain the Processor interface pattern and help you think through what a custom processor needs.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s the minimum interface a Processor must implement — what goes in, what comes out?</li><li>How does a processor communicate what it found to the LLM — through annotations on the frame, or through some other channel?</li><li>If you wanted to build a &#x27;count people in a room&#x27; processor, what would the key steps be?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that a Processor takes a frame (image array) and returns an annotated frame. The core pattern is:</p>

<ol>
<li>Receive frame → 2. Run your detection/analysis → 3. Draw annotations (boxes, labels, overlays) → 4. Return annotated frame.</li>
</ol>

<p>The LLM sees the visual annotations, so your processor communicates through what it draws. For a &#x27;people counter&#x27;, you&#x27;d run YOLO person detection, draw bounding boxes, and add a text overlay like &#x27;People: 7&#x27; in the corner. The LLM would see that number and incorporate it into its reasoning.</p>

<p>The agent should emphasize that processors are stateful — you can track things across frames (like counting unique people who&#x27;ve entered a room over time).</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Processors can be stateful! The security camera processor tracks packages across frames — it knows &#x27;that package has been on the porch for 10 minutes&#x27; because it maintains state between frames. This is how you go from &#x27;I see a package&#x27; to &#x27;a package was stolen.&#x27;
      </div>
        
      <details class="reveal">
        <summary>What about the fps parameter on processors?</summary>
        <div class="reveal-body"><p>Processors can specify their own <code>fps</code> — how many frames per second they process. This is separate from the video fps and the LLM fps.</p>

<p>Why would you want different rates? Because different tasks need different speeds:</p>

<ul>
<li><strong>Pose estimation for sports coaching</strong>: High fps (15-30) — you need to catch every moment of a fast swing.</li>
<li><strong>Package detection for security</strong>: Low fps (2-5) — packages don&#x27;t move fast, and running detection 30 times per second wastes GPU.</li>
<li><strong>Face recognition</strong>: Very low fps (1-2) — faces don&#x27;t change that fast and recognition is more expensive.</li>
</ul>

<p>The framework handles the frame sampling for you. Set <code>fps=5</code> on your processor and it&#x27;ll only run on every 6th frame of a 30fps video. The annotated frames persist between runs, so the visual overlay stays smooth even at low processor fps.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You now understand the full architecture: Edge for transport, Processors for fast per-frame detection, LLMs for reasoning, plus optional TTS/STT for voice. You know how processors compose together and how custom processors plug into the framework. Time to design your own agent!</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You want to build a real-time cooking assistant that watches someone cook and gives advice. You need to detect ingredients, track what&#x27;s on the stove, and provide cooking tips. Which setup makes the most sense?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_9" id="decision_9_opt0">
          <label for="decision_9_opt0">Custom YOLO (trained on food/kitchen items) + Gemini Realtime with cooking instructions</label>
          <div class="decision-feedback correct">&#10003; Correct! This follows the proven pattern: a fast, domain-specific processor (custom YOLO trained on common ingredients and kitchen tools) paired with a reasoning LLM (Gemini Realtime for continuous conversational coaching). The custom YOLO handles the &#x27;what&#x27;s on the counter&#x27; detection, and Gemini handles the &#x27;you should add salt now&#x27; reasoning. The cooking expertise lives in the instructions markdown file.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_9" id="decision_9_opt1">
          <label for="decision_9_opt1">Send raw video to GPT-4o at 30fps</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. GPT-4o is great at understanding images, but sending 30 frames per second to a cloud API would be incredibly expensive (think hundreds of dollars per hour), way too slow for real-time feedback, and unnecessary. The two-tier approach with a local YOLO model for detection and an LLM for occasional reasoning is orders of magnitude more practical.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_9" id="decision_9_opt2">
          <label for="decision_9_opt2">YOLO detection only, no LLM</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. YOLO can tell you &#x27;there&#x27;s a tomato and a knife on the counter&#x27; but it can&#x27;t tell you &#x27;you should dice the tomato next because the sauce needs to simmer for 20 minutes.&#x27; The LLM provides the reasoning layer — understanding recipes, timing, technique. Without it, you just have an expensive label maker.</div>
        </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Design a real-time vision agent for a specific use case of YOUR choosing. Get your AI agent to help you plan the architecture — which processors, which LLM configuration, what instructions, and what the user experience looks like.</div>
      <div class="your-turn-context">You&#x27;ve seen the golf coach (pose estimation + coaching LLM), the security camera (multi-processor + event detection), and the invisible assistant (screen capture + text coaching). Now it&#x27;s your turn to design one. Pick something that excites you — a gym workout tracker, a bird watching identifier, a warehouse inventory monitor, a classroom engagement detector, a pet behavior analyzer... anything where real-time video + AI reasoning would be useful.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What specific objects or activities does your agent need to detect? This determines your Processor choice.</li><li>Does your agent need to talk (TTS) or listen (STT)? Or is text output enough?</li><li>Should the LLM run continuously (Realtime) or only respond to events (standard LLM)?</li><li>What expertise should go in the instructions file? What does the &#x27;brain&#x27; need to know about your domain?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>I want to design a real-time vision agent for a home gym workout tracker. Here&#x27;s what I&#x27;m thinking:

**Use case**: The agent watches someone working out via webcam and provides real-time form correction, rep counting, and workout tracking.

**Help me design the architecture using Stream&#x27;s Vision Agents framework:**
1. What Processor(s) do I need? I&#x27;m thinking YOLO Pose for body tracking — is that the right choice? Do I need any custom processors?
2. Should I use Gemini Realtime (continuous coaching) or a standard LLM (event-triggered feedback)? What fps makes sense?
3. What should go in my instructions markdown file? What fitness knowledge does the LLM need?
4. Should I add TTS so it can verbally coach? What about STT for voice commands like &#x27;start set&#x27; or &#x27;switch exercise&#x27;?
5. Can the processor be stateful to count reps? How would I track the up/down motion of a squat across frames?

Give me the full architecture plan — which components, how they connect, and what the user experience feels like.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>The Bigger Picture: Where Real-Time Vision Agents Are Heading</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s zoom out and think about <em>why</em> this matters beyond the cool demos.</p>

<p>We&#x27;re at an interesting inflection point. For years, computer vision and language models existed in separate worlds. CV people built detection models. NLP people built chatbots. The idea of composing them into a single real-time pipeline was technically possible but practically painful — you&#x27;d need to handle video streaming, model serving, frame synchronization, latency optimization... it was a full-time infrastructure job.</p>

<p>Frameworks like Vision Agents collapse that complexity. The Agent class handles the orchestration. The Edge handles the streaming. You just pick your models and write your instructions.</p>

<p>This is the same pattern we saw with web development (React collapsed the UI complexity), mobile development (Flutter/React Native collapsed the cross-platform complexity), and now AI agents (LangChain, CrewAI, and now Vision Agents collapse the multi-model complexity).</p>

<p><strong>Some use cases that become tractable with this pattern:</strong></p>

<ul>
<li><strong>Assisted living</strong>: An agent that watches an elderly person&#x27;s home, detects falls, and calls for help — with the ability to have a voice conversation about whether they&#x27;re okay.</li>
<li><strong>Manufacturing QA</strong>: An agent on the assembly line that detects defects in real time and explains what went wrong.</li>
<li><strong>Wildlife monitoring</strong>: Cameras in nature preserves that identify species, track movements, and alert researchers to unusual behavior.</li>
<li><strong>Retail analytics</strong>: Understanding customer behavior in stores — where they go, what they look at, how long they browse.</li>
</ul>

<p>The common thread? <strong>Fast detection + slow reasoning + real-time transport.</strong> That&#x27;s the recipe, and now there&#x27;s a framework for it.</p>
        
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        Real-time video AI raises serious privacy and ethical concerns. Facial recognition, behavior tracking, and automated surveillance are powerful tools that can easily be misused. Before building any vision agent that watches people, think carefully about consent, data storage, and who has access to the system.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Vision Agents is open source but Stream&#x27;s edge network is a commercial product. For local development and testing, everything runs on localhost. For production with real users, you&#x27;ll need a Stream account. The framework IS designed to work with other edge networks too.
      </div>
        
      <details class="reveal">
        <summary>How does this compare to other approaches?</summary>
        <div class="reveal-body"><p><strong>vs. Running everything through a multimodal LLM (GPT-4o, Gemini):</strong> Those models can analyze video, but not at real-time speeds or reasonable costs for continuous monitoring. Vision Agents&#x27; two-tier approach is fundamentally more efficient for always-on use cases.</p>

<p><strong>vs. Traditional CV pipelines (OpenCV + custom models):</strong> You can absolutely build real-time detection with OpenCV alone. But you lose the reasoning layer — traditional CV tells you <em>what</em> but not <em>why</em> or <em>what to do about it</em>. Vision Agents adds the LLM layer on top.</p>

<p><strong>vs. Roboflow / other CV platforms:</strong> Roboflow is great for training and deploying detection models. Vision Agents actually integrates with Roboflow as a processor source. They&#x27;re complementary, not competitive.</p>

<p><strong>vs. Building it yourself:</strong> You <em>could</em> wire together YOLO + Gemini + WebRTC + a streaming server. Vision Agents just saves you from writing all that glue code. Whether the abstraction is worth the dependency is a judgment call for your team.</p></div>
      </details>
      </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><h2>What We Explored</h2>

<p>We dug into Stream&#x27;s <strong>Vision Agents</strong> — an open-source framework for building AI agents that process live video in real time. We understood the three-layer architecture (Edge → Processors → LLM), dissected the golf coaching and security camera examples, learned how custom processors plug into the system, and designed our own vision agent architecture.</p>

<p>The big idea: <strong>fast, cheap CV models handle the &#x27;what&#x27;s there&#x27; at frame-rate speed, while reasoning LLMs handle the &#x27;what does it mean&#x27; at a lower frequency.</strong> The Agent class composes these layers together, and the Edge network makes it all feel instantaneous to the user.</p>

<p>This isn&#x27;t just about Stream&#x27;s framework — it&#x27;s a pattern that&#x27;s emerging across the AI industry. The ability to compose specialized models into real-time pipelines is becoming a fundamental skill for AI engineers.</p></div>
      <ul class="takeaways-list"><li>The two-tier architecture (fast CV model + reasoning LLM) is the key design pattern for real-time video AI — each tier runs at its own optimal speed and cost point</li><li>Processors are the reusable building blocks — same YOLO pose detector works for golf, yoga, physical therapy, or dance by swapping the LLM&#x27;s instructions</li><li>The &#x27;instructions as expertise&#x27; pattern means domain knowledge lives in markdown files, not in code — making agents reconfigurable without redeployment</li><li>Multi-modal composition (video + voice + reasoning) is now a configuration choice, not an engineering project — the Agent class handles the orchestration</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Clone the Vision Agents repo and run the golf coach example locally — it&#x27;s the fastest way to see the architecture in action</li><li>Try training a custom YOLO model on Roboflow for objects specific to your use case, then drop the weights into a YOLOProcessor</li><li>Experiment with different LLM configurations — compare Gemini Realtime (continuous) vs standard Gemini (event-driven) for your use case</li><li>Explore the framework&#x27;s client SDKs (React, iOS, Android) to understand how the frontend connects to your vision agent</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/GetStream/Vision-Agents" target="_blank" rel="noopener">GetStream/Vision-Agents</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis assistant that collects data, breaks filter bubbles, and predicts trends.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching how to build AI agents from scratch, covering theory and practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and various file formats into Markdown, with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Modular node-based GUI for building and running diffusion model image generation workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Community-curated list of free programming books and learning resources in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">If AI writes code, should the session be part of the commit?</div>
            <div class="oa-summary">Tool and discussion about including AI coding session transcripts as metadata in git commits.</div>
            <div class="oa-meta">Hacker News AI · Mar 2</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues that AI coding tools lower the code-writing bar but raise the bar for engineering judgment.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">OpenFang</div>
            <div class="oa-summary">Open-source operating system designed for running and managing AI agents.</div>
            <div class="oa-meta">Product Hunt AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "If AI writes code, should the session be part of the commit?", "tags": "coding", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "OpenFang", "tags": "agents,open-source", "source": "Product Hunt AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "general";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>