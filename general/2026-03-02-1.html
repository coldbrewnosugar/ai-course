<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Real-Time Vision AI Agents with Stream&#x27;s Open Vision Agents — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-general">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Teaching AI to Watch the World</h1>
      <div class="hero-subtitle">Stream just open-sourced a framework for building AI agents that see, hear, and respond to live video — in under 30 milliseconds. Let&#x27;s figure out how it works and build one.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">vision-ai</span> <span class="tag">real-time</span> <span class="tag">multi-modal</span> <span class="tag">yolo</span> <span class="tag">gemini</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Here&#x27;s the deal: <strong>Stream</strong> — the company behind the chat and video APIs powering apps like Strava and Imgur — just open-sourced <strong>Vision Agents</strong>, a Python framework for building AI agents that process live video in real-time.</p>

<p>Why is this interesting? Because until now, building something like &quot;an AI that watches a golf swing and coaches you&quot; required stitching together a dozen different tools: a video feed, an object detection model, a reasoning LLM, text-to-speech, a low-latency network... it was a mess.</p>

<p>Vision Agents wraps all of that into a clean architecture: <strong>Agent → Processors → LLM</strong>. You pick your detection model (YOLO, Roboflow), pick your reasoning brain (Gemini, OpenAI), and Stream&#x27;s edge network handles the plumbing to keep latency under 30ms.</p>

<p>The repo already has working examples: a <strong>golf coach</strong> that uses YOLO pose detection + Gemini to analyze your swing in real-time, a <strong>security camera</strong> that detects package theft and auto-generates wanted posters, and an <strong>invisible interview assistant</strong> (yes, really). Let&#x27;s dig into how this architecture works and build our own vision agent.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>The Mental Model: Fast Eyes + Smart Brain</h2>
      </div>
      <div class="step-body">
        <p>Before we build anything, let&#x27;s understand the core insight behind Vision Agents. It&#x27;s actually a really elegant design pattern.</p>

<p>Imagine you&#x27;re a lifeguard at a pool. You don&#x27;t deeply analyze every single thing happening in the water — that would be way too slow. Instead, your eyes do fast pattern recognition: &quot;movement looks normal... normal... wait, that splash pattern is weird.&quot; Only THEN does your brain kick in to reason: &quot;Is that person struggling? Should I act?&quot;</p>

<p>Vision Agents works the same way. It splits the work into two layers:</p>

<ol>
<li><strong>Processors</strong> (the fast eyes) — Models like YOLO that can process 30+ frames per second. They detect objects, track poses, recognize faces. They&#x27;re fast but not smart. They just tag what they see: &quot;person here, ball there, package on porch.&quot;</li>
</ol>

<ol>
<li><strong>LLM</strong> (the smart brain) — Models like Gemini Realtime or GPT-4o that receive those tagged frames and <em>reason</em> about them. &quot;The golfer&#x27;s elbow is dropping on the backswing&quot; or &quot;that person just picked up a package that isn&#x27;t theirs.&quot;</li>
</ol>

<p>The <strong>Agent</strong> is the orchestrator that ties it all together — it manages the video stream, runs frames through processors, feeds the annotated results to the LLM, and handles the response (voice, text, whatever).</p>

<p>This two-layer pattern — fast detection feeding smart reasoning — is the key architectural insight. Let&#x27;s get our agent to help us understand it better.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain the tradeoffs between running YOLO at different FPS rates when paired with an LLM like Gemini Realtime
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What happens to cost and latency as you increase frames per second?</li><li>Why might you want YOLO running at 30fps but only send frames to Gemini at 1-10fps?</li><li>Think about what information changes frame-to-frame vs. what needs deeper analysis...</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain the core tradeoff: YOLO is cheap and fast (runs locally on GPU, can handle 30+ fps easily), while LLM calls are expensive and slower. So you run YOLO at high FPS for smooth detection, but only send key frames to the LLM — maybe every 100ms (10fps) or even every second. The agent should mention that the golf coach example uses <code>fps=10</code> for Gemini but YOLO runs on every frame. It should also note the comment in the code: &#x27;Careful with FPS can get expensive&#x27; for OpenAI Realtime.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        This fast-eyes/smart-brain pattern isn&#x27;t unique to Vision Agents. It shows up everywhere in AI systems — RAG uses fast retrieval + smart generation, autonomous driving uses fast sensors + planning models. Once you see it, you can&#x27;t unsee it.
      </div>
        
      <details class="reveal">
        <summary>Why not just send raw video to the LLM?</summary>
        <div class="reveal-body"><p>You technically <em>can</em> — Gemini Realtime and GPT-4o both accept video input. But there are three problems:</p>

<ol>
<li><strong>Cost</strong>: Sending 30 raw frames per second to an LLM API would bankrupt you in hours. YOLO is essentially free by comparison since it runs locally.</li>
</ol>

<ol>
<li><strong>Precision</strong>: LLMs are good at reasoning but bad at pixel-level detection. YOLO can tell you the exact coordinates of a person&#x27;s elbow joint. An LLM will say &#x27;their arm is raised-ish.&#x27;</li>
</ol>

<ol>
<li><strong>Latency</strong>: Pre-processing with YOLO means the LLM gets <em>annotated</em> frames — bounding boxes, pose skeletons, labels. This dramatically reduces what the LLM needs to figure out, making it faster and more accurate.</li>
</ol>

<p>The processor layer essentially gives the LLM &#x27;reading glasses&#x27; so it can focus on reasoning instead of squinting at pixels.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Anatomy of a Vision Agent</h2>
      </div>
      <div class="step-body">
        <p>Now let&#x27;s look at how you actually compose a Vision Agent. The API surface is surprisingly small — the Agent constructor takes about five things, and honestly that&#x27;s the whole framework.</p>

<p>Think of it like ordering at a build-your-own-burger place:</p>

<ul>
<li><strong>Edge</strong> = your bun (the delivery network that gets video from point A to B)</li>
<li><strong>Processors</strong> = your toppings (detection models that annotate each frame)</li>
<li><strong>LLM</strong> = the patty (the reasoning engine that makes sense of everything)</li>
<li><strong>TTS/STT</strong> = condiments (optional text-to-speech and speech-to-text)</li>
<li><strong>Instructions</strong> = your order ticket (tells the agent its personality and job)</li>
</ul>

<p>The golf coach example shows this perfectly — it&#x27;s literally just one Agent constructor with a YOLO pose processor and Gemini Realtime. The security camera adds more toppings: face recognition, custom YOLO weights, ElevenLabs TTS, and Deepgram STT.</p>

<p>Let&#x27;s get our agent to help us understand the processor chain concept.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to design a processor chain for a real-time workout coaching app — what processors would you need and in what order?
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What does a workout coach need to detect? (Think about body position, rep counting, form...)</li><li>Which tasks need a fast model vs. a reasoning model?</li><li>Should processors run in sequence or parallel? What are the tradeoffs?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should propose something like: (1) YOLOPoseProcessor for skeleton/joint detection running at full frame rate, (2) a custom RepCounterProcessor that tracks joint angles over time to count reps, (3) Gemini Realtime as the LLM at maybe 5-10fps to analyze form and give coaching cues. The key insight is that processors are a <em>list</em> — they run in order, each one annotating the frame further before the LLM sees it. The agent might also suggest an AudioProcessor if the user wants voice commands like &#x27;start set&#x27; or &#x27;switch exercise.&#x27;</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        To actually run Vision Agents, you&#x27;ll need API keys for: <strong>Stream</strong> (free tier available for the edge network), <strong>Google AI</strong> (for Gemini) or <strong>OpenAI</strong> (for GPT-4o), and optionally <strong>Ultralytics</strong> for YOLO. The framework is model-agnostic though — you can swap any of these.
      </div>
        
      <details class="reveal">
        <summary>What makes processors composable?</summary>
        <div class="reveal-body"><p>The elegant part is that each processor follows the same interface: it receives a frame, does something to it (adds annotations, bounding boxes, metadata), and passes it along. This means you can stack them like LEGO blocks.</p>

<p>The security camera example chains: face recognition → YOLO object detection → a custom SecurityCameraProcessor that combines the outputs. Each processor enriches the frame with more context before the LLM ever sees it.</p>

<p>This is a classic pipeline pattern from software engineering, but applied to real-time video. The framework handles all the threading and frame synchronization — you just define what each processor does.</p></div>
      </details>
      <details class="reveal">
        <summary>How does Stream&#x27;s edge network fit in?</summary>
        <div class="reveal-body"><p>Stream&#x27;s edge network is the secret sauce for latency. Instead of your video going client → your server → back to client (which adds 100-300ms easily), Stream routes through edge nodes close to both parties.</p>

<p>The numbers: <strong>500ms to join</strong> a call and <strong>under 30ms</strong> for ongoing audio/video latency. That&#x27;s faster than most people&#x27;s reaction time, which means the AI coach feels truly real-time.</p>

<p>The framework abstracts this as the <code>Edge</code> class. You can use <code>getstream.Edge()</code> for Stream&#x27;s network, but the README says it works with any video edge network — so you could theoretically plug in your own WebRTC infrastructure.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point, you should understand the three-layer architecture: **Processors** (fast detection) feed annotated frames to an **LLM** (smart reasoning), all orchestrated by an **Agent** running on an **Edge** network. You should also understand *why* this split exists — cost, precision, and latency.</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re building an AI referee for a basketball game. You need to detect players, track the ball, and call fouls in real-time. Where should foul detection logic live?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt0">
          <label for="decision_5_opt0">In the LLM reasoning layer — let Gemini watch the annotated video and decide what&#x27;s a foul</label>
          <div class="decision-feedback correct">&#10003; Correct! Correct! Foul detection requires understanding context, rules, and judgment — exactly what LLMs are good at. YOLO handles the fast work (tracking player positions and ball location), then the LLM reasons about whether contact was legal. You wouldn&#x27;t want YOLO trying to understand basketball rules, and you wouldn&#x27;t want the LLM trying to track 10 players at 30fps.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt1">
          <label for="decision_5_opt1">In a custom YOLO processor — train it to detect fouls directly from video frames</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. While you *could* train a custom model for this, fouls are contextual — the same contact might be legal in one situation and illegal in another. Object detection models are great at &#x27;what is where&#x27; but bad at &#x27;what does this mean given the rules.&#x27; This is exactly the kind of reasoning you want to offload to the LLM layer.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt2">
          <label for="decision_5_opt2">Split it — detect contact events in a processor, then send only those moments to the LLM for judgment</label>
          <div class="decision-feedback correct">&#10003; Correct! This is also a great answer! It&#x27;s actually the most efficient approach — a processor could detect when players are within contact distance (cheap computation), and only flag those moments for LLM review. This reduces LLM calls dramatically. It&#x27;s the same pattern as the security camera example, where the processor detects package-related events and the LLM decides if theft is occurring.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Building the Golf Coach — Deconstructing the Example</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s break down the golf coach example because it&#x27;s the clearest demonstration of the pattern in action.</p>

<p>The golf coach does something genuinely impressive: it watches you swing a golf club via live video, tracks your body pose in real-time using YOLO, and has Gemini Realtime analyze your form and give voice coaching. All with sub-30ms latency.</p>

<p>Here&#x27;s what&#x27;s happening under the hood:</p>

<ol>
<li>Your camera feed goes to Stream&#x27;s edge network</li>
<li>Every frame hits the <strong>YOLOPoseProcessor</strong>, which detects 17 body keypoints (shoulders, elbows, wrists, hips, knees, ankles) and draws a skeleton overlay</li>
<li>Every ~100ms (10fps), an annotated frame goes to <strong>Gemini Realtime</strong> with instructions from a <code>golf_coach.md</code> file</li>
<li>Gemini sees the skeleton + video and reasons about your swing: &quot;Your left elbow is bending too much at the top of your backswing&quot;</li>
<li>The response streams back as audio through the edge network</li>
</ol>

<p>The instructions file is doing a LOT of heavy lifting here. It&#x27;s essentially a prompt that turns Gemini into a golf expert. Let&#x27;s explore that.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to write a system prompt (instructions file) for a real-time AI golf coach that receives YOLO pose data alongside video frames
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What does the coach need to know about golf swing mechanics?</li><li>How should it reference the YOLO keypoint data? (Think: joint angles, body alignment)</li><li>What tone should a good coach use — encouraging or clinical?</li><li>Should it give feedback constantly or only when it spots something wrong?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a markdown instructions file that covers: the coach&#x27;s personality (encouraging but precise), key swing phases to watch for (setup, backswing, downswing, impact, follow-through), specific YOLO keypoints to monitor (shoulder-hip alignment, elbow angles, knee flex), when to speak up vs. stay quiet (only comment on significant form issues, not every frame), and how to structure feedback (&#x27;I noticed X, try Y instead&#x27;). The best prompts will also tell the LLM to track improvement over time: &#x27;If the user corrects an issue, acknowledge it positively.&#x27;</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The <code>instructions = &quot;Read @golf_coach.md&quot;</code> pattern is worth stealing. Instead of cramming a huge prompt into code, put it in a markdown file and reference it. This makes it easy to iterate on the prompt without touching code — and you can version control it separately.
      </div>
        
      <details class="reveal">
        <summary>Why YOLO Pose specifically?</summary>
        <div class="reveal-body"><p>YOLO has several variants. The standard YOLOv11 does object detection (bounding boxes around things). <strong>YOLO Pose</strong> specifically detects human body keypoints — it outputs 17 (x, y) coordinates representing joints.</p>

<p>For the golf coach, this is gold. Instead of the LLM trying to figure out where your elbow is from raw pixels, it gets precise coordinates. It can literally calculate: &#x27;The angle between shoulder-elbow-wrist is 145°, optimal is 170°.&#x27;</p>

<p>The model file <code>yolo11n-pose.pt</code> is the &#x27;nano&#x27; variant — the smallest and fastest. On a modern GPU, it processes frames in about 5-8ms. There are larger variants (small, medium, large, extra-large) that are more accurate but slower. For real-time coaching where speed matters, nano is the sweet spot.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Going Multi-Modal: The Security Camera That Talks</h2>
      </div>
      <div class="step-body">
        <p>The security camera example cranks things up a notch. It&#x27;s not just watching — it&#x27;s a full multi-modal agent with <strong>vision</strong> (YOLO + face recognition), <strong>speech</strong> (ElevenLabs TTS + Deepgram STT), and <strong>reasoning</strong> (Gemini).</p>

<p>Honestly, this example is kind of wild. It detects faces, tracks packages, identifies when someone takes a package that wasn&#x27;t delivered to them, generates a &quot;WANTED&quot; poster, and <em>posts it to X in real-time</em>. That&#x27;s... a lot.</p>

<p>But architecturally, it&#x27;s the same pattern — just with more processors in the chain. The interesting addition here is the <strong>TTS and STT</strong> modules. This means the agent can:</p>
<ul>
<li><strong>Listen</strong> to people approaching (Deepgram converts speech to text)</li>
<li><strong>Reason</strong> about what&#x27;s happening (Gemini processes video + audio context)</li>
<li><strong>Speak</strong> alerts or warnings (ElevenLabs converts the LLM&#x27;s response to natural speech)</li>
</ul>

<p>This is what makes it a true <em>agent</em> and not just a detection system. It can have a conversation about what it sees.</p>

<p>Let&#x27;s think about how to add voice interaction to any vision agent.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to design the architecture for a real-time kitchen safety monitor — it watches a kitchen camera and can both see hazards and verbally warn the cook
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What objects should it detect? (Knives, flames, spills, smoke...)</li><li>When should it speak up vs. stay quiet? Nobody wants a nagging kitchen AI.</li><li>How would you handle the case where the cook asks &#x27;is the oven still on?&#x27; — that&#x27;s STT → reasoning → TTS</li><li>What&#x27;s the difference between using `gemini.Realtime()` vs `gemini.LLM()` here?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should design something with: a YOLO processor trained on kitchen objects (or a general one detecting people, knives, flames), a custom HazardProcessor that flags dangerous situations (knife near edge of counter, unattended flame, smoke detected), Gemini as the LLM with clear instructions about when to alert vs. stay quiet (only for genuine safety risks, not normal cooking), Deepgram STT for voice commands, and ElevenLabs TTS for spoken warnings. The key design decision: use <code>gemini.Realtime()</code> for the interactive voice version (bidirectional conversation) vs <code>gemini.LLM()</code> with separate TTS/STT for more control over the voice pipeline.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        The difference between <code>gemini.Realtime()</code> and <code>gemini.LLM()</code> matters. <strong>Realtime</strong> is Gemini&#x27;s native multimodal streaming mode — it handles audio in/out natively, which is simpler but gives you less control. <strong>LLM</strong> with separate TTS/STT modules gives you more flexibility (choose different voice providers, control audio routing) but is more complex to set up.
      </div>
        
      <details class="reveal">
        <summary>The invisible assistant pattern</summary>
        <div class="reveal-body"><p>The most provocative example in the repo is the &#x27;invisible assistant&#x27; — a Cluely-style overlay that watches your screen and listens to your audio but only responds via text (no audio broadcast).</p>

<p>This is architecturally interesting because it flips the output modality. Instead of speaking, it sends text to an overlay on your screen. The use cases they mention are... ethically spicy: job interview coaching, sales call assistance, on-the-job training through AR glasses.</p>

<p>The key technical detail: it uses <code>gemini.Realtime()</code> which handles both screen video and audio input natively, but the agent only outputs text — no TTS module. This keeps it &#x27;invisible&#x27; to the other party.</p>

<p>Whether you should <em>build</em> this is a different question from whether you <em>can</em>. But the architecture is clean and reusable for less controversial applications like accessibility tools or real-time translation overlays.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You now understand the full Vision Agents stack: **Edge** for low-latency video transport, **Processors** for fast per-frame detection, **LLMs** for reasoning, and **TTS/STT** for voice interaction. You&#x27;ve seen how the golf coach and security camera examples compose these pieces differently for different use cases.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Designing Your Own Vision Agent</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s the fun part. Now that you understand the architecture, you can design almost any real-time vision AI application by asking yourself four questions:</p>

<ol>
<li><strong>What needs to be detected fast?</strong> → This is your Processor layer. YOLO for objects/poses, face recognition for identity, custom models for domain-specific detection.</li>
</ol>

<ol>
<li><strong>What needs to be reasoned about?</strong> → This is your LLM layer. Pattern analysis, rule application, contextual understanding, natural language responses.</li>
</ol>

<ol>
<li><strong>How should the agent communicate?</strong> → Voice (TTS/STT), text overlay, API calls, push notifications, posting to social media (like the security camera&#x27;s X integration).</li>
</ol>

<ol>
<li><strong>What are the latency requirements?</strong> → Real-time coaching needs sub-100ms. Security monitoring can tolerate a second or two. This determines your FPS settings and model choices.</li>
</ol>

<p>The framework&#x27;s genius is that these four decisions map directly to constructor parameters. Once you&#x27;ve answered these questions, the code almost writes itself — which is exactly what we want our AI agent to help with.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to generate a complete Vision Agent setup for a use case you&#x27;re genuinely interested in — pick something weird and specific
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What&#x27;s a real problem in your life that &#x27;AI watching video&#x27; could solve?</li><li>Start by describing the use case in plain English before asking for code</li><li>What processors does it need? What should the LLM focus on?</li><li>What&#x27;s the right FPS balance between quality and cost?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you an Agent constructor with your specific processors and LLM choice, plus a draft instructions markdown file. The setup should be compact — maybe 10-15 lines of config plus the instructions file. The key thing to evaluate: did the agent pick the right processor types for your use case? Did it set reasonable FPS values? Does the instructions prompt give the LLM enough context to be useful without being overwhelming?</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        When evaluating what your agent gives back, the most important thing to check isn&#x27;t the code — it&#x27;s the <strong>instructions prompt</strong>. That markdown file is where 80% of the agent&#x27;s behavior comes from. A perfectly architected pipeline with a mediocre prompt will give mediocre results. Spend your iteration cycles there.
      </div>
        
      <details class="reveal">
        <summary>Custom processors — when YOLO isn&#x27;t enough</summary>
        <div class="reveal-body"><p>YOLO is great for general object detection and pose estimation, but what if you need something domain-specific? The framework supports custom processors.</p>

<p>A processor is just a class that receives frames and returns annotated frames. You could build one that:</p>
<ul>
<li>Runs a Roboflow model trained on your specific objects</li>
<li>Does optical flow analysis to detect sudden movements</li>
<li>Applies color histogram analysis (useful for detecting fire or smoke)</li>
<li>Runs OCR on text visible in the frame</li>
<li>Tracks objects across frames with a simple centroid tracker</li>
</ul>

<p>The <code>SecurityCameraProcessor</code> in the repo is a custom processor that wraps YOLO with domain-specific logic for package tracking. That&#x27;s the pattern to follow: wrap a fast model with your business logic, expose it as a processor, and let the framework handle the plumbing.</p></div>
      </details>
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re building a real-time AI tutor that watches a student solve math problems on a whiteboard via webcam. Which LLM configuration makes the most sense?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt0">
          <label for="decision_10_opt0">gemini.Realtime(fps=10) — full multimodal streaming with voice</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the best fit! A tutor needs to both see the whiteboard AND have a voice conversation with the student. Gemini Realtime handles both video frames and audio natively, so the student can ask &#x27;where did I go wrong?&#x27; and the AI can reference what it sees. 10fps is reasonable since math problems don&#x27;t change frame-to-frame as fast as a golf swing.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt1">
          <label for="decision_10_opt1">gemini.LLM(&#x27;gemini-2.5-flash-lite&#x27;) with separate TTS/STT</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Flash-lite is optimized for speed and cost, but a tutoring session benefits from the richer reasoning of Realtime mode. Also, using separate TTS/STT adds complexity without clear benefit here — you&#x27;re not doing anything special with the audio pipeline that would require separate control.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt2">
          <label for="decision_10_opt2">openai.Realtime(fps=30) — maximum frame rate for accuracy</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. 30fps for a whiteboard? That&#x27;s massive overkill — math doesn&#x27;t change 30 times per second. You&#x27;d be burning through API credits for frames that are essentially identical. OpenAI Realtime is also currently more expensive per frame than Gemini. The code comment in the repo literally warns: &#x27;Careful with FPS can get expensive.&#x27;</div>
        </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Design and prompt your agent to build a &#x27;Gym Form Checker&#x27; — a vision agent that watches someone exercise via webcam and gives real-time form corrections with voice feedback</div>
      <div class="your-turn-context">This is basically a variation of the golf coach, but for general gym exercises. The tricky part: it needs to handle multiple exercise types (squats, deadlifts, push-ups) and switch its analysis based on what it detects the person doing. Think about how the instructions prompt needs to be structured to handle this flexibility.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What processor do you need to detect body pose during exercises?</li><li>How should the instructions prompt handle different exercises — one giant prompt or a dynamic approach?</li><li>What FPS makes sense for exercise form? (Hint: think about how fast a squat is vs. a golf swing)</li><li>When should the agent speak vs. stay quiet? (Nobody wants interrupted mid-rep)</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Build me a real-time gym form checker using Stream&#x27;s Vision Agents framework. It should:

1. Use YOLOPoseProcessor to track body keypoints during exercises
2. Use Gemini Realtime at 5fps for the reasoning and voice coaching
3. Include an instructions.md that covers: detecting which exercise the user is doing (squat, deadlift, push-up, overhead press), key form cues for each exercise (knee tracking over toes for squats, neutral spine for deadlifts, etc.), a rule to only give feedback between reps — not mid-rep — to avoid breaking concentration, and an encouraging coaching tone
4. Use Stream&#x27;s edge network for the video transport

Also write the instructions.md prompt file. The coach should track rep count, acknowledge good form, and only interrupt for safety-critical form breakdowns (like a rounding lower back on deadlifts).</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>We just walked through Stream&#x27;s Open Vision Agents framework — a genuinely elegant approach to building AI that can watch, listen, and respond to live video.</p>

<p>The core architectural pattern is worth remembering even if you never use this specific framework: <strong>fast detection models</strong> (YOLO, face recognition) handle the high-frequency work of tagging what&#x27;s in each frame, while <strong>reasoning LLMs</strong> (Gemini, OpenAI) handle the slower but smarter work of understanding what&#x27;s happening and deciding how to respond. An <strong>edge network</strong> keeps the whole loop under 30ms.</p>

<p>What makes Vision Agents compelling isn&#x27;t any single piece — YOLO and Gemini existed before this. It&#x27;s the <strong>composability</strong>. The processor chain pattern means you can stack detection models like building blocks, swap LLMs without changing your pipeline, and add voice interaction with two extra parameters. That&#x27;s the kind of framework design that actually gets used.</p></div>
      <ul class="takeaways-list"><li>The &#x27;fast eyes + smart brain&#x27; pattern (cheap detection models feeding expensive reasoning LLMs) is the fundamental architecture for real-time AI — and it applies far beyond video</li><li>Processor chains are composable: stack YOLO, face recognition, custom detectors, and domain-specific logic as a pipeline that feeds annotated frames to your LLM</li><li>The instructions prompt (that markdown file) is where 80% of your agent&#x27;s behavior lives — optimizing that prompt matters more than optimizing the code</li><li>FPS settings are your biggest cost lever: YOLO at 30fps is nearly free (runs locally), but sending 30fps to Gemini Realtime will drain your API budget fast</li><li>Multi-modal means more than vision — combining STT, TTS, and video creates agents that can have natural conversations about what they see</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Clone the Vision Agents repo and run the golf coach example locally — seeing YOLO pose detection on your own webcam is a &#x27;wow&#x27; moment</li><li>Experiment with the instructions prompt — try making the golf coach give feedback in different styles (drill sergeant, zen master, sports commentator) and see how it changes the experience</li><li>Design a custom processor for a domain you know well — the SecurityCameraProcessor example shows the pattern for wrapping detection models with business logic</li><li>Explore the client SDKs (React, iOS, Android, Flutter) — the Python agent is the backend, but Stream provides native frontends so you can ship real apps</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/GetStream/Vision-Agents" target="_blank" rel="noopener">GetStream/Vision-Agents</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis tool that collects data, breaks filter bubbles, and predicts trends.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching how to build AI agents from scratch, covering theory and practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and various file formats into Markdown, with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Modular node-based GUI for building and running image generation pipelines with diffusion models.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Community-curated list of free programming books and learning resources in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues AI coding tools simplify writing code but raise the bar for actual engineering judgment.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">OpenFang</div>
            <div class="oa-summary">Open-source operating system designed for building, deploying, and managing AI agents.</div>
            <div class="oa-meta">Product Hunt AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a trillion-parameter language model locally using a cluster of Ryzen AI laptops.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "OpenFang", "tags": "agents,open-source", "source": "Product Hunt AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "general";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>