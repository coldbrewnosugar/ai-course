<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DeerFlow 2.0: Build a Creative AI Pipeline with Sub-Agents and Sandboxes — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-image-gen">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>DeerFlow 2.0: Your Personal Creative AI Swarm</h1>
      <div class="hero-subtitle">ByteDance&#x27;s open-source super-agent hit #1 on GitHub Trending. Let&#x27;s set it up and build a visual content pipeline where sub-agents research, generate, and iterate — all inside sandboxed environments.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">multi-agent</span> <span class="tag">sandbox</span> <span class="tag">creative-pipelines</span> <span class="tag">open-source</span> <span class="tag">orchestration</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>On February 28th, 2026, <strong>DeerFlow 2.0</strong> claimed the #1 spot on GitHub Trending — and for good reason. ByteDance completely rewrote their open-source agent framework from scratch (zero shared code with v1), turning what was originally a &quot;Deep Research&quot; tool into a full-blown <strong>super-agent harness</strong>.</p>

<p>Here&#x27;s why this matters for creative work: DeerFlow doesn&#x27;t just call an LLM and return text. It orchestrates <strong>sub-agents</strong> that each have their own specialty — one might research visual references on the web, another might write and execute Python code inside a <strong>sandboxed Docker container</strong> to generate images, and a third might critique the results and suggest refinements. All of this is coordinated through a <strong>skills system</strong> that you can extend, backed by <strong>persistent memory</strong> so the system actually learns from previous runs.</p>

<p>Think of it like hiring a small creative agency, except every employee is an AI agent and the office is a Docker container. Today we&#x27;re going to set up that agency and give it its first creative brief.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Get the Lay of the Land: DeerFlow&#x27;s Architecture</h2>
      </div>
      <div class="step-body">
        <p>Before we touch any code, let&#x27;s understand what DeerFlow actually <em>is</em>. Most people hear &quot;agent framework&quot; and think of a single LLM with tool access. DeerFlow is more like an <strong>orchestra conductor</strong> — it doesn&#x27;t play any instrument itself, but it decides who plays what and when.</p>

<p>The architecture has five key pieces:</p>

<ul>
<li><strong>Sub-agents</strong>: Specialized workers. One might be great at web research, another at writing code, another at analysis. They&#x27;re not generic — each has a focused role.</li>
<li><strong>Sandbox</strong>: An isolated execution environment (Docker container) where agents can run code without touching your actual machine. This is <em>critical</em> for image generation — you want Python scripts running diffusion models in a container, not loose on your laptop.</li>
<li><strong>Skills</strong>: Think of these as &quot;plugins&quot; or &quot;recipes&quot; that teach agents new tricks. A skill might be &quot;generate an image using Stable Diffusion&quot; or &quot;create a video storyboard.&quot;</li>
<li><strong>Memory</strong>: Persistent context that survives across sessions. The system remembers what worked, what didn&#x27;t, and what you prefer.</li>
<li><strong>Tools</strong>: External capabilities like web search (via Tavily), file I/O, and MCP servers.</li>
</ul>

<p>The mental model I like: <strong>sub-agents are the team, skills are their training, sandbox is their workshop, memory is their notebook, and tools are their equipment.</strong></p>

<p>Let&#x27;s get our AI agent to help us set up the local environment.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your AI agent to walk you through cloning DeerFlow and generating the initial configuration — tailored to a creative image-generation use case.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What prerequisites does DeerFlow need? (Think: Node.js version, Python tooling, Docker...)</li><li>The README mentions `make config` — what does that actually generate for you?</li><li>We want image generation eventually. Which sandbox mode would make sense for running arbitrary Python code safely?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a step-by-step setup sequence: clone the repo, run <code>make config</code> to generate <code>config.yaml</code> and <code>.env</code> from templates, then run <code>make check</code> to verify prerequisites (Node.js 22+, pnpm, uv, nginx). It should flag that you&#x27;ll want <strong>Docker sandbox mode</strong> since we&#x27;ll be running image-generation code. The key commands are just three lines:</p>

<p>```bash</p>
<p>git clone https://github.com/bytedance/deer-flow.git</p>
<p>cd deer-flow</p>
<p>make config</p>
<p>```</p>

<p>The agent should explain that <code>make config</code> creates your local config files from example templates — you&#x27;ll edit these next.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        DeerFlow 2.0 is a complete ground-up rewrite — it shares zero code with v1. If you find old tutorials referencing the <code>1.x</code> branch, that&#x27;s the original &quot;Deep Research&quot; framework. We&#x27;re working with the new hotness.
      </div>
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        You&#x27;ll need Docker Desktop running for sandbox mode. If you&#x27;re on a machine where Docker isn&#x27;t an option, DeerFlow also supports a local execution mode — but you lose the isolation that makes running arbitrary generated code safe.
      </div>
        
      <details class="reveal">
        <summary>Why does the sandbox matter so much for creative work?</summary>
        <div class="reveal-body"><p>Here&#x27;s the thing about image generation pipelines: they involve running <em>generated</em> Python code that imports heavy libraries (torch, diffusers, PIL), downloads model weights, and writes files to disk. Without a sandbox, a hallucinated <code>rm -rf</code> or a corrupted dependency could wreck your system.</p>

<p>Docker sandbox mode means each code execution spins up in an isolated container with its own filesystem. The agent writes code, the sandbox runs it, results come back — and if anything goes wrong, you just throw away the container. It&#x27;s like having a disposable art studio that you can trash and rebuild in seconds.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Configure Your Models and API Keys</h2>
      </div>
      <div class="step-body">
        <p>DeerFlow&#x27;s <code>config.yaml</code> is where the magic routing happens. Unlike simpler frameworks where you pick one model and go, DeerFlow lets you configure <strong>multiple models</strong> and assign them to different roles. This is actually a really smart design choice for creative work.</p>

<p>Think about it: you might want a fast, cheap model (like GPT-4o-mini or Claude Haiku) handling the research sub-agent that&#x27;s just fetching and summarizing references. But for the actual creative direction — deciding composition, color palette, art style — you probably want your heavy hitter (GPT-4, Claude Opus, etc.).</p>

<p>The config uses <strong>LangChain class paths</strong> under the hood, which means you can plug in basically any model provider: OpenAI, Anthropic, local models via Ollama, whatever. DeerFlow doesn&#x27;t care — it just needs a LangChain-compatible wrapper.</p>

<p>Let&#x27;s get our agent to help us write a config optimized for creative image generation.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to help you write a config.yaml that sets up two models — a powerful one for creative decisions and a fast one for research — plus the Docker sandbox configuration.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What goes in each model entry? (name, display_name, use, model, api_key, max_tokens, temperature...)</li><li>For creative work, would you want a higher or lower temperature? Why?</li><li>The sandbox section needs to specify Docker mode — what would your agent need to know about that?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a <code>config.yaml</code> snippet with two model entries. The key insight is the <code>use</code> field — it&#x27;s a LangChain class path like <code>langchain_openai:ChatOpenAI</code> or <code>langchain_anthropic:ChatAnthropic</code>. For the creative model, temperature around 0.8-0.9 encourages variety. For research, 0.3-0.5 keeps things factual. The sandbox config should point to Docker execution mode. The whole thing is maybe 15 lines of YAML — nothing scary.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        DeerFlow needs at minimum two API keys: one for your LLM provider (OpenAI, Anthropic, etc.) and one for <strong>Tavily</strong> (the web search tool that research sub-agents use). Put both in your <code>.env</code> file. Tavily has a free tier that&#x27;s generous enough for this workshop.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Use environment variable references (<code>$OPENAI_API_KEY</code>) in your config.yaml instead of pasting raw keys. DeerFlow reads your <code>.env</code> file automatically. Future you will thank present you.
      </div>
        
      <details class="reveal">
        <summary>What&#x27;s this LangChain class path thing?</summary>
        <div class="reveal-body"><p>The <code>use</code> field in each model config (like <code>langchain_openai:ChatOpenAI</code>) tells DeerFlow which Python class to instantiate for that model. It&#x27;s a clever abstraction — DeerFlow itself doesn&#x27;t implement any model integrations. It piggybacks on LangChain&#x27;s massive library of connectors.</p>

<p>This means if you want to use a local model via Ollama, you&#x27;d just set <code>use: langchain_ollama:ChatOllama</code>. Want to use Google&#x27;s Gemini? <code>langchain_google_genai:ChatGoogleGenerativeAI</code>. The DeerFlow team doesn&#x27;t need to support every provider — LangChain already does.</p>

<p>The tradeoff? You inherit LangChain&#x27;s dependency weight. But honestly, for a framework that already needs Docker and Node.js 22, a few extra Python packages are a rounding error.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point you should have: (1) DeerFlow cloned locally, (2) `make config` run to generate your config files, (3) `config.yaml` edited with at least one model and Docker sandbox mode, and (4) API keys in your `.env` file. If `make check` passes, you&#x27;re golden. If not, ask your agent to troubleshoot the specific error — it&#x27;s almost always a missing Node.js version or Docker not running.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Launch DeerFlow and Meet Your Sub-Agents</h2>
      </div>
      <div class="step-body">
        <p>Time to spin this thing up. DeerFlow&#x27;s Docker setup is refreshingly straightforward — two <code>make</code> commands and you&#x27;ve got a full agent orchestration system running on <code>localhost:2026</code>.</p>

<p>But before we launch, let&#x27;s talk about what&#x27;s actually happening when you hit that URL. DeerFlow&#x27;s web UI isn&#x27;t just a chatbot interface — it&#x27;s a <strong>mission control</strong> where you can:</p>

<ul>
<li>See each sub-agent&#x27;s status and what it&#x27;s working on</li>
<li>Watch sandbox code execution in real-time</li>
<li>Browse the system&#x27;s memory and see what it&#x27;s learned</li>
<li>Define and trigger skills</li>
</ul>

<p>The sub-agent orchestration is the star of the show. When you give DeerFlow a task like &quot;create a cyberpunk cityscape illustration,&quot; it doesn&#x27;t just fire off one prompt. The <strong>coordinator agent</strong> breaks the task down, assigns pieces to specialized sub-agents, and manages the flow between them. One agent researches reference images. Another writes the image-generation code. Another executes it in the sandbox. Another critiques the result.</p>

<p>It&#x27;s honestly a bit like watching a relay race where each runner has a different specialty.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain the Docker launch sequence and what each service does, then have it help you verify everything&#x27;s running correctly.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>There are two make commands for Docker — what does each one do?</li><li>What&#x27;s the &#x27;provisioner&#x27; service, and when does it start?</li><li>Once it&#x27;s running, how would you verify all services are healthy?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain: <code>make docker-init</code> pulls the sandbox Docker image (one-time setup), and <code>make docker-start</code> launches all services. It should note that the provisioner service only starts if your config.yaml uses the <code>AioSandboxProvider</code> — which is the container-based sandbox mode we want. Once running, hit <code>http://localhost:2026</code> and you should see DeerFlow&#x27;s web dashboard. The agent might suggest checking <code>docker ps</code> to confirm all containers are up.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        First launch pulls a decent-sized Docker image for the sandbox. Grab some coffee — on a typical connection it takes 2-5 minutes. Subsequent starts are fast because the image is cached.
      </div>
        
      <details class="reveal">
        <summary>How does sub-agent orchestration actually work under the hood?</summary>
        <div class="reveal-body"><p>DeerFlow&#x27;s orchestration follows a pattern called <strong>hierarchical task decomposition</strong>. The coordinator agent receives your high-level request, then:</p>

<ol>
<li><strong>Plans</strong>: Breaks the task into sub-tasks with dependencies (&quot;research must finish before code generation can start&quot;)</li>
<li><strong>Assigns</strong>: Routes each sub-task to the best-suited sub-agent based on its capabilities</li>
<li><strong>Monitors</strong>: Watches progress and handles failures (if the code agent&#x27;s sandbox execution fails, it can retry with modified code)</li>
<li><strong>Synthesizes</strong>: Combines results from all sub-agents into a coherent output</li>
</ol>

<p>This is conceptually similar to how a project manager works — they don&#x27;t do the individual tasks, but they make sure the right people do them in the right order. The key difference from simpler &quot;agent chain&quot; approaches is that DeerFlow&#x27;s coordinator can dynamically re-plan if something goes wrong, rather than just failing on the first error.</p></div>
      </details>
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You want DeerFlow to generate an image using a diffusion model. The code agent writes a Python script that imports `torch` and `diffusers`. Where should this script execute?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt0">
          <label for="decision_6_opt0">In the Docker sandbox container</label>
          <div class="decision-feedback correct">&#10003; Correct! Correct! The sandbox container is pre-configured with GPU passthrough (if available) and isolated from your host filesystem. Running arbitrary generated code — especially code that downloads large model weights and manipulates tensors — is exactly what the sandbox is built for. If the script crashes or does something unexpected, the container is disposable.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt1">
          <label for="decision_6_opt1">Directly on the host machine in local execution mode</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This technically works, but it&#x27;s risky. The code agent generates Python scripts dynamically — which means bugs, hallucinated imports, or unintended file operations could affect your host system. Local execution mode is fine for trusted, simple scripts, but image generation code with heavy dependencies belongs in a sandbox.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt2">
          <label for="decision_6_opt2">In a separate cloud VM via API</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. DeerFlow doesn&#x27;t have a built-in cloud VM execution mode. You could technically set up something like this via MCP or a custom skill, but the Docker sandbox is the designed solution for this exact use case. Don&#x27;t over-engineer it when the framework already has what you need.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Build a Creative Pipeline: Research → Generate → Iterate</h2>
      </div>
      <div class="step-body">
        <p>Alright, the fun part. We&#x27;ve got DeerFlow running — now let&#x27;s actually use it for something creative.</p>

<p>Here&#x27;s the workflow we&#x27;re going to build: a <strong>visual content pipeline</strong> where DeerFlow coordinates multiple sub-agents to go from a vague creative brief to an actual generated image. The flow looks like this:</p>

<ol>
<li><strong>Research agent</strong> searches the web for visual references, art styles, and composition ideas related to your prompt</li>
<li><strong>Planning agent</strong> synthesizes the research into a specific image generation plan (dimensions, style parameters, model choice, prompt engineering)</li>
<li><strong>Code agent</strong> writes a Python script that uses a diffusion model to generate the image, runs it inside the sandbox</li>
<li><strong>Critique agent</strong> evaluates the result and suggests improvements</li>
<li><strong>Loop</strong>: The code agent revises and re-generates based on the critique</li>
</ol>

<p>The beautiful thing is you don&#x27;t need to wire this up manually. DeerFlow&#x27;s skill system lets you define this as a <strong>reusable pipeline</strong> — kind of like saving a recipe that your agent kitchen can cook anytime.</p>

<p>The key concept here is that <strong>memory ties it all together</strong>. When the research agent finds that &quot;cyberpunk art typically uses neon colors against dark backgrounds,&quot; that insight gets stored in DeerFlow&#x27;s memory. The next time you ask for cyberpunk anything, the system already knows this — it doesn&#x27;t start from zero.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to help you craft the initial creative prompt for DeerFlow and explain how to structure it so the sub-agent orchestration kicks in effectively.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What makes a good prompt for a multi-agent system vs. a single LLM? (Think about what each sub-agent needs...)</li><li>Should you be specific about the end output format, or leave room for the research agent to discover options?</li><li>How might you mention that you want iterative refinement — not just one shot?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that DeerFlow prompts work best when you give a <strong>clear creative intent</strong> but leave room for the sub-agents to contribute. Something like: &quot;Research cyberpunk cityscape art styles, then generate an image using a diffusion model in the sandbox. I want neon-heavy, rain-slicked streets. Iterate at least twice based on composition critique.&quot; The agent should note that mentioning &quot;research,&quot; &quot;sandbox,&quot; and &quot;iterate&quot; are signal words that help DeerFlow&#x27;s coordinator activate the right sub-agents. It&#x27;s like writing a creative brief for a team, not a precise instruction for a single person.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        DeerFlow&#x27;s memory means your second and third creative sessions get dramatically better. After the first run, the system has learned what models work well for your style, what prompt patterns produce good results, and what critique patterns are most useful. It&#x27;s like breaking in a new pair of shoes — awkward at first, great after.
      </div>
        
      <details class="reveal">
        <summary>How do DeerFlow skills work for creative workflows?</summary>
        <div class="reveal-body"><p>Skills in DeerFlow are essentially <strong>packaged capabilities</strong> that teach the system how to perform specific tasks. They&#x27;re more structured than just a prompt — a skill includes:</p>

<ul>
<li><strong>Trigger conditions</strong>: When should this skill activate? (e.g., &quot;when the user asks for image generation&quot;)</li>
<li><strong>Sub-agent configuration</strong>: Which agents are involved and what are their roles?</li>
<li><strong>Tool requirements</strong>: What external tools does the skill need? (e.g., sandbox access, specific Python libraries)</li>
<li><strong>Memory hooks</strong>: What should the system remember from this skill&#x27;s execution?</li>
</ul>

<p>For creative work, you might define a skill like &quot;style-transfer-pipeline&quot; that knows it needs a research agent to find the source style, a code agent with sandbox access to run the transfer model, and a critique agent to evaluate results. Once defined, you can invoke this skill by name in future sessions.</p>

<p>The extensibility is the real power move — the DeerFlow community is already publishing skills as shareable packages. Imagine a plugin marketplace, but for agent capabilities.</p></div>
      </details>
      <details class="reveal">
        <summary>What models can the sandbox actually run?</summary>
        <div class="reveal-body"><p>The sandbox is a Docker container, so theoretically anything that runs in Docker runs in the sandbox. In practice, for image generation you&#x27;d typically use:</p>

<ul>
<li><strong>Stable Diffusion</strong> (via the <code>diffusers</code> library) — the workhorse for most generation tasks</li>
<li><strong>SDXL</strong> — higher resolution, better at complex scenes</li>
<li><strong>ControlNet variants</strong> — when you need structural control over the output</li>
</ul>

<p>The catch: the sandbox container needs to have GPU passthrough configured if you want reasonable generation times. CPU-only diffusion is <em>possible</em> but painfully slow (minutes per image vs. seconds). DeerFlow&#x27;s Docker setup supports NVIDIA GPU passthrough if you have the right drivers installed.</p>

<p>For this workshop, even CPU generation works fine — the images just take longer. The architectural patterns are the same regardless.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Extend the Pipeline: Adding MCP Tools and Custom Skills</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s where DeerFlow gets really interesting — and honestly, this is the part that made me go &quot;oh, this is different from the other agent frameworks.&quot;</p>

<p>DeerFlow supports <strong>MCP (Model Context Protocol) servers</strong> as a first-class integration. If you&#x27;ve been following the MCP ecosystem, you know it&#x27;s becoming the USB-C of AI tool connections — a standard way for AI systems to connect to external data sources and capabilities.</p>

<p>What this means for our creative pipeline: you can plug in MCP servers for things like:</p>
<ul>
<li><strong>Figma access</strong> — let the research agent pull design references directly from your Figma files</li>
<li><strong>Image hosting</strong> — automatically upload generated images to S3 or Cloudinary</li>
<li><strong>Version control</strong> — track iterations of generated images with metadata</li>
</ul>

<p>But the <em>real</em> extensibility play is the <strong>skill system</strong>. Skills are DeerFlow&#x27;s answer to &quot;how do I teach this thing new tricks without modifying the core code?&quot; You define a skill as a structured configuration, and DeerFlow&#x27;s coordinator learns when and how to use it.</p>

<p>Think of skills like apps on your phone. The phone (DeerFlow) provides the platform. Each app (skill) adds a new capability. You don&#x27;t need to understand the phone&#x27;s operating system to install a new app.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Ask your agent to explain how you&#x27;d create a custom DeerFlow skill for a &#x27;mood board generator&#x27; — something that researches visual styles, collects reference images, and organizes them with annotations.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What would the skill&#x27;s trigger conditions look like?</li><li>Which sub-agents would be involved, and what would each one do?</li><li>How would memory help this skill improve over time?</li><li>Would this skill need sandbox access, or is it more of a research-and-organize task?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should walk you through the conceptual structure of a DeerFlow skill: you&#x27;d define trigger conditions (keywords like &#x27;mood board&#x27; or &#x27;visual references&#x27;), specify which sub-agents participate (research agent for finding images, analysis agent for categorizing them, a code agent if you want to generate a visual collage in the sandbox), and configure memory hooks so the system remembers your style preferences. The key insight: this skill probably doesn&#x27;t need heavy sandbox access — it&#x27;s mostly research and organization, with maybe a lightweight code step to arrange images into a grid. The agent should emphasize that skills are <em>declarative</em> — you describe what you want, not how to implement it.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        DeerFlow&#x27;s MCP integration means you can connect it to any MCP-compatible server. The ecosystem is growing fast — check the MCP server registry for tools that might fit your creative workflow. Image analysis, font identification, color palette extraction — there&#x27;s probably already an MCP server for it.
      </div>
        
      <details class="reveal">
        <summary>DeerFlow vs. other agent frameworks — what&#x27;s actually different?</summary>
        <div class="reveal-body"><p>Fair question. There are a LOT of agent frameworks right now. Here&#x27;s what makes DeerFlow&#x27;s architecture distinct:</p>

<p><strong>vs. AutoGen/CrewAI</strong>: Those frameworks focus on conversational agent teams. DeerFlow adds sandboxed execution and persistent memory as core primitives, not afterthoughts. The sandbox is deeply integrated — agents can write code, execute it, see the results, and iterate, all within the orchestration loop.</p>

<p><strong>vs. LangGraph</strong>: LangGraph gives you lower-level graph-based workflow control. DeerFlow is more opinionated — it provides the orchestration patterns out of the box. Less flexible at the edges, but way faster to get productive.</p>

<p><strong>vs. OpenAI Assistants</strong>: DeerFlow is fully open-source and model-agnostic. You can use any LLM provider, run everything locally, and inspect every step. No black boxes.</p>

<p>The real differentiator is the <strong>skill + memory + sandbox</strong> triangle. Skills define capabilities, memory accumulates context over time, and sandboxes provide safe execution. That combination is uniquely powerful for creative workflows where you&#x27;re iterating toward a subjective goal.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have DeerFlow running on `localhost:2026` with Docker sandbox mode, at least one model configured, and you&#x27;ve sent your first creative prompt through the system. You&#x27;ve seen how the coordinator breaks tasks into sub-agent assignments, how the sandbox executes generated code, and how memory persists between interactions. If something went sideways, check `docker logs` for the specific failing service — DeerFlow&#x27;s logging is pretty decent.</div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Create a DeerFlow prompt that uses the research sub-agent to find three distinct art movements (e.g., Art Deco, Bauhaus, Vaporwave), then has the code agent generate a comparison grid showing a simple landscape rendered in each style — all inside the sandbox.</div>
      <div class="your-turn-context">This challenges you to think about multi-step orchestration: the research phase must complete before the generation phase, and the code agent needs to produce a single composite image rather than three separate ones. It&#x27;s a great test of how well you can steer DeerFlow&#x27;s sub-agent coordination through your prompt.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>How do you signal to DeerFlow that the research step should happen BEFORE the generation step? (Think about dependency language...)</li><li>What should you tell the code agent about the output format — a grid, not separate images?</li><li>Should you name the specific art movements, or let the research agent discover interesting ones?</li><li>How might you ask for iteration — maybe the critique agent should suggest which style worked best?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Research three visually distinctive art movements that have strong, recognizable characteristics (surprise me — don&#x27;t just pick the obvious ones). For each movement, summarize the key visual elements: color palettes, line styles, composition patterns, and typical subject treatment.

Then, using the sandbox, write a Python script that generates a 3-panel comparison grid showing the same simple landscape scene (rolling hills, a sunset, a single tree) rendered in each of the three art styles. Use a diffusion model with style-specific prompts derived from the research phase. The output should be a single composite image, not three separate files.

After generation, critique the result: which style was most faithfully represented? Which one would benefit from prompt refinement? Iterate once on the weakest panel.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You notice DeerFlow&#x27;s research sub-agent is producing great visual references, but the code agent&#x27;s generated images don&#x27;t match the researched styles well. What&#x27;s the most likely fix?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt0">
          <label for="decision_11_opt0">Improve the handoff — make the research agent output structured style parameters that the code agent can directly use in its diffusion prompts</label>
          <div class="decision-feedback correct">&#10003; Correct! Bingo. The problem is usually in the *translation* between sub-agents. If the research agent writes a beautiful prose description of Art Deco style, the code agent still has to figure out how to turn that into effective diffusion model parameters. By asking the research agent to output structured attributes (color hex codes, line weight descriptions, composition rules), you give the code agent concrete inputs to work with. This is a core principle of multi-agent design: optimize the interfaces between agents, not just the agents themselves.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt1">
          <label for="decision_11_opt1">Switch to a more powerful model for the code agent</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. A more powerful model might write slightly better code, but the real bottleneck is the information flow between agents, not the code generation ability. If the code agent doesn&#x27;t receive clear, actionable style parameters from the research phase, even GPT-5 would struggle to faithfully reproduce a specific art style. Fix the interface first, then upgrade the model if needed.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt2">
          <label for="decision_11_opt2">Skip the research agent and hardcode the style descriptions</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This works as a quick fix but defeats the purpose of the multi-agent pipeline. The whole point is that the research agent can discover nuances you wouldn&#x27;t think to hardcode — like the fact that Art Deco uses specific geometric proportions, not just &#x27;geometric shapes.&#x27; Hardcoding trades short-term convenience for long-term capability.</div>
        </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>We went from zero to a running DeerFlow 2.0 instance with a creative image-generation pipeline. But more importantly, we explored a <em>fundamentally different</em> way of thinking about AI-assisted creative work.</p>

<p>Instead of one model doing everything, we orchestrated a <strong>team of specialized sub-agents</strong> — each contributing what it&#x27;s best at. The research agent finds references. The code agent writes generation scripts. The sandbox runs them safely. The critique agent drives iteration. And memory means the whole system gets smarter over time.</p>

<p>This is the direction creative AI tooling is heading: not bigger models, but <strong>better orchestration</strong> of specialized capabilities. DeerFlow 2.0 is one of the first open-source frameworks to make this pattern accessible.</p>

<p>The skill system is what gives this legs beyond a single workshop. Once you&#x27;ve defined a creative pipeline as a skill, it becomes a reusable capability — like adding a new tool to your creative toolbox that gets better every time you use it.</p></div>
      <ul class="takeaways-list"><li>Multi-agent orchestration (research → generate → critique → iterate) produces better creative results than single-shot prompting, because each sub-agent can focus on what it does best</li><li>Sandboxed code execution is non-negotiable for image generation pipelines — Docker containers give you safe, disposable environments where generated code can run without risk to your system</li><li>The interface between agents matters more than the agents themselves — structured handoffs (style parameters, composition rules) beat prose descriptions every time</li><li>Persistent memory transforms one-off generation into an improving creative partnership — DeerFlow remembers what works and builds on it across sessions</li><li>DeerFlow&#x27;s skill system turns ad-hoc workflows into reusable, shareable capabilities — today&#x27;s experiment becomes tomorrow&#x27;s reliable tool</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Define a custom DeerFlow skill for your most common creative workflow — whether that&#x27;s thumbnail generation, style exploration, or visual prototyping</li><li>Explore DeerFlow&#x27;s MCP integration to connect external tools (Figma, image hosting, asset libraries) into your creative pipeline</li><li>Experiment with different model assignments — try using a vision model for the critique agent to get visual (not just textual) feedback on generated images</li><li>Check out the DeerFlow community&#x27;s shared skill library — other creators are publishing reusable pipelines you can plug into your instance</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/bytedance/deer-flow" target="_blank" rel="noopener">bytedance/deer-flow</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis assistant that collects, analyzes, and predicts trends in online discourse.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching how to build AI agents from scratch, covering theory and practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and various file formats into Markdown, with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Modular node-based GUI for building and running diffusion model image generation workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Curated list of freely available programming books and learning resources in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">If AI writes code, should the session be part of the commit?</div>
            <div class="oa-summary">Tool and discussion about including AI conversation transcripts as metadata in git commits.</div>
            <div class="oa-meta">Hacker News AI · Mar 2</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues that AI coding tools simplify writing code but increase the complexity of engineering decisions.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a trillion-parameter language model locally using a cluster of Ryzen AI Max+ systems.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "If AI writes code, should the session be part of the commit?", "tags": "coding", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "image-gen";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>