<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GUI-Owl 1.5: Multi-Platform Vision Agent Models You Can Run Today — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-image-gen">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Teach a Model to See Your Screen — Then Click Things</h1>
      <div class="hero-subtitle">GUI-Owl 1.5 is a family of vision models that can look at any interface and actually operate it. Let&#x27;s pull one down and put it to work.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">vision-models</span> <span class="tag">gui-agents</span> <span class="tag">qwen3-vl</span> <span class="tag">huggingface</span> <span class="tag">automation</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Two weeks ago, Alibaba&#x27;s Tongyi Lab released <strong>GUI-Owl 1.5</strong> — and it&#x27;s kind of a big deal. Here&#x27;s the short version:</p>

<p>Most vision models can <em>describe</em> what&#x27;s on a screen. GUI-Owl can describe it, understand the UI structure, locate specific elements by coordinates, and <em>tell you what to click next</em> to accomplish a task. It&#x27;s a purpose-built GUI automation brain.</p>

<p>The model family ranges from a tiny <strong>2B</strong> version (runs on a laptop) all the way up to <strong>235B</strong> (you&#x27;ll need a cluster for that one). They&#x27;re all built on top of <strong>Qwen3-VL</strong>, Alibaba&#x27;s vision-language architecture, and the weights are sitting right there on HuggingFace.</p>

<p>What makes this interesting isn&#x27;t just the model — it&#x27;s the ecosystem. GUI-Owl powers <strong>Mobile-Agent-v3.5</strong>, a multi-agent framework where one agent plans, another perceives the screen, and another reflects on whether things are going right. Think of it as a team of AI coworkers who can actually use a computer.</p>

<p>The models come in two flavors: <strong>Instruct</strong> (fast, direct answers) and <strong>Thinking</strong> (chain-of-thought reasoning before acting). They hit state-of-the-art on 20+ GUI benchmarks, including grounding, tool calling, MCP integration, and long-horizon memory tasks.</p>

<p>Today we&#x27;re going to pull down the <strong>7B Instruct</strong> model — big enough to be capable, small enough to actually run — and explore what it can do.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Get the Model Running Locally</h2>
      </div>
      <div class="step-body">
        <p>Before we do anything fancy, we need GUI-Owl sitting on our machine and responding to prompts. The model is built on Qwen3-VL, which means it plays nicely with the <code>transformers</code> library and <code>vLLM</code> for faster inference.</p>

<p>Here&#x27;s the mental model: GUI-Owl is a <strong>vision-language model</strong> (VLM). You feed it an image (a screenshot) plus a text prompt (&quot;what&#x27;s on this screen?&quot; or &quot;where should I click to open Settings?&quot;), and it gives you back text — descriptions, coordinates, action plans.</p>

<p>Think of it like giving someone a photograph and asking them a question about it. Except this someone has been trained on millions of UI screenshots and knows what buttons, text fields, and menus look like across desktop, mobile, and web.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to write you a setup script that downloads GUI-Owl-7B from HuggingFace and runs a basic inference — send it a screenshot and get a description back.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What libraries does a Qwen3-VL-based model need? (Think HuggingFace ecosystem.)</li><li>The model needs to handle both images and text — what kind of processor does that require?</li><li>Where does the model live on HuggingFace? The repo names follow a pattern like `X-PLUG/GUI-Owl-7B`.</li><li>How will you feed it an image? Think about loading a local screenshot file.</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a Python script that does three things: (1) installs the needed packages (<code>transformers</code>, <code>torch</code>, <code>accelerate</code>, <code>Pillow</code>), (2) loads the GUI-Owl-7B model and its processor from HuggingFace using <code>AutoModelForVision2Seq</code> and <code>AutoProcessor</code>, and (3) takes a screenshot file path, loads the image, builds a chat-style prompt with the image embedded, and runs inference. The key pattern looks something like:</p>

<p>```python</p>
<p>from transformers import AutoProcessor, AutoModelForVision2Seq</p>
<p>model = AutoModelForVision2Seq.from_pretrained(</p>
<p>&quot;X-PLUG/GUI-Owl-7B&quot;, torch_dtype=torch.bfloat16,</p>
<p>device_map=&quot;auto&quot;</p>
<p>)</p>
<p>processor = AutoProcessor.from_pretrained(&quot;X-PLUG/GUI-Owl-7B&quot;)</p>
<p>```</p>

<p>The actual inference uses the Qwen3-VL chat template format with an image token and your text query. The agent should handle the image preprocessing and tokenization for you.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        The 7B model needs about <strong>16GB of VRAM</strong> in bfloat16. If you&#x27;re on a laptop GPU, try the 2B or 4B variant instead — just swap the model name. On a Mac with Apple Silicon, you can use <code>device_map=&#x27;mps&#x27;</code> but expect slower speeds.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        If you want faster inference, ask your agent to set it up with <strong>vLLM</strong> instead of raw transformers. vLLM handles batching and memory management much better for serving. But for experimenting, transformers is simpler.
      </div>
        
      <details class="reveal">
        <summary>What&#x27;s the difference between Instruct and Thinking variants?</summary>
        <div class="reveal-body"><p><strong>Instruct</strong> models give you a direct answer — &quot;click at coordinates (450, 320)&quot;. Fast, efficient, great for simple tasks.</p>

<p><strong>Thinking</strong> models do chain-of-thought reasoning first — &quot;I see a settings icon in the top right. The user wants to change their password. I should click on Settings first, then navigate to Security.&quot; Slower, but much better for complex multi-step tasks where the model needs to plan.</p>

<p>For our workshop, Instruct is perfect. But if you later build something that needs to handle ambiguous instructions like &quot;book me a flight to Tokyo,&quot; the Thinking variant will be dramatically better at figuring out the steps.</p></div>
      </details>
      <details class="reveal">
        <summary>Why Qwen3-VL as the base?</summary>
        <div class="reveal-body"><p>Qwen3-VL is Alibaba&#x27;s latest vision-language architecture, and it has a killer feature for GUI work: <strong>dynamic resolution support</strong>. Instead of resizing every image to a fixed size (which destroys tiny text and small UI elements), Qwen3-VL can process images at their native resolution by splitting them into tiles.</p>

<p>This matters enormously for screenshots. A toolbar icon might be 24×24 pixels in a 1920×1080 screenshot. A model that squishes everything to 224×224 would literally not be able to see it. Qwen3-VL keeps that detail intact.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>GUI Grounding: From &#x27;Find the Search Bar&#x27; to Pixel Coordinates</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s where GUI-Owl gets interesting. Most vision models can tell you <em>what&#x27;s</em> on screen. GUI-Owl can tell you <em>where</em> things are — as exact pixel coordinates.</p>

<p>This capability is called <strong>GUI grounding</strong>. You say &quot;where is the search bar?&quot; and instead of &quot;it&#x27;s at the top of the page&quot; you get <code>(734, 52)</code>. That&#x27;s the kind of output you can actually feed into a mouse-click automation script.</p>

<p>Think of it like the difference between asking someone for directions and getting &quot;it&#x27;s near the park&quot; versus getting exact GPS coordinates. Both are helpful, but only one lets you write an autopilot.</p>

<p>GUI-Owl was specifically trained on grounding tasks across desktop, mobile, and browser UIs. It knows what a hamburger menu looks like on Android, what a close button looks like on macOS, and where to find the address bar in Chrome.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to build a grounding demo: take a screenshot, ask GUI-Owl to locate a specific UI element, and visualize the result by drawing a bounding box on the original image.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>How should you phrase the grounding prompt? GUI-Owl expects specific task formats — think about how you&#x27;d ask it to *locate* vs. *describe*.</li><li>The model returns coordinates — but in what format? Normalized 0-1 values? Raw pixels? You need to know to draw the box correctly.</li><li>What library would you use to draw a rectangle on an image? (Think simple Python image manipulation.)</li><li>What happens if the element isn&#x27;t on screen? How should your script handle that?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a script that: (1) loads a screenshot, (2) sends it to GUI-Owl with a grounding prompt like <code>&lt;grounding&gt;Find the element: search bar&lt;/grounding&gt;</code>, (3) parses the coordinate output from the model&#x27;s response, and (4) uses Pillow to draw a colored rectangle at those coordinates and save the annotated image.</p>

<p>The key insight is the prompt format — GUI-Owl uses special tokens like <code>&lt;grounding&gt;</code> to switch into coordinate-output mode. The response comes back with bounding box coordinates that you parse out. Your agent should also add a fallback for when the model says it can&#x27;t find the element.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Try this on different types of screenshots — a desktop app, a mobile screenshot, a web page. You&#x27;ll notice GUI-Owl handles all three because it was trained on all three platforms. Most other vision models are heavily biased toward web content.
      </div>
        
      <details class="reveal">
        <summary>How does grounding actually work under the hood?</summary>
        <div class="reveal-body"><p>During training, GUI-Owl saw millions of screenshots paired with element annotations — &quot;the Login button is at coordinates [x1, y1, x2, y2]&quot;. Over time, it learned to associate visual patterns (button shapes, text labels, icon styles) with spatial locations.</p>

<p>The clever bit is that coordinates are encoded as <strong>text tokens</strong>. The model doesn&#x27;t have a separate &quot;pointing&quot; head — it literally generates the numbers as part of its text output, like saying &quot;the button is at (234, 567)&quot;. This is elegant because it means the same model architecture handles both understanding and locating, and it can reason about spatial relationships in natural language.</p>

<p>This is different from older approaches like OWL-ViT that had specialized detection heads. The text-based approach is more flexible — you can ask compound questions like &quot;find the submit button that&#x27;s below the email field&quot; and the model reasons about spatial relationships naturally.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point, you should have GUI-Owl running locally, able to take a screenshot and (1) describe what&#x27;s on it, and (2) locate specific UI elements with coordinates. If you&#x27;re getting reasonable descriptions but weird coordinates, double-check that you&#x27;re using the grounding prompt format — the special tokens matter.</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You want to build an automation that fills out a web form. Which approach should you use?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt0">
          <label for="decision_5_opt0">Send the full page screenshot and ask GUI-Owl to plan all the steps at once</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This sounds efficient, but GUI-Owl (Instruct) works best on single-step queries. The screen changes after each action — after you click a text field, it gets focus and the cursor appears. You need to re-screenshot after each step to see the updated state. This is exactly why the Mobile-Agent framework uses a perception-action loop rather than one-shot planning.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt1">
          <label for="decision_5_opt1">Build a loop: screenshot → ask GUI-Owl what to do next → execute the action → screenshot again</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the perception-action loop pattern that Mobile-Agent-v3.5 uses. Each cycle gives the model the current state of the screen, so it can react to changes (a dropdown opening, a dialog appearing, a page loading). It&#x27;s more API calls, but it&#x27;s how real GUI automation works — the screen is dynamic, not static.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt2">
          <label for="decision_5_opt2">Use OCR to extract all text first, then use a text-only LLM to plan the clicks</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. You&#x27;d lose all the visual context — where buttons are, what&#x27;s clickable vs. what&#x27;s just a label, spatial relationships between elements. GUI-Owl&#x27;s whole point is that it understands UI *visually*, not just textually. A text-only model wouldn&#x27;t know where to click.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Build the Perception-Action Loop</h2>
      </div>
      <div class="step-body">
        <p>Now we&#x27;re getting to the good stuff. A model that can see a screen and locate elements is useful, but what we really want is a <strong>loop</strong>: look at the screen, decide what to do, do it, look again.</p>

<p>This is the core pattern behind every GUI agent, and it&#x27;s conceptually simple:</p>

<ol>
<li><strong>Perceive</strong> — take a screenshot</li>
<li><strong>Think</strong> — send it to GUI-Owl with the task (&quot;I need to open the Settings app&quot;)</li>
<li><strong>Act</strong> — execute whatever GUI-Owl says (click at coordinates, type text, scroll)</li>
<li><strong>Repeat</strong> — take a new screenshot and check if we&#x27;re done</li>
</ol>

<p>It&#x27;s like giving someone directions one turn at a time. &quot;What do you see now? Okay, turn left. What do you see now? Okay, go straight.&quot; Each step is simple — the intelligence is in the model&#x27;s ability to understand the current screen and map it to the right next action.</p>

<p>The action execution part uses standard desktop automation — <code>pyautogui</code> on desktop, ADB commands on Android, Playwright or Selenium for browsers.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to build a simple perception-action loop that takes a task description, then repeatedly screenshots the screen, asks GUI-Owl for the next action, and executes it.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What library handles taking screenshots and simulating mouse clicks on desktop? (There&#x27;s a popular one that does both.)</li><li>How should you structure the prompt to GUI-Owl so it gives back *actionable* output — not just a description, but &#x27;click at (x, y)&#x27; or &#x27;type hello&#x27;?</li><li>When should the loop stop? Think about how the model would signal that the task is complete.</li><li>What&#x27;s a reasonable safety limit so the loop doesn&#x27;t run forever if something goes wrong?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a script with a main loop that: (1) uses <code>pyautogui.screenshot()</code> to capture the current screen, (2) sends it to GUI-Owl with a prompt like &quot;Task: {user_task}. Current screen is shown. What is the next action? Respond with one of: CLICK(x,y), TYPE(text), SCROLL(direction), or DONE.&quot;, (3) parses the model&#x27;s response to extract the action type and parameters, (4) executes the action using <code>pyautogui</code>, and (5) loops until it gets DONE or hits a max-step limit (something like 20 steps).</p>

<p>The script should include a small delay between actions (half a second or so) to let the UI update, and ideally a confirmation mode where it shows you what it&#x27;s about to do before doing it. The agent should also add error handling for when the model&#x27;s output doesn&#x27;t match the expected action format.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        <strong>Real talk about safety:</strong> This loop is going to simulate mouse clicks and keyboard input on your actual machine. Start with something harmless like opening Calculator and typing <code>2+2</code>. Do NOT let it loose on your email or anything with delete buttons until you&#x27;ve added confirmation prompts. Always include a max-step limit and a keyboard shortcut to kill the script.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Add a <code>--dry-run</code> flag that shows what the model <em>would</em> do without actually clicking anything. This is invaluable for debugging — you can see if the model is locating elements correctly without risking accidental clicks.
      </div>
        
      <details class="reveal">
        <summary>How Mobile-Agent-v3.5 takes this further</summary>
        <div class="reveal-body"><p>Our simple loop is one agent doing everything. Mobile-Agent-v3.5 splits this into <strong>three specialized agents</strong>:</p>

<ol>
<li><strong>Planning Agent</strong> — breaks the high-level task into sub-goals (&quot;to send an email: open mail app → compose → fill in recipient → write body → send&quot;)</li>
<li><strong>Perception Agent</strong> (GUI-Owl) — looks at the current screen and determines the exact action for the current sub-goal</li>
<li><strong>Reflection Agent</strong> — checks whether the action was successful by comparing before/after screenshots, and triggers replanning if something went wrong</li>
</ol>

<p>There&#x27;s also a <strong>memory module</strong> for long-horizon tasks — it remembers what happened 15 steps ago so the agent doesn&#x27;t go in circles. This multi-agent architecture is why Mobile-Agent can handle complex tasks like &quot;book a restaurant on Yelp and add it to my calendar&quot; that involve multiple apps and dozens of steps.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Make It Actually Useful: Task-Specific Prompting</h2>
      </div>
      <div class="step-body">
        <p>You&#x27;ve got the loop running, but honestly, the quality of GUI-Owl&#x27;s output depends hugely on <strong>how you prompt it</strong>. This is where the &quot;agent-first&quot; mindset really pays off — you&#x27;re not coding the intelligence, you&#x27;re shaping the conversation with the model.</p>

<p>There are a few prompt patterns that make a big difference:</p>

<p><strong>Be specific about the platform.</strong> Saying &quot;I&#x27;m on macOS Sonoma, using Chrome&quot; helps the model predict UI patterns — where the address bar is, what right-click menus look like, how notifications appear.</p>

<p><strong>Describe the current goal, not just the end goal.</strong> Instead of &quot;send an email to Bob&quot; every single step, say &quot;I need to click the Compose button to start a new email&quot; for the current step. The model performs better with focused, immediate goals.</p>

<p><strong>Ask for structured output.</strong> Free-form responses like &quot;you should probably click the button in the upper right&quot; are useless for automation. Constrain the output format so you can parse it reliably.</p>

<p>Let&#x27;s refine our loop with better prompting.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to create a prompt template system — a set of reusable prompts for different GUI tasks (navigation, form filling, information extraction) that produce consistent, parseable output from GUI-Owl.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What are the distinct *types* of GUI actions? (Clicking, typing, scrolling, reading — each might need a different prompt structure.)</li><li>How can you make the model&#x27;s output format consistent enough to parse with simple string matching?</li><li>Should the prompt include the task history (what steps we&#x27;ve already taken), or just the current goal?</li><li>What about including a system-level instruction about the platform and screen resolution?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a prompt template module with 3-4 templates: one for <strong>element location</strong> (&quot;find X on screen, return coordinates&quot;), one for <strong>action planning</strong> (&quot;given this screen and task, what&#x27;s the single next action?&quot;), one for <strong>screen description</strong> (&quot;describe what&#x27;s visible on this screen in structured format&quot;), and one for <strong>task completion check</strong> (&quot;is the task complete based on what you see?&quot;).</p>

<p>Each template should enforce a specific output format — like <code>ACTION: click | COORDS: 450, 320 | CONFIDENCE: high</code> — so your loop can parse it without fragile regex. The agent should also build in a system prompt that specifies the platform, resolution, and rules like &quot;always give exactly one action&quot; and &quot;say DONE when the task appears complete.&quot;</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The Thinking variant of GUI-Owl shines here. For complex tasks, it&#x27;ll reason through ambiguity — &quot;I see two &#x27;Submit&#x27; buttons, but based on the form context, the user probably means the one at the bottom of the payment section.&quot; If you&#x27;re hitting accuracy issues with Instruct, try swapping in the Thinking variant for the action-planning step.
      </div>
        
      <details class="reveal">
        <summary>MCP tool calling — the next frontier</summary>
        <div class="reveal-body"><p>GUI-Owl 1.5 was specifically benchmarked on <strong>MCP (Model Context Protocol) tool calling</strong>. This means instead of just clicking pixels on screen, it can invoke structured tools — &quot;call the <code>file_search</code> tool with query=&#x27;quarterly report&#x27;&quot;, or &quot;use the <code>browser_navigate</code> tool to go to gmail.com&quot;.</p>

<p>This is a hybrid approach: use MCP tools when they&#x27;re available (faster, more reliable), fall back to visual grounding when they&#x27;re not. It&#x27;s like the difference between using an API and using the website — the API is better when it exists, but the visual approach works for everything.</p>

<p>If you&#x27;re building a serious automation system, this hybrid approach is the way to go. Ask your agent to help you set up MCP tool definitions that GUI-Owl can invoke alongside its visual capabilities.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have a working perception-action loop with structured prompts. Test it on something simple — open Calculator, type a math problem, read the result. If the model is clicking in the wrong places, your prompt format is probably the issue. If it&#x27;s clicking in the right places but the wrong order, you might need to add task history to the prompt so the model knows what it&#x27;s already done.</div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Build a screenshot-to-accessibility-report tool: give GUI-Owl a screenshot of any app and have it generate a structured report of all interactive elements it can find — buttons, links, text fields, dropdowns — with their locations and labels.</div>
      <div class="your-turn-context">This flips the script — instead of using GUI-Owl to *do* things, you&#x27;re using its perception abilities to *analyze* interfaces. This is genuinely useful for accessibility auditing, UI testing, and building sitemaps of applications you don&#x27;t have source code for. Think of it as reverse-engineering a UI just from looking at it.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What categories of UI elements should you ask the model to identify? Think about interactive vs. display-only elements.</li><li>What output format would be most useful — JSON with element types, labels, and bounding boxes? A markdown table?</li><li>Should you process the whole screen at once, or split it into regions for more detail?</li><li>How would you validate that the model found everything? What about comparing its output to the actual accessibility tree on desktop?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Build me a Python script that takes a screenshot file path as input and uses GUI-Owl-7B to generate an accessibility report. The script should: (1) Load the screenshot, (2) Send it to GUI-Owl with a prompt asking it to identify ALL interactive UI elements — buttons, links, text inputs, dropdowns, checkboxes, toggles — and for each one return the element type, visible label/text, and bounding box coordinates as [x1, y1, x2, y2]. (3) Output the results as a JSON array. (4) Optionally save an annotated version of the screenshot with colored boxes around each element (different colors for different element types). Include error handling for cases where the model can&#x27;t identify any elements.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You need a GUI agent that can handle a 30-step task like &#x27;research flights to Tokyo and save the cheapest option to a spreadsheet.&#x27; Which model configuration should you choose?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt0">
          <label for="decision_10_opt0">GUI-Owl-7B Instruct for every step</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. The Instruct variant is fast but doesn&#x27;t do well with long-horizon planning. By step 15, it&#x27;s lost context on the overall goal and might start repeating actions or going down wrong paths. You need reasoning for complex tasks.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt1">
          <label for="decision_10_opt1">GUI-Owl-7B Thinking for every step</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Better reasoning, but the Thinking variant is significantly slower. Using it for every single step — including trivial ones like &#x27;click OK on this dialog&#x27; — wastes compute. You&#x27;d be waiting 10+ seconds per step for 30 steps.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt2">
          <label for="decision_10_opt2">Instruct for simple actions, Thinking for planning and ambiguous steps</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the hybrid approach Mobile-Agent-v3.5 uses. Use the fast Instruct model for clear, unambiguous actions (click a button, type text). Switch to the Thinking variant when the model needs to plan ahead, handle unexpected dialogs, or choose between multiple possible actions. You get speed where it&#x27;s safe and reasoning where it matters.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt3">
          <label for="decision_10_opt3">GUI-Owl-32B Instruct for everything</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Throwing a bigger model at it helps with accuracy but kills latency and requires serious hardware (64GB+ VRAM). The 7B hybrid approach will be faster and nearly as accurate. Scale up model size only when the 7B is genuinely failing on your use case, not as a default.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Going Multi-Agent: Adding Planning and Reflection</h2>
      </div>
      <div class="step-body">
        <p>Our single-model loop works, but it&#x27;s like driving with only your windshield — you can see what&#x27;s right in front of you, but you have no map and no rearview mirror.</p>

<p>The Mobile-Agent-v3.5 architecture adds two more &quot;perspectives&quot;:</p>

<p><strong>The Planner</strong> breaks your high-level goal into a checklist of sub-tasks. &quot;Send an email to Bob about the meeting&quot; becomes: (1) open email app, (2) click compose, (3) enter Bob&#x27;s address, (4) write subject, (5) write body, (6) click send. This gives the perception agent something focused to work on at each step.</p>

<p><strong>The Reflector</strong> compares the before and after screenshots to verify that the action actually worked. Did clicking that button actually open a menu? Did typing the text actually appear in the field? If not, it flags the failure and the planner can adjust.</p>

<p>This is honestly where the architecture gets elegant. Each agent is simple and focused, but together they handle failures and edge cases that would completely derail our naive loop.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to refactor your single-loop script into a three-agent architecture: a planner that breaks tasks into steps, a perceiver (GUI-Owl) that executes each step, and a reflector that verifies success after each action.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>The planner doesn&#x27;t need to be GUI-Owl — it could be any good language model. What information does it need to create a plan?</li><li>How does the reflector compare &#x27;before&#x27; and &#x27;after&#x27;? Does it need both screenshots, or just the &#x27;after&#x27; plus the expected outcome?</li><li>What should happen when the reflector says an action failed? Retry? Replan? Ask the user?</li><li>How do the three agents communicate? Think about a shared state object that tracks the current plan, completed steps, and failures.</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should restructure your code into three modules or classes:</p>

<ol>
<li><strong>Planner</strong> — takes the user&#x27;s task and generates a numbered list of sub-steps. It can use GUI-Owl (Thinking) or even a text-only LLM. It re-plans when the reflector reports failures.</li>
</ol>

<ol>
<li><strong>Perceiver</strong> — this is your existing loop step, but now it works on one sub-task at a time instead of the whole goal. Uses GUI-Owl Instruct for speed.</li>
</ol>

<ol>
<li><strong>Reflector</strong> — takes the before/after screenshots and the intended action, asks GUI-Owl &quot;did this action succeed?&quot; and returns a yes/no with explanation.</li>
</ol>

<p>The orchestrator ties them together: plan → for each step: perceive → act → reflect → if failed: replan. The agent should include a shared state dict that tracks <code>current_plan</code>, <code>completed_steps</code>, <code>current_step</code>, and <code>failures</code> so all three agents stay coordinated.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The planner doesn&#x27;t need to be a local model at all. You could use Claude or GPT-4 as the planner (they&#x27;re great at breaking tasks into steps) and only use GUI-Owl locally for the visual perception. This hybrid local+cloud approach gives you the best of both worlds — strong reasoning from the cloud, fast visual grounding locally.
      </div>
        
      <details class="reveal">
        <summary>The memory problem — and how Mobile-Agent solves it</summary>
        <div class="reveal-body"><p>After 20+ steps, your context window fills up with screenshots and action history. This is a real problem — the model starts forgetting what happened early on, and might revisit screens it&#x27;s already handled.</p>

<p>Mobile-Agent-v3.5 addresses this with a <strong>long-horizon memory module</strong>. Instead of stuffing every screenshot into the prompt, it maintains a compressed summary: &quot;We&#x27;ve completed the email composition. We&#x27;re now in the Calendar app trying to create an event.&quot;</p>

<p>The practical implementation is straightforward — after each reflection step, generate a one-sentence summary of what happened and add it to a running log. Feed this log (not the full screenshot history) to the planner when replanning. This keeps the context window manageable even for 50+ step tasks.</p>

<p>Ask your agent to add a <code>memory</code> list to your shared state that stores these summaries, and modify the planner to use them.</p></div>
      </details>
      </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>We started with a model that can look at a screenshot, and ended with a multi-agent system that can plan, perceive, act, and self-correct across any GUI. Let&#x27;s step back and think about what we actually built.</p>

<p><strong>GUI-Owl 1.5</strong> isn&#x27;t just another vision model — it&#x27;s a foundation model specifically designed for the perception layer of GUI automation. The fact that it ships in sizes from 2B to 235B means you can run a capable version on a laptop, which is unusual for models with this much specialized training.</p>

<p>The bigger insight is the <strong>architecture pattern</strong>: separating planning, perception, and reflection into distinct agents. This pattern shows up everywhere in AI systems right now, and GUI automation is one of the most tangible examples. Each agent is simple; the intelligence emerges from their coordination.</p>

<p>And honestly? We barely scratched the surface. We didn&#x27;t touch MCP tool calling, the cross-app memory system, or the browser-specific capabilities. But you now have the foundation to explore all of that.</p></div>
      <ul class="takeaways-list"><li>GUI-Owl 1.5 does GUI <em>grounding</em> — it returns pixel coordinates, not just descriptions. This is what makes it usable for actual automation, not just analysis.</li><li>The perception-action loop (screenshot → model → action → repeat) is the fundamental pattern for all GUI agents. Everything else — planning, reflection, memory — is optimization on top of this loop.</li><li>Instruct vs. Thinking variants serve different roles: use Instruct for fast, unambiguous actions and Thinking for planning and complex decisions. The hybrid approach gives you both speed and accuracy.</li><li>Multi-agent architectures (planner + perceiver + reflector) handle failures gracefully in ways that a single-model loop cannot. The reflection step is what makes the system robust.</li><li>You can run meaningful GUI automation with the 7B model on consumer hardware — you don&#x27;t need a 235B parameter model or cloud API for every use case.</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Try the <strong>Thinking variant</strong> of GUI-Owl for complex multi-app tasks and compare its reasoning quality to Instruct</li><li>Explore <strong>MCP tool calling</strong> — set up tool definitions that GUI-Owl can invoke alongside visual grounding for a hybrid automation approach</li><li>Build a <strong>browser-specific agent</strong> using Playwright for screenshots and actions — web automation is the most practical starting point for real use cases</li><li>Check out the full <strong>Mobile-Agent-v3.5 codebase</strong> on GitHub to see how Alibaba implements the multi-agent framework at production scale</li><li>Benchmark GUI-Owl against other vision models (GPT-4o, Gemini) on your specific UI — the SOTA results are on standard benchmarks, but your app might be different</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/X-PLUG/MobileAgent" target="_blank" rel="noopener">X-PLUG/MobileAgent — GUI-Owl 1.5 &amp; Mobile-Agent-v3.5</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis assistant that collects data, breaks filter bubbles, and predicts trends.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching how to build AI agents from scratch, covering theory and practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and various file formats into Markdown, with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Modular node-based GUI for building and running image-generation workflows with diffusion models.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Community-curated list of thousands of free programming books and learning resources in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues AI coding tools simplify writing code but raise the bar for real engineering judgment.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a trillion-parameter language model locally on a cluster of Ryzen AI laptops.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Claude hits #1 on the App Store as users rally behind Anthropic</div>
            <div class="oa-summary">Claude reaches top App Store spot as users support Anthropic amid a government contract dispute.</div>
            <div class="oa-meta">Hacker News AI · Mar 2</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}, {"title": "Claude hits #1 on the App Store as users rally behind Anthropic", "tags": "", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "image-gen";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>