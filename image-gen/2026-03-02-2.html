<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>GUI-Owl 1.5: Multi-Platform Vision Agent Models You Can Run Locally — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-image-gen">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Teach a Model to See Your Screen — Then Let It Drive</h1>
      <div class="hero-subtitle">We&#x27;re pulling Alibaba&#x27;s brand-new GUI-Owl 1.5 weights, running them locally, and building a vision agent that can actually watch a screen and take actions. No cloud APIs required.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">vision-models</span> <span class="tag">gui-automation</span> <span class="tag">qwen3-vl</span> <span class="tag">open-weights</span> <span class="tag">multi-platform</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <p>Two weeks ago (February 14, 2026), Alibaba&#x27;s Tongyi Lab dropped <strong>GUI-Owl 1.5</strong> — a family of open-weight vision models purpose-built for GUI automation. The lineup runs from a tiny 2B model you can squeeze onto a laptop all the way up to a 235B beast.</p>

<p>Why should you care? Because until now, getting a model to <em>look at a screen</em> and <em>do something useful</em> meant either paying per-call for GPT-4o or cobbling together fragile screenshot-plus-OCR pipelines. GUI-Owl 1.5 changes the game: these models natively understand UI elements across <strong>desktop, mobile, and browser</strong> — and they&#x27;re open-weight, sitting right there on HuggingFace.</p>

<p>They&#x27;re built on <strong>Qwen3-VL</strong> (Alibaba&#x27;s latest vision-language foundation model), and they come in two flavors: <strong>Instruct</strong> (fast, follows directions) and <strong>Thinking</strong> (slower, reasons step-by-step before acting). They hit state-of-the-art on over 20 GUI benchmarks, and they support MCP tool calling out of the box — meaning you can wire them into real automation pipelines.</p>

<p>On top of the raw models, the team also shipped <strong>Mobile-Agent-v3</strong>, a multi-agent framework that orchestrates planning, reflection, and memory on top of GUI-Owl. Think of it as the difference between giving someone eyes (the model) vs. giving them eyes <em>plus</em> a brain that can make multi-step plans.</p>

<p>Today we&#x27;ll go from zero to a working local vision agent. Let&#x27;s get into it.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>Pick Your Model Size and Get It Running</h2>
      </div>
      <div class="step-body">
        <p>First things first — we need to actually download and load a GUI-Owl model. The family has five sizes: <strong>2B, 4B, 8B, 32B, and 235B</strong>. For this workshop, the sweet spot is the <strong>8B Instruct</strong> variant. It&#x27;s small enough to run on a single GPU with 24GB VRAM (like an RTX 4090 or A5000), but capable enough to handle real GUI tasks.</p>

<p>Here&#x27;s a useful mental model: think of model sizes like camera resolutions. The 2B is your phone camera — quick and good enough for most shots. The 32B is a DSLR — noticeably better, but heavier to carry. The 235B is a medium-format film camera — incredible results, but you need a whole studio (read: a multi-GPU server) to use it.</p>

<p>The models live on HuggingFace under the <strong>Alibaba-NLP</strong> org. They&#x27;re standard Qwen3-VL architecture, which means the <code>transformers</code> library knows how to load them natively — no custom code needed.</p>

<p>Let&#x27;s get your agent to set up the environment and load the model.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to create a Python environment and write a script that downloads and loads GUI-Owl-1.5-8B-Instruct from HuggingFace, ready for inference.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What libraries do you need? (Think: HuggingFace transformers, plus any vision-specific dependencies for Qwen3-VL)</li><li>How do you typically load a vision-language model from HuggingFace? What&#x27;s the processor&#x27;s role vs. the model&#x27;s role?</li><li>The model is big — what dtype and device settings make sense for fitting it in GPU memory?</li><li>What&#x27;s the actual HuggingFace model ID going to look like for a model from Alibaba-NLP?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a setup script with two parts. First, the pip installs — <code>transformers</code>, <code>torch</code>, <code>accelerate</code>, and <code>qwen-vl-utils</code> (a small helper library Alibaba publishes for image preprocessing). Second, a short loading script that uses <code>AutoModelForVision2Seq</code> and <code>AutoProcessor</code> from transformers, pointed at something like <code>Alibaba-NLP/GUI-Owl-1.5-8B-Instruct</code>. The key detail is loading in <code>bfloat16</code> with <code>device_map=&quot;auto&quot;</code> so it fits in memory. The whole loading bit is maybe 5-6 lines — processor, model, done.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        If you only have 8GB VRAM (or no GPU at all), ask your agent about the <strong>2B</strong> or <strong>4B</strong> variants instead, or about running in 4-bit quantization with <code>bitsandbytes</code>. The 2B can even run on a MacBook with MPS.
      </div>
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        The 8B model is roughly 16GB on disk. Make sure you have enough storage before downloading. HuggingFace caches models in <code>~/.cache/huggingface/</code> by default.
      </div>
        
      <details class="reveal">
        <summary>Instruct vs. Thinking — what&#x27;s the difference?</summary>
        <div class="reveal-body"><p><strong>Instruct</strong> variants are trained to follow instructions directly — you say &quot;click the search bar&quot; and they output the coordinates. Fast and predictable.</p>

<p><strong>Thinking</strong> variants add a chain-of-thought step before acting. The model first reasons about what it sees (&quot;I see a search bar at the top of the page, a navigation menu on the left...&quot;) and <em>then</em> decides what to do. This is slower but significantly better for complex, multi-step tasks where the model needs to plan ahead.</p>

<p>For our workshop, Instruct is the right pick. Once you&#x27;re comfortable, try swapping in the Thinking variant and comparing outputs — you&#x27;ll literally see the model&#x27;s reasoning process in the output.</p></div>
      </details>
      <details class="reveal">
        <summary>What is Qwen3-VL and why does it matter?</summary>
        <div class="reveal-body"><p>Qwen3-VL is Alibaba&#x27;s latest vision-language foundation model. Think of it as the &quot;engine&quot; that GUI-Owl 1.5 is built on top of. Qwen3-VL handles the core ability to understand images and text together. GUI-Owl then fine-tunes that engine specifically for GUI understanding — recognizing buttons, text fields, menus, and other UI elements.</p>

<p>This matters because it means GUI-Owl inherits all the general visual understanding of Qwen3-VL (it can describe photos, read text in images, etc.) but <em>adds</em> specialized skills for GUI interaction. It&#x27;s like taking a car engine and tuning it specifically for off-road driving — the base is solid, the specialization makes it excel at its job.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Feed It a Screenshot and See What It Sees</h2>
      </div>
      <div class="step-body">
        <p>Now for the fun part — let&#x27;s give the model a screenshot and see if it can actually understand what&#x27;s on screen.</p>

<p>GUI-Owl&#x27;s core capability is <strong>grounding</strong> — it doesn&#x27;t just describe what&#x27;s in an image, it can point to specific UI elements with coordinates. Ask it &quot;where&#x27;s the submit button?&quot; and it gives you <code>(x, y)</code> pixel coordinates. This is the secret sauce that makes it useful for automation: the model doesn&#x27;t just <em>see</em>, it can <em>point</em>.</p>

<p>The way you talk to GUI-Owl is through a structured prompt format. You give it an image plus a text instruction, and it responds with either a description, coordinates, or an action plan. The prompt format follows Qwen3-VL&#x27;s chat template — your agent can handle the specifics.</p>

<p>Let&#x27;s take a screenshot of something simple (like a browser with a search engine open) and ask the model to identify elements on the page.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to write an inference function that takes a screenshot file path and a natural-language question, sends them to GUI-Owl, and returns the model&#x27;s response — including any grounding coordinates.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>How does a vision-language model receive images? (Think about the processor&#x27;s job — turning pixels into tokens.)</li><li>What kind of questions should we ask to test grounding? Something like &#x27;Where is the search box?&#x27; vs &#x27;What do you see?&#x27;</li><li>How does the model format its coordinate output? Does it use pixel values, percentages, or bounding boxes?</li><li>What chat template format does the Qwen3-VL family expect?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a function that loads an image (using PIL), builds a chat-format message with the image and your question, runs it through the processor and model, and decodes the output. The key insight is the message format: it&#x27;s a list of <code>{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [...]}</code> where content includes both an image block and a text block. When you ask a grounding question like &quot;Where is the search box?&quot;, the model returns coordinates in a format like <code>&lt;box&gt;(x1, y1, x2, y2)&lt;/box&gt;</code> — a bounding box around the element, typically in normalized coordinates (0-1000 scale). The whole function is maybe 10-12 lines, but the interesting part is how the message is structured.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Take a screenshot right now of whatever&#x27;s on your screen — that&#x27;s your test image. Real-world messiness (multiple windows, notifications, dark mode) is actually a great test of the model&#x27;s robustness.
      </div>
        
      <details class="reveal">
        <summary>How does GUI grounding actually work under the hood?</summary>
        <div class="reveal-body"><p>When you send GUI-Owl a screenshot, here&#x27;s what happens inside:</p>

<ol>
<li><strong>The image gets chopped into patches</strong> — the model divides your screenshot into a grid of small tiles (like puzzle pieces), each becoming a visual token.</li>
<li><strong>Visual tokens meet text tokens</strong> — your question (&quot;Where is the search box?&quot;) gets tokenized normally and concatenated with the visual tokens. The model processes them together.</li>
<li><strong>Attention does the heavy lifting</strong> — the transformer&#x27;s attention mechanism learns which visual patches correspond to which words. When it sees &quot;search box,&quot; it attends to the patches that contain the search input field.</li>
<li><strong>Coordinates come out as text</strong> — this is the clever bit. The model doesn&#x27;t have a special &quot;coordinate head.&quot; It literally generates the bounding box coordinates as text tokens: <code>&lt;box&gt;(234, 89, 567, 112)&lt;/box&gt;</code>. It learned to do this during fine-tuning on millions of GUI screenshots with labeled elements.</li>
</ol>

<p>This is why GUI-Owl can handle arbitrary UIs — it&#x27;s not looking for hardcoded patterns like &quot;a white rectangle with a magnifying glass icon.&quot; It genuinely understands the <em>concept</em> of a search box across different designs.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point, you should have GUI-Owl loaded locally and be able to send it a screenshot with a question like &quot;Where is the submit button?&quot; — getting back bounding box coordinates. If the model is loading but responses are garbled, double-check that you&#x27;re using the right chat template and that images are being processed through the processor (not just raw-loaded).</div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You want to build an automation pipeline that opens a browser, searches for something, and clicks the first result. Which approach makes more sense?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt0">
          <label for="decision_5_opt0">Use GUI-Owl in a loop: screenshot → ask model → parse coordinates → simulate click → repeat</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the core pattern for vision-based GUI automation. Each iteration is: capture the screen, ask the model what to do next (or where a specific element is), parse the coordinates from the response, simulate a mouse click at those coordinates, then screenshot again to see the result. It&#x27;s simple, robust, and works across any application — the model sees pixels, not DOM elements.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_5" id="decision_5_opt1">
          <label for="decision_5_opt1">Use GUI-Owl once to generate a complete script of all actions, then execute them all at once</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This sounds efficient but it&#x27;s brittle. GUIs are dynamic — a page might load slowly, a popup might appear, layout might shift. If you pre-generate all actions without checking the screen between steps, you&#x27;ll click on the wrong things. The screenshot-act-screenshot loop is essential because it gives the model fresh visual feedback after every action, just like a human glances at the screen between clicks.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Build the See-Think-Act Loop</h2>
      </div>
      <div class="step-body">
        <p>Now we&#x27;re getting to the really cool part. We&#x27;re going to build what I call the <strong>see-think-act loop</strong> — the core pattern behind every vision-based GUI agent:</p>

<ol>
<li><strong>See</strong>: Take a screenshot</li>
<li><strong>Think</strong>: Send it to GUI-Owl with the current goal</li>
<li><strong>Act</strong>: Parse the model&#x27;s response and simulate the action (click, type, scroll)</li>
<li><strong>Repeat</strong> until the task is done</li>
</ol>

<p>This is honestly how humans use computers too — you look at the screen, decide what to click, click it, then look again to see what happened. We&#x27;re just automating that cycle.</p>

<p>For the &quot;act&quot; part, we need a way to simulate mouse clicks and keyboard input. On macOS that&#x27;s <code>pyautogui</code>, on Linux it&#x27;s <code>pyautogui</code> or <code>xdotool</code>, on Windows it&#x27;s <code>pyautogui</code> again. The screenshot capture can use <code>Pillow</code> with <code>ImageGrab</code> or <code>mss</code> for speed.</p>

<p>The tricky design question is: how does the model know what it&#x27;s <em>supposed</em> to be doing? We need to give it context — the overall goal, plus what step we&#x27;re on. This is where a simple system prompt does a lot of heavy lifting.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to build a simple automation loop that takes a high-level task (like &#x27;open the calculator app and compute 42 × 17&#x27;), captures screenshots, sends them to GUI-Owl with the task context, parses the action from the response, and executes it using pyautogui.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What information does the model need in each iteration? Just the screenshot, or also the history of what it&#x27;s already done?</li><li>How do you parse the model&#x27;s output? It might say &#x27;click at (234, 567)&#x27; or &#x27;type hello&#x27; — what&#x27;s the output format?</li><li>When should the loop stop? How does the model signal &#x27;I&#x27;m done&#x27;?</li><li>What safety measures should you include? (Think: what if the model tries to click somewhere dangerous?)</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a class or set of functions implementing the loop. The key pieces are: (1) a <code>capture_screen()</code> function using <code>mss</code> or <code>PIL.ImageGrab</code>, (2) a <code>ask_model()</code> function that formats the screenshot + task + action history into a GUI-Owl prompt and gets back a structured response, (3) an <code>execute_action()</code> function that takes the model&#x27;s output (like <code>click(234, 567)</code> or <code>type(&quot;hello&quot;)</code> or <code>scroll(down)</code>) and calls the corresponding <code>pyautogui</code> function, and (4) a main loop that ties them together with a max-steps safety limit. The model&#x27;s output format typically looks like <code>&lt;action&gt;click&lt;/action&gt;&lt;coords&gt;(x, y)&lt;/coords&gt;</code> or similar structured tags. A critical detail: add a confirmation step or a <code>pyautogui.FAILSAFE = True</code> so moving your mouse to the corner kills the automation if something goes wrong.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        <strong>Safety first!</strong> Any script that controls your mouse and keyboard is potentially dangerous. Always include: (1) a max iteration limit, (2) <code>pyautogui.FAILSAFE = True</code> so you can abort by slamming your mouse into a screen corner, and (3) a short <code>pyautogui.PAUSE</code> delay between actions so you can see what&#x27;s happening.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Start with a dead-simple task — like opening Calculator and typing <code>2+2</code>. Once that works reliably, scale up to multi-step tasks. Debugging a 15-step browser workflow when you can&#x27;t even get a single click right is no fun.
      </div>
        
      <details class="reveal">
        <summary>How is this different from Selenium or Playwright?</summary>
        <div class="reveal-body"><p>Great question. Traditional automation tools like Selenium and Playwright work by manipulating the <strong>DOM</strong> — the underlying HTML structure of a web page. They find elements by CSS selectors, XPaths, or IDs.</p>

<p>GUI-Owl-based automation works by looking at <strong>pixels</strong> — the actual visual appearance of the screen. This has huge trade-offs:</p>

<p><strong>Pixel-based wins:</strong></p>
<ul>
<li>Works on <em>any</em> application (desktop apps, mobile, games, remote desktops) — not just browsers</li>
<li>Never breaks because a developer changed a CSS class name</li>
<li>Handles dynamic UIs, Canvas-based apps, and anything that renders to screen</li>
</ul>

<p><strong>DOM-based wins:</strong></p>
<ul>
<li>Way faster (no model inference per step)</li>
<li>More precise (exact element references, not approximate coordinates)</li>
<li>Can interact with hidden elements or read element properties</li>
</ul>

<p>The trend in 2026 is converging the two: models like GUI-Owl can also consume accessibility trees and DOM info alongside screenshots, getting the best of both worlds. But the pure vision approach is what makes GUI-Owl special — it works everywhere.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>Level Up: Mobile-Agent-v3&#x27;s Multi-Agent Brain</h2>
      </div>
      <div class="step-body">
        <p>Our see-think-act loop works, but honestly, it&#x27;s kind of dumb. It has no memory of what happened five steps ago, no ability to plan ahead, and no way to recover when something goes wrong. It&#x27;s like giving someone amazing eyesight but no short-term memory.</p>

<p>This is exactly what <strong>Mobile-Agent-v3</strong> solves. It&#x27;s a multi-agent framework that wraps around GUI-Owl and adds three critical capabilities:</p>

<ul>
<li><strong>Planning Agent</strong>: Before doing anything, it breaks the high-level task into a step-by-step plan. &quot;To book a flight, I need to: 1) open the browser, 2) go to the airline site, 3) enter departure city...&quot; etc.</li>
<li><strong>Reflection Agent</strong>: After each action, it checks whether the result matches what was expected. If the page didn&#x27;t load right or a popup appeared, it can adapt the plan.</li>
<li><strong>Memory Agent</strong>: It maintains a running summary of what&#x27;s been accomplished and what&#x27;s left. This prevents the loop from getting stuck repeating the same action or forgetting what it already did.</li>
</ul>

<p>Think of it like this: GUI-Owl is the <strong>eyes and hands</strong>. Mobile-Agent-v3 is the <strong>prefrontal cortex</strong> — the part that plans, remembers, and course-corrects.</p>

<p>The framework&#x27;s code is open-source in the same GitHub repo. Let&#x27;s get your agent to set it up.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to explain the Mobile-Agent-v3 architecture and help you set up a simple multi-agent pipeline using its planning and reflection capabilities on top of your local GUI-Owl model.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>Mobile-Agent-v3 is in the X-PLUG/MobileAgent GitHub repo. What does the project structure look like?</li><li>How do the three agents (planning, acting, reflecting) communicate? Is it a pipeline, or do they run in parallel?</li><li>Can you use your local GUI-Owl model as the backbone, or does it expect an API?</li><li>What config or setup does the framework need — model paths, screen capture settings, action space definitions?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should walk you through cloning the MobileAgent repo and explain its structure: there&#x27;s a main orchestrator that coordinates three sub-agents. The <strong>planning agent</strong> takes the user&#x27;s task and current screenshot, producing a step-by-step plan. The <strong>action agent</strong> (powered by GUI-Owl) executes one step at a time. The <strong>reflection agent</strong> compares the expected outcome of each step with the actual screenshot result and flags mismatches. Configuration typically involves pointing to your local model path, setting the screen capture method, and defining the action space (click, type, scroll, etc.). The key insight is that it&#x27;s essentially our simple loop from Step 3, but with two extra model calls per iteration — one to update the plan, one to verify the result.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        You don&#x27;t <em>have</em> to use all three agents. For simple tasks, you can just use the planning + action agents and skip reflection. Each extra agent adds latency (another model forward pass per step), so there&#x27;s a real speed/reliability trade-off.
      </div>
        
      <details class="reveal">
        <summary>MCP Tool Calling — what&#x27;s that about?</summary>
        <div class="reveal-body"><p>GUI-Owl 1.5 supports <strong>Model Context Protocol (MCP)</strong> tool calling. MCP is a standard for letting models invoke external tools — think of it as function calling, but with a standardized protocol that works across different model providers.</p>

<p>In the GUI automation context, this means GUI-Owl doesn&#x27;t <em>have</em> to simulate mouse clicks for everything. If you wire up MCP tools for things like &quot;open URL in browser,&quot; &quot;read clipboard contents,&quot; or &quot;launch application,&quot; the model can call those tools directly instead of laboriously navigating the GUI to accomplish the same thing.</p>

<p>Alibaba even released <strong>OSWorld-MCP</strong>, a benchmark specifically for evaluating how well models use MCP tools in real GUI scenarios. This is the direction things are heading — models that can seamlessly mix pixel-level GUI interaction with direct API/tool calls, choosing whichever is more efficient for each sub-task.</p>

<p>To set this up, you&#x27;d define your MCP tools as a JSON schema and include the tool descriptions in the system prompt. The model outputs tool calls in its response, which your orchestrator parses and executes.</p></div>
      </details>
      <details class="reveal">
        <summary>How much VRAM do I need for the multi-agent setup?</summary>
        <div class="reveal-body"><p>Here&#x27;s the thing that might not be obvious: you don&#x27;t need three separate models for three agents. You use the <strong>same GUI-Owl model</strong> for all three roles — you just change the system prompt.</p>

<p>The planning agent gets a prompt like: &quot;Given this task and screenshot, create a step-by-step plan.&quot; The action agent gets: &quot;Given this plan step and screenshot, what action should I take?&quot; The reflection agent gets: &quot;Given the expected outcome and this screenshot, did the action succeed?&quot;</p>

<p>So your VRAM requirement is the same as running one model — you&#x27;re just doing more inference calls per loop iteration. With the 8B model, you&#x27;re looking at ~16GB VRAM. The 32B is ~64GB (doable on an A100 or two consumer GPUs with tensor parallelism).</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>You should now have the full picture: GUI-Owl handles vision and grounding, your see-think-act loop handles the automation cycle, and Mobile-Agent-v3 adds planning and reflection on top. If you&#x27;ve been running code along the way, you should be able to give a natural-language task and watch the agent attempt it on your screen. If you&#x27;ve been reading along, you understand the architecture well enough to build it when you&#x27;re ready.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Make It Practical: Prompt Engineering for GUI Agents</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s something that honestly confused me at first: even though GUI-Owl is fine-tuned for GUI tasks, the <em>way you prompt it</em> matters enormously. A vague instruction like &quot;do my email&quot; will get you nowhere. A specific instruction like &quot;open Gmail in Chrome and compose a new email to alice@example.com with subject &#x27;Meeting Tomorrow&#x27; and body &#x27;Can we move our 2pm to 3pm?&#x27;&quot; will work surprisingly well.</p>

<p>There are a few prompt patterns that make a big difference:</p>

<p><strong>Be specific about the target application and platform.</strong> &quot;Click the search bar&quot; is ambiguous if there are three search bars on screen. &quot;Click the Google search bar in the center of the Chrome browser window&quot; is unambiguous.</p>

<p><strong>Describe visual landmarks when helpful.</strong> If the UI is complex, help the model orient: &quot;In the left sidebar, under the &#x27;Settings&#x27; heading, click &#x27;Account&#x27;.&quot;</p>

<p><strong>State the success condition.</strong> &quot;Keep going until you see the confirmation page that says &#x27;Order placed&#x27;.&quot; This gives the reflection agent something to check against.</p>

<p>Let&#x27;s get your agent to help you craft an effective prompt template for GUI automation tasks.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get the agent to help you design a reusable system prompt template for GUI-Owl that includes the task description, platform context, action format specification, and success criteria — something you can fill in for any automation task.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What context does the model need upfront? (Operating system, screen resolution, which app is open...)</li><li>How should you format the action output so it&#x27;s easy to parse programmatically?</li><li>What should the model do when it encounters something unexpected — like a popup or an error message?</li><li>How do you tell the model to signal that the task is complete?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a system prompt template with clear sections: a <strong>role definition</strong> (&quot;You are a GUI automation agent operating on [platform]&quot;), a <strong>task description</strong> slot, an <strong>action format specification</strong> (defining the exact output format — like <code>&lt;action&gt;click&lt;/action&gt;&lt;position&gt;(x,y)&lt;/position&gt;</code> or <code>&lt;action&gt;type&lt;/action&gt;&lt;text&gt;hello&lt;/text&gt;</code>), <strong>special instructions</strong> for handling errors/popups, and a <strong>completion signal</strong> (like outputting <code>&lt;action&gt;done&lt;/action&gt;</code> when the task is finished). The template should be platform-aware and include the screen resolution so coordinate outputs are meaningful. It&#x27;s basically a fill-in-the-blanks document, not code.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Test your prompts on static screenshots before running them in a live loop. Take 4-5 screenshots from different stages of a task and run them through the model one at a time. This lets you debug prompt issues without waiting for the full automation cycle.
      </div>
        
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re building a GUI agent to automate filling out a multi-page form. The form has 4 pages. Which strategy will be most reliable?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt0">
          <label for="decision_10_opt0">Give the model the entire form task at once and let it handle page navigation naturally</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. While GUI-Owl with the Thinking variant can handle this, it&#x27;s risky. The model might lose track of which page it&#x27;s on, forget which fields it already filled, or get confused if a page takes longer to load. Multi-page tasks are where the see-think-act loop is most likely to go off the rails without explicit state tracking.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt1">
          <label for="decision_10_opt1">Break it into 4 sub-tasks (one per page), with explicit checkpoints between pages</label>
          <div class="decision-feedback correct">&#10003; Correct! This is the Mobile-Agent-v3 philosophy in action. By breaking the task into sub-goals with clear success conditions (&quot;Page 2 should show the address form&quot;), you give the reflection agent something concrete to verify at each stage. If something goes wrong on page 2, you can retry just that page instead of restarting the entire form. This is also easier to debug — you can see exactly where things went sideways.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_10" id="decision_10_opt2">
          <label for="decision_10_opt2">Use DOM-based automation (Selenium/Playwright) instead of GUI-Owl for forms</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This is a reasonable instinct — forms are very DOM-friendly. But the question asked about the best strategy *using a GUI agent*. In practice, you might actually use a hybrid: GUI-Owl for navigation and visual verification, DOM tools for precise form filling. But if you&#x27;re committed to the vision approach, sub-task decomposition is the way to go.</div>
        </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Build a &#x27;Screen Narrator&#x27; — an accessibility tool that takes a screenshot every 5 seconds and generates a plain-English description of what&#x27;s happening on screen, highlighting any changes since the last screenshot.</div>
      <div class="your-turn-context">This is a practical application of GUI-Owl&#x27;s visual understanding that doesn&#x27;t require mouse/keyboard automation — making it safer to experiment with. It&#x27;s also genuinely useful: imagine a tool that narrates screen activity for visually impaired users, or creates an automatic log of what you did during a work session.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What prompt would make the model describe the screen in accessible, human-friendly language instead of outputting coordinates?</li><li>How would you detect and describe *changes* between consecutive screenshots? Does the model need to see both images, or can you diff them first?</li><li>What output format makes sense — raw text, structured JSON with regions, or something else?</li><li>How would you handle the 5-second loop? Think about threading, async, or just a simple while loop with a sleep.</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Build me a Python script called screen_narrator.py that: (1) Takes a screenshot every 5 seconds using mss. (2) Sends each screenshot to my local GUI-Owl-1.5-8B-Instruct model with the prompt &#x27;Describe what is currently shown on this screen in 2-3 plain English sentences, focusing on the active application and any notable UI elements. If I also provide a previous screenshot, briefly note what changed.&#x27; (3) Keeps track of the previous screenshot and sends both current and previous to the model after the first iteration. (4) Prints each description to the terminal with a timestamp. (5) Runs until I hit Ctrl+C. Keep it simple — no GUI, just terminal output. Use the same model loading code we already have.</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><p>We just went from &quot;what even is GUI-Owl&quot; to having a locally-running vision agent that can see your screen and take actions. Let&#x27;s recap the architecture:</p>

<p><strong>GUI-Owl 1.5</strong> is the eyes — a fine-tuned vision-language model that understands UI elements across desktop, mobile, and browser. It can describe what it sees, point to specific elements with bounding boxes, and output structured actions.</p>

<p><strong>The see-think-act loop</strong> is the skeleton — screenshot, ask the model, execute the action, repeat. Simple but powerful.</p>

<p><strong>Mobile-Agent-v3</strong> is the brain — adding planning (break the task into steps), reflection (did that step work?), and memory (what have we done so far?) on top of the raw vision model.</p>

<p>The biggest takeaway? <strong>Open-weight vision models have crossed the usefulness threshold for GUI automation.</strong> You no longer need cloud APIs or proprietary tools to build an agent that watches your screen and does useful work. A 24GB GPU and some good prompting gets you surprisingly far.</p>

<p>And this is just the beginning — MCP tool integration, hybrid vision+DOM approaches, and RL-tuned variants (like the OSWorld checkpoint) are all active frontiers. This stuff is moving fast.</p></div>
      <ul class="takeaways-list"><li>GUI-Owl 1.5 is a family of open-weight vision models (2B-235B) that natively understand UI elements and can output grounding coordinates — no OCR pipeline needed.</li><li>The core automation pattern is a see-think-act loop: screenshot → model inference → action execution → repeat. It works across any application because it operates on pixels, not DOM elements.</li><li>Mobile-Agent-v3 adds a planning/reflection/memory layer on top, turning a simple reactive loop into a genuine agent that can handle multi-step tasks and recover from errors.</li><li>Prompt engineering matters as much for GUI agents as it does for chatbots — specific tasks, clear action formats, and explicit success conditions make the difference between a demo and something useful.</li><li>The 8B Instruct model on a single consumer GPU is the sweet spot for experimentation. Scale up to 32B when you need more reliability on complex tasks.</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Try the Thinking variant of GUI-Owl and compare its step-by-step reasoning with the Instruct variant&#x27;s direct outputs — it&#x27;s fascinating to see what the model &#x27;thinks about&#x27; before acting.</li><li>Wire up MCP tools (file operations, browser bookmarks, clipboard access) and see how the model learns to mix GUI actions with direct tool calls.</li><li>Explore the OSWorld and AndroidWorld benchmarks in the MobileAgent repo — they&#x27;re full evaluation environments that let you systematically test your agent on real tasks.</li><li>Try running two GUI-Owl instances: one as the actor and one as a critic that evaluates whether each action was correct. This self-play pattern can dramatically improve reliability.</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/X-PLUG/MobileAgent" target="_blank" rel="noopener">X-PLUG/MobileAgent — GUI-Owl 1.5 &amp; Mobile-Agent-v3</a> <span class="source-name">(GitHub)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis assistant that collects data, breaks filter bubbles, and predicts trends.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching how to build AI agents from scratch, covering theory and practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and various file formats into Markdown, with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Modular node-based GUI for building and running diffusion model image generation workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Community-curated list of free programming books and learning resources in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">If AI writes code, should the session be part of the commit?</div>
            <div class="oa-summary">Tool and discussion about recording AI coding sessions as metadata alongside git commits.</div>
            <div class="oa-meta">Hacker News AI · Mar 2</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues AI handles syntax but makes higher-level engineering judgment and system design more critical.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a trillion-parameter language model locally using Ryzen AI Max+ hardware.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "If AI writes code, should the session be part of the commit?", "tags": "coding", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "image-gen";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>