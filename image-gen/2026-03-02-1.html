<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Build Real-Time Vision AI Agents with Stream&#x27;s Open Vision Agents — Tinker</title>
  <style>
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;0,9..144,700;0,9..144,800;1,9..144,400&family=IBM+Plex+Mono:wght@400;500;600&display=swap');
:root {
  --bg: #F7F4F0;
  --bg-subtle: #EFEBE5;
  --bg-elevated: #FFFDF9;
  --ink: #1C1917;
  --ink-secondary: #57534E;
  --muted: #A8A29E;
  --accent: #2B6B6B;
  --accent-hover: #1F5252;
  --accent-light: rgba(43,107,107,0.06);
  --accent-subtle: rgba(43,107,107,0.14);
  --blue: #3B82F6;
  --red: #EF4444;
  --yellow: #EAB308;
  --green: #22C55E;
  --surface: #EFEBE5;
  --border: #DDD7CF;
  --border-subtle: #EFEBE5;
  --shadow-sm: 0 1px 2px rgba(0,0,0,0.04);
  --shadow-md: 0 2px 8px rgba(0,0,0,0.06), 0 0 0 1px rgba(0,0,0,0.03);
  --shadow-lg: 0 4px 16px rgba(0,0,0,0.08), 0 0 0 1px rgba(0,0,0,0.02);
  --mono: 'IBM Plex Mono', monospace;
  --display: 'Fraunces', Georgia, serif;
  --sans: 'Inter', -apple-system, system-ui, sans-serif;
  --max-w: 680px;
  --max-w-wide: 780px;
  --radius-sm: 4px;
  --radius-md: 8px;
  --radius-lg: 12px;
}
*, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
body {
  font-family: var(--sans);
  background: var(--bg);
  color: var(--ink);
  min-height: 100vh;
  -webkit-font-smoothing: antialiased;
  line-height: 1.7;
  font-size: 17px;
  border-top: 3px solid var(--accent);
}

/* ── Track color worlds ── */
body.track-general { /* default teal — uses :root values */ }
body.track-image-gen { --accent: #C4563A; --accent-hover: #A8452E; --accent-light: rgba(196,86,58,0.06); --accent-subtle: rgba(196,86,58,0.14); }
body.track-audio { --accent: #6B5B95; --accent-hover: #574A7D; --accent-light: rgba(107,91,149,0.06); --accent-subtle: rgba(107,91,149,0.14); }

/* ── Layout ── */
.session-container {
  max-width: var(--max-w);
  margin: 0 auto;
  padding: 0 1.5rem 5rem;
}

/* ── Back link ── */
.back-link {
  display: inline-flex;
  align-items: center;
  gap: 0.35rem;
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 500;
  color: var(--muted);
  text-decoration: none;
  padding: 2rem 0 1.25rem;
  transition: color 0.15s;
}
.back-link:hover { color: var(--accent); }

/* ── Hero ── */
.session-hero {
  padding: 1rem 0 2.5rem;
  margin-bottom: 2rem;
  border-bottom: 1px solid var(--border);
}
.session-hero .hero-tag {
  display: inline-block;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  color: var(--accent);
  background: var(--accent-light);
  padding: 0.3rem 0.75rem;
  margin-bottom: 1.25rem;
  border-radius: var(--radius-sm);
}
.session-hero h1 {
  font-family: var(--display);
  font-size: 2.5rem;
  font-weight: 800;
  line-height: 1.15;
  letter-spacing: -0.025em;
  margin-bottom: 0.6rem;
  font-optical-sizing: auto;
}
.session-hero .hero-subtitle {
  font-size: 1.1rem;
  color: var(--ink-secondary);
  font-weight: 400;
  line-height: 1.5;
}
.session-hero .hero-meta {
  display: flex;
  gap: 1.25rem;
  margin-top: 1.25rem;
  font-family: var(--mono);
  font-size: 0.7rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.hero-meta .tag {
  display: inline-block;
  background: var(--surface);
  padding: 0.2rem 0.55rem;
  font-size: 0.65rem;
  border-radius: var(--radius-sm);
  border: 1px solid var(--border);
}

/* ── Section divider ── */
.section-divider {
  border: none;
  width: 32px;
  height: 2px;
  background: var(--accent);
  margin: 3rem 0;
}

/* ── Context block ── */
.context-block {
  background: var(--bg-elevated);
  padding: 1.5rem 1.75rem;
  margin-bottom: 2.5rem;
  border-radius: var(--radius-md);
  border: 1px solid var(--border);
}
.context-block h2 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.75rem;
}
.context-block p { margin-bottom: 0.75rem; }
.context-block p:last-child { margin-bottom: 0; }

/* ── Steps ── */
.step-section { margin-bottom: 3rem; }
.step-header {
  display: flex;
  align-items: flex-start;
  gap: 1rem;
  margin-bottom: 1.25rem;
}
.step-number {
  flex-shrink: 0;
  width: 44px; height: 44px;
  background: var(--ink);
  color: #fff;
  font-family: var(--mono);
  font-size: 0.9rem;
  font-weight: 600;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: var(--radius-sm);
}
.step-header h2 {
  font-family: var(--display);
  font-size: 1.3rem;
  font-weight: 700;
  line-height: 1.25;
  padding-top: 0.35rem;
}
.step-body p { margin-bottom: 0.75rem; }
.step-body ul, .step-body ol { margin: 0.5rem 0 0.75rem 1.5rem; }
.step-body li { margin-bottom: 0.35rem; }
.step-body strong { font-weight: 600; }
.step-body a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-subtle); text-underline-offset: 2px; }
.step-body a:hover { text-decoration-color: var(--accent); }

/* ── Code blocks ── */
.code-block {
  position: relative;
  margin: 1.25rem 0;
  background: var(--ink);
  border-radius: var(--radius-md);
  overflow: hidden;
}
.code-caption {
  display: block;
  padding: 0.55rem 1rem;
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 500;
  color: rgba(255,255,255,0.4);
  border-bottom: 1px solid rgba(255,255,255,0.08);
  letter-spacing: 0.03em;
}
.code-block pre {
  padding: 1rem;
  overflow-x: auto;
  margin: 0;
  background: transparent;
}
.code-block code {
  font-family: var(--mono);
  font-size: 0.82rem;
  line-height: 1.6;
  color: #E4E4E7;
}
.copy-btn {
  position: absolute;
  top: 0.45rem;
  right: 0.5rem;
  font-family: var(--mono);
  font-size: 0.55rem;
  font-weight: 600;
  letter-spacing: 0.04em;
  text-transform: uppercase;
  background: rgba(255,255,255,0.1);
  color: rgba(255,255,255,0.5);
  border: none;
  padding: 0.25rem 0.55rem;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-sm);
}
.copy-btn:hover { background: rgba(255,255,255,0.2); color: #fff; }
.copy-btn.copied { background: var(--green); color: #fff; }

/* ── Callouts ── */
.callout {
  padding: 1rem 1.25rem;
  margin: 1.25rem 0;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
  border: 1px solid;
  background: var(--bg-elevated);
}
.callout-tip {
  border-color: rgba(43,107,107,0.2);
  background: rgba(43,107,107,0.04);
}
.callout-warning {
  border-color: rgba(196,86,58,0.3);
  background: rgba(196,86,58,0.04);
}
.callout-api-key-note {
  border-color: rgba(202,138,4,0.25);
  background: rgba(202,138,4,0.04);
}
.callout-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  margin-bottom: 0.35rem;
}
.callout-tip .callout-label { color: var(--accent); }
.callout-warning .callout-label { color: #B84533; }
.callout-api-key-note .callout-label { color: #CA8A04; }

/* ── Reveals (details/summary) ── */
.reveal {
  margin: 1rem 0;
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
  background: var(--bg-elevated);
}
.reveal summary {
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  padding: 0.75rem 1rem;
  cursor: pointer;
  background: var(--bg-elevated);
  list-style: none;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: background 0.15s;
}
.reveal summary:hover { background: var(--surface); }
.reveal summary::before {
  content: "+";
  font-family: var(--mono);
  font-size: 0.85rem;
  font-weight: 600;
  color: var(--accent);
}
.reveal[open] summary::before {
  content: "\2212";
}
.reveal .reveal-body {
  padding: 1rem;
  border-top: 1px solid var(--border);
  font-size: 0.92rem;
}
.reveal .reveal-body p { margin-bottom: 0.5rem; }
.reveal .reveal-body p:last-child { margin-bottom: 0; }

/* ── Checkpoint ── */
.checkpoint {
  display: flex;
  align-items: center;
  gap: 0.85rem;
  padding: 0.85rem 1.25rem;
  background: var(--accent-light);
  color: var(--ink);
  margin: 2rem 0;
  font-family: var(--sans);
  font-size: 0.85rem;
  font-weight: 600;
  border-radius: var(--radius-md);
  border: 1px solid var(--accent-subtle);
}
.checkpoint-icon {
  flex-shrink: 0;
  width: 26px; height: 26px;
  background: var(--accent);
  border-radius: var(--radius-sm);
  display: flex;
  align-items: center;
  justify-content: center;
  font-size: 0.75rem;
  color: #fff;
}

/* ── Decision point ── */
.decision-point {
  margin: 2rem 0;
  padding: 1.5rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
}
.decision-point h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.decision-point .question {
  font-family: var(--display);
  font-size: 1.1rem;
  font-weight: 600;
  margin-bottom: 1rem;
  line-height: 1.35;
}
.decision-option {
  margin-bottom: 0.5rem;
}
.decision-option input[type="radio"] {
  display: none;
}
.decision-option label {
  display: block;
  padding: 0.7rem 1rem;
  background: var(--surface);
  border: 1.5px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  font-weight: 500;
  font-size: 0.92rem;
  border-radius: var(--radius-md);
}
.decision-option label:hover {
  background: var(--accent-light);
  border-color: var(--accent);
}
.decision-option input:checked + label {
  border-color: var(--accent);
  background: var(--accent-light);
}
.decision-feedback {
  display: none;
  padding: 0.65rem 0.85rem;
  margin-top: 0.35rem;
  font-size: 0.85rem;
  border-left: 3px solid;
  border-radius: var(--radius-sm);
}
.decision-option input:checked ~ .decision-feedback {
  display: block;
}
.decision-feedback.correct {
  border-color: #3D9A6D;
  background: rgba(61,154,109,0.06);
  color: #2D7A54;
}
.decision-feedback.incorrect {
  border-color: #C4563A;
  background: rgba(196,86,58,0.05);
  color: #A8452E;
}

/* ── Agent interaction ── */
.agent-interaction {
  margin: 1.5rem calc((var(--max-w) - var(--max-w-wide)) / 2);
  border-radius: var(--radius-md);
  overflow: hidden;
  border: 1px solid var(--border);
}
.agent-goal {
  padding: 1rem 1.25rem;
  background: var(--ink);
  color: #E4E4E7;
  font-family: var(--mono);
  font-size: 0.82rem;
  font-weight: 500;
  line-height: 1.5;
}
.agent-goal::before {
  content: none;
}
.agent-goal-label {
  font-size: 0.55rem;
  letter-spacing: 0.1em;
  text-transform: uppercase;
  margin-bottom: 0.4rem;
  display: flex;
  align-items: center;
  gap: 0.4rem;
  color: rgba(255,255,255,0.35);
}
.agent-goal-label::before {
  content: "";
  display: inline-block;
  width: 6px; height: 6px;
  background: var(--green);
  border-radius: 50%;
}
.agent-hints {
  padding: 1rem 1.25rem;
  background: var(--surface);
  border-bottom: 1px solid var(--border);
}
.agent-hints-label {
  font-family: var(--mono);
  font-size: 0.6rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.agent-hints ul {
  list-style: none;
  padding: 0;
}
.agent-hints li {
  padding: 0.3rem 0 0.3rem 1.25rem;
  position: relative;
  font-size: 0.9rem;
  font-style: italic;
  color: var(--ink-secondary);
}
.agent-hints li::before {
  content: "\203A";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
  font-style: normal;
  font-family: var(--mono);
}

/* Agent interaction reveals */
.agent-interaction .reveal {
  border-radius: 0;
  border: none;
  border-top: 1px solid var(--border);
}
.agent-interaction .reveal summary {
  font-size: 0.8rem;
  background: var(--surface);
}

/* ── Your turn ── */
.your-turn {
  padding: 1.5rem;
  margin: 2rem 0;
  border-radius: var(--radius-md);
  background: var(--accent-light);
  border: 1.5px solid var(--accent-subtle);
}
.your-turn h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--accent);
  margin-bottom: 0.5rem;
}
.your-turn .your-turn-goal {
  font-family: var(--sans);
  font-size: 1.05rem;
  font-weight: 600;
  margin-bottom: 0.75rem;
  line-height: 1.4;
}
.your-turn .your-turn-context {
  font-size: 0.92rem;
  color: var(--ink-secondary);
  margin-bottom: 1rem;
}

/* ── Recap ── */
.recap-section {
  padding-top: 2.5rem;
  margin-top: 3rem;
  border-top: 1px solid var(--border);
}
.recap-section h2 {
  font-family: var(--display);
  font-size: 1.35rem;
  font-weight: 700;
  margin-bottom: 1rem;
}
.recap-body { margin-bottom: 1.5rem; }
.recap-body p { margin-bottom: 0.75rem; }
.takeaways-list {
  list-style: none;
  padding: 0;
  margin-bottom: 1.5rem;
}
.takeaways-list li {
  padding: 0.55rem 0 0.55rem 1.75rem;
  position: relative;
  font-size: 0.95rem;
}
.takeaways-list li::before {
  content: "\2713";
  position: absolute;
  left: 0;
  color: var(--green);
  font-weight: 700;
  font-size: 0.85rem;
}
.next-steps h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.5rem;
}
.next-steps ul {
  list-style: none;
  padding: 0;
}
.next-steps li {
  padding: 0.3rem 0 0.3rem 1.5rem;
  position: relative;
}
.next-steps li::before {
  content: "\2192";
  position: absolute;
  left: 0;
  color: var(--accent);
  font-weight: 700;
}

/* ── Sources ── */
.sources-section {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
  border-top: 1px solid var(--border);
}
.sources-section h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.75rem;
}
.sources-list {
  list-style: none;
  padding: 0;
}
.sources-list li {
  padding: 0.3rem 0;
}
.sources-list a {
  color: var(--accent);
  text-decoration: underline;
  text-decoration-color: var(--accent-subtle);
  text-underline-offset: 2px;
  font-size: 0.9rem;
}
.sources-list a:hover { text-decoration-color: var(--accent); }
.sources-list .source-name {
  font-family: var(--mono);
  font-size: 0.65rem;
  color: var(--muted);
  margin-left: 0.35rem;
}

/* ── Other articles ── */
.other-articles {
  margin-top: 2.5rem;
  padding-top: 1.5rem;
}
.other-articles h3 {
  font-family: var(--mono);
  font-size: 0.65rem;
  font-weight: 600;
  letter-spacing: 0.08em;
  text-transform: uppercase;
  color: var(--muted);
  margin-bottom: 0.25rem;
}
.other-articles .oa-intro {
  font-size: 0.82rem;
  color: var(--muted);
  margin-bottom: 1rem;
}
.other-article-card {
  padding: 0.85rem 1rem;
  margin-bottom: 0.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  border-radius: var(--radius-md);
  background: var(--bg-elevated);
  border: 1px solid var(--border);
  transition: all 0.15s;
}
.other-article-card:hover {
  border-color: var(--accent-subtle);
  background: var(--accent-light);
}
.oa-info {
  flex: 1;
  min-width: 0;
}
.oa-title {
  font-weight: 600;
  font-size: 0.9rem;
  margin-bottom: 0.1rem;
}
.oa-summary {
  font-size: 0.82rem;
  color: var(--ink-secondary);
  margin: 0.1rem 0;
  line-height: 1.4;
}
.oa-meta {
  font-family: var(--mono);
  font-size: 0.6rem;
  color: var(--muted);
  letter-spacing: 0.02em;
}
.oa-votes {
  display: flex;
  gap: 0.25rem;
  flex-shrink: 0;
}
.oa-toggle {
  display: none;
}
.oa-toggle-label {
  display: inline-flex;
  align-items: center;
  justify-content: center;
  width: 34px; height: 34px;
  font-size: 0.9rem;
  border: 1px solid var(--border);
  cursor: pointer;
  transition: all 0.15s;
  background: var(--bg-elevated);
  user-select: none;
  border-radius: var(--radius-sm);
}
.oa-toggle-label:hover {
  background: var(--surface);
  border-color: var(--muted);
}
.oa-toggle:checked + .oa-toggle-label.vote-up {
  background: rgba(61,154,109,0.1);
  border-color: #3D9A6D;
  color: #3D9A6D;
}
.oa-toggle:checked + .oa-toggle-label.vote-down {
  background: rgba(196,86,58,0.08);
  border-color: #C4563A;
  color: #C4563A;
}
.oa-submit-row {
  margin-top: 1rem;
  display: flex;
  align-items: center;
  gap: 1rem;
}
.oa-submit-btn {
  font-family: var(--sans);
  font-size: 0.8rem;
  font-weight: 600;
  padding: 0.55rem 1.25rem;
  background: var(--ink);
  color: #fff;
  border: none;
  cursor: pointer;
  transition: all 0.15s;
  border-radius: var(--radius-md);
}
.oa-submit-btn:hover { background: #27272A; }
.oa-submit-btn:disabled {
  background: var(--border);
  color: var(--muted);
  cursor: default;
}
.oa-submit-hint {
  font-size: 0.75rem;
  color: var(--muted);
}

/* ── Footer ── */
.session-footer {
  text-align: center;
  color: var(--muted);
  font-family: var(--mono);
  font-size: 0.6rem;
  letter-spacing: 0.06em;
  text-transform: uppercase;
  margin-top: 4rem;
  padding: 1.5rem 0 2.5rem;
  border-top: 1px solid var(--border);
}
.session-footer span { color: var(--ink); font-weight: 600; }

/* ── Responsive ── */
@media (max-width: 600px) {
  body { font-size: 16px; }
  .session-hero h1 { font-size: 1.75rem; }
  .step-number { width: 36px; height: 36px; font-size: 0.8rem; }
  .session-container { padding: 0 1.15rem 3rem; }
  .hero-meta { flex-wrap: wrap; gap: 0.75rem; }
  .agent-interaction { margin-left: 0; margin-right: 0; }
}
@media (prefers-reduced-motion: reduce) {
  * { transition: none !important; }
}
</style>
</head>
<body class="track-image-gen">
  <div class="session-container">
    <a href="../index.html" class="back-link">&larr; Back to calendar</a>
    
    <div class="session-hero">
      <div class="hero-tag">Workshop</div>
      <h1>Teaching Cameras to Think</h1>
      <div class="hero-subtitle">Stream just open-sourced Vision Agents — a framework that lets you chain fast detection models like YOLO with reasoning LLMs like Gemini to build AI that watches, listens, and understands live video. Let&#x27;s build one.</div>
      <div class="hero-meta">
        <span>40 min</span>
        <span> <span class="tag">vision-agents</span> <span class="tag">real-time-video</span> <span class="tag">YOLO</span> <span class="tag">Gemini</span> <span class="tag">multimodal</span> <span class="tag">open-source</span></span>
      </div>
    </div>
    <div class="context-block">
      <h2>What's happening</h2>
      <h2>So what just happened?</h2>

<p>Stream — the company you probably know for their chat and activity feed APIs — just dropped something genuinely cool into the open-source world. It&#x27;s called <strong>Vision Agents</strong>, and it solves a problem that&#x27;s been bugging developers for a while.</p>

<p>Here&#x27;s the thing: we&#x27;ve had great vision models for years. YOLO can detect objects at ridiculous speeds. Gemini can reason about images like a human. But <strong>wiring them together for real-time video?</strong> That&#x27;s been a nightmare of WebRTC configs, frame buffer management, and latency optimization that makes most developers give up before lunch.</p>

<p>Vision Agents gives you a clean abstraction: you define an <strong>Agent</strong> with an LLM brain (Gemini, OpenAI, Claude), bolt on <strong>Processors</strong> that do fast computer vision work (YOLO detection, pose estimation, face recognition), and the framework handles the brutal plumbing of getting video frames through the pipeline at sub-30ms latency.</p>

<p>The examples they shipped are wild — a golf coach that watches your swing with YOLO pose detection and has Gemini critique your form in real-time, and a security camera that recognizes faces, tracks packages, and auto-generates wanted posters when someone steals your delivery. Yeah.</p>

<p>Let&#x27;s dig into how this works and get your AI agent to help you build something with it.</p>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">1</div>
        <h2>The Mental Model: Why Two Brains Are Better Than One</h2>
      </div>
      <div class="step-body">
        <p>Before we touch any code, let&#x27;s understand the architecture — because honestly, this is the part that clicked for me and changed how I think about video AI.</p>

<p>Imagine you&#x27;re watching a basketball game. Your <strong>eyes</strong> are incredibly fast — you can track the ball, see players moving, notice someone&#x27;s about to shoot. That&#x27;s your fast brain. But your <strong>reasoning</strong> — understanding strategy, predicting plays, knowing that was a terrible pass — that&#x27;s your slow brain.</p>

<p>Vision Agents works exactly like this:</p>

<ul>
<li><strong>Processors</strong> (YOLO, Roboflow, custom models) = the fast brain. They run on every frame, detecting objects, estimating poses, recognizing faces. They&#x27;re fast but dumb — they see, but they don&#x27;t <em>understand</em>.</li>
<li><strong>LLM</strong> (Gemini Realtime, OpenAI) = the slow brain. It gets the processed frames plus all the detection metadata and actually <em>reasons</em> about what&#x27;s happening. It can talk, answer questions, give coaching advice.</li>
</ul>

<p>The magic is that these run as a <strong>chain</strong>. Processors annotate each frame (&quot;there&#x27;s a person in golf pose, left arm at 45 degrees&quot;), and those annotations get passed to the LLM along with the video. The LLM doesn&#x27;t have to figure out where the person is — it just has to think about what the pose <em>means</em>.</p>

<p>This is why it works in real-time. You&#x27;re not asking a massive LLM to do pixel-level detection. You&#x27;re letting cheap, fast models do the grunt work and reserving the expensive reasoning for... actual reasoning.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain the processor-chain architecture and help you understand what happens to a single video frame as it moves through the pipeline.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What would you ask to understand the journey of one frame from camera to AI response?</li><li>How would you ask about the latency budget — where does time get spent?</li><li>What&#x27;s the difference between processing every frame vs. sampling at a lower FPS?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should walk you through the lifecycle: a frame enters from the video stream, gets routed to each Processor in order (YOLO runs detection, adds bounding boxes and keypoints as metadata), then the annotated frame gets queued for the LLM at whatever FPS you configured (e.g., 10fps for Gemini). The key insight it should explain is that processors run at full frame rate but the LLM only sees sampled frames — that&#x27;s how you keep costs down while maintaining smooth detection overlays.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The FPS setting on the LLM is crucial. Gemini at 10fps is reasonable. OpenAI at 10fps will bankrupt you. The examples default OpenAI to 1fps for a reason — LLM inference per frame isn&#x27;t free.
      </div>
        
      <details class="reveal">
        <summary>Why not just send raw video to Gemini?</summary>
        <div class="reveal-body"><p>You <em>could</em> — Gemini Realtime supports raw video input. But think about what happens: the LLM has to do ALL the work. Finding the golf club, estimating the body pose, tracking the ball, AND reasoning about technique. That&#x27;s like asking a philosophy professor to also be the cameraman and the instant replay system. By offloading detection to YOLO (which can process a frame in ~5ms), you free the LLM to focus on what it&#x27;s actually good at: understanding context and generating natural language responses. Plus, YOLO&#x27;s detections become structured data the LLM can reference precisely — &quot;left elbow angle: 87°&quot; is way more useful than the LLM trying to eyeball it from pixels.</p></div>
      </details>
      </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">2</div>
        <h2>Scaffolding Your First Vision Agent</h2>
      </div>
      <div class="step-body">
        <p>Alright, let&#x27;s get practical. We&#x27;re going to have our AI agent scaffold a basic Vision Agent setup — think of it as the &quot;hello world&quot; of real-time video AI.</p>

<p>The core concept is dead simple. A Vision Agent has four main pieces:</p>

<ol>
<li><strong>Edge</strong> — the video transport layer (Stream&#x27;s network, or your own)</li>
<li><strong>LLM</strong> — the reasoning brain (Gemini Realtime, OpenAI, etc.)</li>
<li><strong>Processors</strong> — the fast vision models (YOLO, face recognition, etc.)</li>
<li><strong>Instructions</strong> — what the agent should actually <em>do</em> with what it sees</li>
</ol>

<p>That&#x27;s it. The framework handles connecting to video calls, routing frames, managing audio/video sync, and all the WebRTC pain you&#x27;d normally deal with.</p>

<p>Let&#x27;s get your agent to set this up for us.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to create a minimal Vision Agent that uses YOLO for object detection and Gemini Realtime for reasoning — a simple &#x27;describe what you see&#x27; agent.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What dependencies does this project need? Think about the vision-agents package itself plus any model weights.</li><li>What API keys and credentials will be required? (Hint: you&#x27;ll need Stream AND an LLM provider)</li><li>How should you describe the agent&#x27;s personality and instructions?</li><li>What FPS makes sense for a first experiment — fast enough to be useful, cheap enough to not drain your wallet?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you a short Python script — really just 10-15 lines of meaningful code. The key parts are: creating a Stream Edge connection, setting up Gemini Realtime at a conservative FPS (like 2-5fps), adding a YOLO processor for general object detection, and pointing instructions to a markdown file that tells the agent how to behave. It should also mention you need <code>uv add vision-agents</code> and API keys for both Stream (STREAM_API_KEY, STREAM_API_SECRET) and Google (GEMINI_API_KEY). The whole thing fits in a single file.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-api-key-note">
        <div class="callout-label">API Key Note</div>
        You&#x27;ll need API keys from two services: <strong>Stream</strong> (getstream.io — free tier available) for the video transport, and either <strong>Google AI Studio</strong> (for Gemini) or <strong>OpenAI</strong> for the LLM. The Vision Agents README walks through getting both.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The <code>instructions</code> field supports markdown file references with the @ syntax — like <code>&quot;Read @my_instructions.md&quot;</code>. This is way better than cramming a huge system prompt into your Python code. Keep your agent&#x27;s personality and rules in a separate file.
      </div>
        
      <details class="reveal">
        <summary>What&#x27;s this &#x27;Edge&#x27; thing really doing?</summary>
        <div class="reveal-body"><p>The Edge is probably the most underappreciated piece here. Stream operates a global edge network — basically servers spread across the world that handle video routing. When your agent joins a video call, the Edge handles:</p>

<ul>
<li><strong>WebRTC negotiation</strong> — the handshake that establishes peer-to-peer-ish video connections</li>
<li><strong>Frame routing</strong> — getting video frames from the user&#x27;s camera to your processors with minimal hops</li>
<li><strong>Audio sync</strong> — making sure the LLM&#x27;s voice response lines up with the video</li>
<li><strong>Reconnection</strong> — handling network blips without dropping the session</li>
</ul>

<p>Stream claims 500ms join time and sub-30ms ongoing latency. That&#x27;s fast enough for real-time coaching. The framework is designed to work with other edge networks too, but Stream&#x27;s is the default and the most battle-tested.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>At this point you should understand the two-brain architecture (fast processors + reasoning LLM), know the four building blocks of a Vision Agent (Edge, LLM, Processors, Instructions), and have your agent ready to scaffold a basic setup. If the processor chain concept isn&#x27;t clicking yet, re-read Step 1&#x27;s reveal — it&#x27;s the key mental model for everything that follows.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">3</div>
        <h2>Building the Golf Coach: YOLO Pose + Gemini Analysis</h2>
      </div>
      <div class="step-body">
        <p>Now let&#x27;s build something actually impressive. The golf coach example from Stream&#x27;s repo is a perfect case study because it shows the processor chain in action beautifully.</p>

<p>Here&#x27;s what the golf coach does:</p>
<ul>
<li><strong>YOLO Pose Estimation</strong> watches the video feed, tracking 17 keypoints on the golfer&#x27;s body in real-time (shoulders, elbows, wrists, hips, knees, ankles)</li>
<li><strong>Gemini Realtime</strong> receives both the video AND the pose data, then provides spoken coaching advice — &quot;Your backswing is too steep, try keeping your left arm straighter&quot;</li>
</ul>

<p>The cool part? The pose processor doesn&#x27;t just draw skeleton overlays (though it does that too). It calculates actual joint angles and body positions as structured data. So Gemini doesn&#x27;t have to guess whether your elbow is bent — it gets <code>left_elbow_angle: 142°</code> as hard data.</p>

<p>This is the processor chain pattern at its best: YOLO does the spatial math at 30fps, Gemini does the coaching at 10fps. Each model does what it&#x27;s good at.</p>

<p>Let&#x27;s get our agent to build this out.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to build a golf coaching Vision Agent that uses YOLO pose estimation and Gemini Realtime, with a coaching instruction file.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What specific YOLO model do you need? (Hint: there are different YOLO variants for detection vs. pose estimation)</li><li>What should the instruction markdown file tell Gemini about how to coach? Think about tone, what to watch for, when to speak vs. stay quiet.</li><li>Should the LLM speak continuously or only when it notices something worth correcting?</li><li>What device should YOLO run on — and what happens if the user doesn&#x27;t have a GPU?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you two things: the Python agent setup (very short — Agent with a YOLOPoseProcessor using &#x27;yolo11n-pose.pt&#x27; and Gemini Realtime at ~10fps), and a coaching instructions markdown file. The instructions file is where the real magic is — it should tell Gemini to act as a golf coach, explain what pose keypoints it&#x27;ll receive, describe common swing faults to watch for (like chicken wing elbow, early extension, over-the-top swing path), and specify that it should only speak up when it notices an issue rather than narrating constantly. The agent should mention that &#x27;cuda&#x27; is ideal for the device but &#x27;cpu&#x27; works too (just slower).</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-warning">
        <div class="callout-label">Warning</div>
        The YOLO pose model (&#x27;yolo11n-pose.pt&#x27;) is the nano version — fast but less accurate. For production coaching, you might want &#x27;yolo11m-pose.pt&#x27; (medium) or &#x27;yolo11l-pose.pt&#x27; (large). But start with nano — it&#x27;ll run on a laptop CPU and is good enough to prove the concept.
      </div>
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        The instructions file is secretly the most important part of this whole setup. A mediocre instruction file with great models will give you mediocre coaching. A great instruction file with decent models will feel magical. Spend time on your prompts — they&#x27;re doing heavy lifting.
      </div>
        
      <details class="reveal">
        <summary>How does YOLO pose estimation actually work?</summary>
        <div class="reveal-body"><p>YOLO (You Only Look Once) is a family of models that detect objects in a single pass through the network — no multi-stage pipeline needed. The pose variant adds a second output head that predicts keypoint locations for detected people.</p>

<p>The 17 keypoints follow the COCO format: nose, left/right eye, left/right ear, left/right shoulder, left/right elbow, left/right wrist, left/right hip, left/right knee, left/right ankle. From these points, you can calculate any angle or relative position you need.</p>

<p>The &#x27;n&#x27; in &#x27;yolo11n-pose.pt&#x27; stands for nano — the smallest variant. Here&#x27;s the tradeoff ladder:</p>
<ul>
<li><strong>Nano (n)</strong>: ~3ms/frame on GPU, good enough for demos</li>
<li><strong>Small (s)</strong>: ~5ms/frame, noticeably better accuracy</li>
<li><strong>Medium (m)</strong>: ~8ms/frame, solid for production</li>
<li><strong>Large (l)</strong>: ~12ms/frame, best accuracy</li>
<li><strong>Extra-large (x)</strong>: ~15ms/frame, diminishing returns</li>
</ul>

<p>All of these are fast enough for real-time video. The question is whether you need to detect subtle joint angle differences (use medium+) or just general body position (nano is fine).</p></div>
      </details>
      <details class="reveal">
        <summary>What makes a great coaching instruction file?</summary>
        <div class="reveal-body"><p>Think about what a real golf coach does. They don&#x27;t narrate everything — &quot;I see you standing there... now you&#x27;re gripping the club...&quot; That would be maddening. A good coach:</p>

<ol>
<li><strong>Watches silently</strong> until they see something worth mentioning</li>
<li><strong>Gives specific, actionable feedback</strong> — &quot;keep your left arm straight through impact&quot; not &quot;your form needs work&quot;</li>
<li><strong>Explains the why</strong> — &quot;that chicken wing is causing your slice because...&quot;</li>
<li><strong>Encourages</strong> — &quot;nice improvement on that one!&quot;</li>
<li><strong>Remembers context</strong> — &quot;that&#x27;s the same issue we talked about two swings ago&quot;</li>
</ol>

<p>Your instruction file should encode all of this. Tell Gemini what keypoint data it&#x27;ll receive, what common faults look like in the data (e.g., left elbow angle &lt; 160° at impact = chicken wing), when to speak vs. stay quiet, and what tone to use. The more specific you are, the better the coaching feels.</p></div>
      </details>
      </div>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You&#x27;re building a vision agent for a physical therapy app that monitors patients doing rehab exercises. The agent needs to count reps, check form, and alert if a patient is doing a movement dangerously wrong. Which architecture makes more sense?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt0">
          <label for="decision_6_opt0">YOLO Pose processor at high FPS + Gemini at low FPS (2-3fps) for form analysis</label>
          <div class="decision-feedback correct">&#10003; Correct! Exactly right. Rep counting and danger detection need to happen on EVERY frame — that&#x27;s a job for the fast processor. YOLO Pose at 30fps catches the moment a knee goes past safe angles. Gemini at 2-3fps handles the higher-level coaching: explaining what&#x27;s wrong, suggesting corrections, encouraging the patient. You wouldn&#x27;t want Gemini processing 30 frames per second just to count reps — that&#x27;s expensive and slow. Let the fast brain do fast things, the smart brain do smart things.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt1">
          <label for="decision_6_opt1">Skip YOLO, just send video frames directly to Gemini at 10fps — it can handle everything</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Gemini CAN do pose estimation from raw video, but it&#x27;s a bad fit here. First, at 10fps you might miss the exact frame where a patient&#x27;s knee goes dangerously past safe range — that&#x27;s a 100ms gap. Second, Gemini&#x27;s pose angle estimates from pixels aren&#x27;t as precise as YOLO&#x27;s keypoint detection. Third, you&#x27;re paying for LLM inference on every frame for work that a lightweight model does better and faster. For safety-critical applications especially, you want the reliable, fast processor doing the detection and the LLM adding the reasoning layer on top.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_6" id="decision_6_opt2">
          <label for="decision_6_opt2">Two separate agents — one YOLO-only for detection, one Gemini-only for coaching — communicating via API</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. This technically works but you&#x27;re reinventing what Vision Agents already gives you. The processor chain IS two models communicating — but through a well-optimized frame pipeline instead of API calls. Building this as two separate services adds network latency, state synchronization headaches, and deployment complexity. The whole point of the framework is that you DON&#x27;T have to build the plumbing between your fast models and your reasoning models.</div>
        </div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">4</div>
        <h2>The Security Camera: Multi-Processor Chains and Custom Logic</h2>
      </div>
      <div class="step-body">
        <p>Let&#x27;s level up. The security camera example is where Vision Agents gets really interesting, because it chains <strong>multiple processors</strong> together and adds custom business logic.</p>

<p>Here&#x27;s the pipeline:</p>
<ol>
<li><strong>Face Recognition processor</strong> — identifies known vs. unknown people</li>
<li><strong>YOLO Object Detection</strong> — with a custom model trained to detect packages specifically</li>
<li><strong>Custom SecurityCameraProcessor</strong> — combines the outputs, tracks package state (&quot;package on porch&quot; → &quot;package picked up by unknown person&quot; → &quot;THEFT ALERT&quot;)</li>
<li><strong>Gemini LLM</strong> — generates natural language alerts, answers questions about what happened, and yes — auto-generates wanted posters</li>
</ol>

<p>This is the key insight: <strong>processors can be custom Python classes</strong>, not just off-the-shelf models. You can write a processor that takes YOLO&#x27;s detections, applies your business rules, and passes enriched metadata to the LLM.</p>

<p>Think of processors like middleware in a web framework. Each one transforms the data a little, adding its own intelligence, before the next one picks it up.</p>

<p>The security example also introduces two new components:</p>
<ul>
<li><strong>TTS (Text-to-Speech)</strong> via ElevenLabs — the agent can speak alerts out loud</li>
<li><strong>STT (Speech-to-Text)</strong> via Deepgram — you can ask the agent questions by voice</li>
</ul>

<p>So now you&#x27;ve got an agent that watches, detects, reasons, speaks, and listens. That&#x27;s a full multi-modal loop.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to explain how to build a custom processor class and how it fits into the processor chain. Don&#x27;t worry about the full security system — focus on understanding the processor interface.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>What methods does a custom processor need to implement? Think about what the framework needs to call.</li><li>How does a processor receive frames and pass enriched data to the next processor (or the LLM)?</li><li>What state might a security processor need to track between frames? (Think about temporal reasoning — things that change over time)</li><li>How would you handle the FPS setting differently for face recognition vs. object detection?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should explain that a custom processor is a Python class that implements a process method — it receives a video frame and returns annotations/metadata. The key pieces are: the processor gets each frame, can maintain internal state (like a dictionary tracking which packages are on the porch and when they arrived), and returns structured data that gets passed along the chain. For the security case, the processor would track package positions over time, correlate them with face detections from a previous processor, and flag a &#x27;theft event&#x27; when a package disappears while an unknown face is present. The agent should note that each processor has its own FPS setting, so face recognition might run at 5fps while object detection runs at 15fps.</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Custom processors are where domain expertise lives. The YOLO model doesn&#x27;t know that a package disappearing when a stranger is present means theft — that&#x27;s YOUR logic, encoded in YOUR processor. This is the extensibility point that makes Vision Agents more than just a YOLO+LLM wrapper.
      </div>
        
      <details class="reveal">
        <summary>The full multi-modal loop explained</summary>
        <div class="reveal-body"><p>The security camera example has one of the richest multi-modal loops I&#x27;ve seen in an open-source project:</p>

<p><strong>Input streams:</strong></p>
<ul>
<li>Video → face recognition → object detection → custom logic → Gemini</li>
<li>Audio → Deepgram STT → text → Gemini</li>
</ul>

<p><strong>Output streams:</strong></p>
<ul>
<li>Gemini text response → ElevenLabs TTS → speaker</li>
<li>Gemini text response → UI overlay</li>
<li>Custom processor alerts → notification system (X/Twitter posts, etc.)</li>
</ul>

<p>The beautiful part is that Gemini has access to ALL of this context simultaneously. When you ask &quot;what happened in the last 5 minutes?&quot;, it knows because the processors have been feeding it structured event data the whole time. It&#x27;s not re-analyzing stored video — it&#x27;s querying its own running context window.</p>

<p>This is a fundamentally different paradigm from traditional security cameras where you record everything and analyze later. Here, understanding happens in real-time, and the video is just one of several input modalities.</p></div>
      </details>
      <details class="reveal">
        <summary>Why ElevenLabs + Deepgram instead of built-in TTS/STT?</summary>
        <div class="reveal-body"><p>Gemini and OpenAI both have built-in voice capabilities, so why use separate TTS and STT services? A few reasons:</p>

<ol>
<li><strong>Quality</strong> — ElevenLabs is still the gold standard for natural-sounding TTS. Gemini&#x27;s built-in voice is good but ElevenLabs is <em>great</em>.</li>
<li><strong>Latency</strong> — Deepgram&#x27;s STT is incredibly fast (often under 100ms). Running STT through the LLM adds the full LLM inference latency to every speech recognition result.</li>
<li><strong>Flexibility</strong> — You can swap providers without changing your LLM. Move from Gemini to OpenAI? Your TTS and STT stay the same.</li>
<li><strong>Cost</strong> — Dedicated STT/TTS services are usually cheaper per minute than burning LLM tokens on speech processing.</li>
</ol>

<p>That said, for simpler use cases (like the invisible assistant example), the built-in Gemini Realtime voice works fine and is fewer services to manage.</p></div>
      </details>
      </div>
    </div>
    <div class="checkpoint">
      <div class="checkpoint-icon">&#10003;</div>
      <div>Nice — you now understand multi-processor chains, custom processor logic, and the full multi-modal loop (video in → detection → reasoning → voice out → voice in). You&#x27;ve seen two complete examples: the golf coach (single processor + LLM) and the security system (multi-processor + TTS + STT). Time to build your own.</div>
    </div>
    <div class="step-section">
      <div class="step-header">
        <div class="step-number">5</div>
        <h2>Designing Your Own Vision Agent: The Thinking Framework</h2>
      </div>
      <div class="step-body">
        <p>Here&#x27;s where it gets fun. Let&#x27;s use what we&#x27;ve learned to design a custom Vision Agent from scratch. But instead of just coding, let&#x27;s think through the <strong>design decisions</strong> first — because honestly, the architecture choices matter way more than the code.</p>

<p>Whenever you&#x27;re designing a Vision Agent, ask yourself these four questions:</p>

<p><strong>1. What needs to be FAST? (→ Processor)</strong></p>
<p>Anything that needs to happen on every frame or at high frequency. Object detection, pose estimation, face recognition, motion detection, OCR. If missing one frame could mean missing the event, it&#x27;s a processor.</p>

<p><strong>2. What needs to be SMART? (→ LLM)</strong></p>
<p>Anything requiring reasoning, context, natural language, or judgment. Coaching advice, situational assessment, answering questions, generating alerts with context. If it needs to <em>understand</em>, it&#x27;s an LLM task.</p>

<p><strong>3. What needs to PERSIST? (→ Custom Processor State)</strong></p>
<p>Anything temporal — tracking things over time, counting events, detecting sequences (like &quot;person arrives → picks up package → leaves&quot;). This goes in custom processor state.</p>

<p><strong>4. What&#x27;s the interaction model? (→ TTS/STT/Text)</strong></p>
<p>Does the user talk to the agent? Does the agent speak? Is it text-only overlay? This determines whether you need TTS/STT services.</p>

<p>Let&#x27;s design a workout coaching agent as our example — it&#x27;s close enough to the golf coach that the concepts transfer, but different enough to exercise your design muscles.</p>
        
      <div class="agent-interaction">
        <div class="agent-goal">
          <div class="agent-goal-label">Ask your agent</div>
          Get your agent to help you design (not build!) a workout coaching Vision Agent. You want it to count reps, check form for squats/pushups/planks, and give real-time voice feedback.
        </div>
        
        <div class="agent-hints">
          <div class="agent-hints-label">Think about it</div>
          <ul><li>Which of the four design questions above maps to rep counting? To form checking? To &#x27;you&#x27;re doing great, 3 more&#x27;?</li><li>Should rep counting happen in YOLO, in a custom processor, or in the LLM? Think about what information each layer has access to.</li><li>What happens when the user switches exercises mid-set? Who detects that — and who adapts the coaching?</li><li>What&#x27;s the right FPS balance between processor and LLM for this use case?</li></ul>
        </div>
        
      <details class="reveal">
        <summary>What the agent gives back</summary>
        <div class="reveal-body"><p>The agent should give you an architecture design, not code. Something like: YOLO Pose at 15-30fps for keypoint detection → Custom WorkoutProcessor at 15fps that analyzes joint angles to detect exercise type, count reps (by tracking keypoint position cycles), and flag form issues (e.g., knees caving in on squats = knee angle deviating from hip-width) → Gemini at 2-3fps for coaching voice output. The custom processor maintains state: current exercise, rep count, form score per rep, common mistakes detected. Gemini gets this structured data and provides encouragement, corrections, and rep callouts. STT via Deepgram so the user can say &#x27;switching to pushups&#x27; or &#x27;how was my form on that set?&#x27;</p></div>
      </details>
      </div>
        
        
      <div class="callout callout-tip">
        <div class="callout-label">Tip</div>
        Notice how the LLM FPS for a workout coach (2-3fps) is much lower than for the golf coach (10fps). That&#x27;s because golf swings happen in ~1 second — you need high temporal resolution. Squats take 3-5 seconds per rep. Match your LLM sampling rate to how fast meaningful things change in your domain.
      </div>
        
      <details class="reveal">
        <summary>The hidden power: instructions as domain expertise encoding</summary>
        <div class="reveal-body"><p>Here&#x27;s something I keep coming back to. The instruction file for your workout coach is essentially a <strong>compressed personal trainer</strong>. Think about what goes in there:</p>

<ul>
<li>What correct squat form looks like in terms of keypoint angles (knees tracking over toes, hip crease below knee at bottom, neutral spine)</li>
<li>Common mistakes and how they show up in the pose data</li>
<li>When to encourage vs. correct (don&#x27;t interrupt mid-rep unless it&#x27;s dangerous)</li>
<li>How to adapt to different fitness levels based on observed range of motion</li>
<li>Exercise detection rules (how to tell a squat from a deadlift from the keypoints)</li>
</ul>

<p>This is domain knowledge, not code. And it lives in a markdown file that anyone can edit — a physical therapist, a certified trainer, or even another LLM that specializes in fitness. The barrier to improving the coaching quality isn&#x27;t engineering skill; it&#x27;s domain expertise. That&#x27;s a huge deal.</p></div>
      </details>
      </div>
    </div>
    <div class="your-turn">
      <h3>Your Turn</h3>
      <div class="your-turn-goal">Design and prompt your agent to create a real-time &#x27;meeting room intelligence&#x27; agent — it watches a conference room camera and provides useful context to remote participants.</div>
      <div class="your-turn-context">Imagine you&#x27;re the remote person on a video call. You can see the room, but you miss a lot — who just walked in, what&#x27;s written on the whiteboard, whether people are engaged or checking their phones. A Vision Agent could be your eyes in the room, providing context that video alone doesn&#x27;t capture.</div>
      
      <div class="agent-hints">
        <div class="agent-hints-label">Think about it</div>
        <ul><li>What processors would you chain together? Think about what needs fast detection (faces, objects, text) vs. what needs reasoning.</li><li>What should the agent track over time? (Hint: think about engagement patterns, who&#x27;s been speaking, whiteboard changes)</li><li>How should the agent surface information — spoken audio might be disruptive in a meeting. What alternatives are there?</li><li>What privacy considerations should your instruction file address?</li></ul>
      </div>
      
      <details class="reveal">
        <summary>See a sample prompt</summary>
        <div class="reveal-body">
          <div class="code-block">
            <span class="code-caption">One way you could prompt it</span>
            <button class="copy-btn">COPY</button>
            <pre><code>Help me design a Vision Agent for meeting room intelligence. Here&#x27;s what I want:

**Use case:** A remote meeting participant gets real-time context about what&#x27;s happening in a physical conference room.

**Processors I&#x27;m thinking:**
1. YOLO for person detection + counting (who&#x27;s in the room, when people enter/leave)
2. Face recognition for identifying known colleagues
3. OCR processor for reading whiteboard/screen content when it changes

**LLM (Gemini at 2fps):**
- Synthesizes all processor data into useful context
- Answers questions like &#x27;what&#x27;s on the whiteboard?&#x27; or &#x27;who just walked in?&#x27;
- Detects engagement (are people looking at the speaker or at phones?)

**Output: text-only overlay, no audio** (to avoid disrupting the meeting)

**Privacy rules for the instruction file:**
- Never store or transmit face data beyond the session
- Don&#x27;t comment on individuals&#x27; attention unless the remote user specifically asks
- Whiteboard OCR should only activate when the user requests it

Can you help me flesh out the architecture, write the instruction file, and identify which parts would need custom processors vs. off-the-shelf ones?</code></pre>
          </div>
        </div>
      </details>
    </div>
    <div class="decision-point">
      <h3>Quick Check</h3>
      <div class="question">You want to build a Vision Agent that monitors a kitchen and helps someone cook a recipe in real-time. The agent should identify ingredients on the counter, track cooking progress, and warn about safety issues (pot boiling over, knife too close to edge). What&#x27;s the best LLM configuration?</div>
      
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt0">
          <label for="decision_11_opt0">Gemini Realtime at 5fps — good balance of awareness and cost for a dynamic environment</label>
          <div class="decision-feedback correct">&#10003; Correct! Cooking is dynamic but not rapid — things change over seconds, not milliseconds. 5fps gives Gemini enough temporal resolution to notice a pot starting to boil over or a timer running out, while keeping costs manageable. Pair this with YOLO at 15-30fps for object detection (ingredients, utensils, hands) and a custom processor for tracking cooking state (what step are we on, what ingredients have been used). The safety warnings (boiling over, knife placement) should trigger from the YOLO processor with immediate alerts, not wait for the LLM&#x27;s next frame.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt1">
          <label for="decision_11_opt1">OpenAI Realtime at 10fps — highest quality reasoning for complex recipe steps</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. 10fps with OpenAI would be extremely expensive for a cooking session that might last 30-60 minutes. At current pricing, you&#x27;d burn through significant API costs per cooking session. OpenAI&#x27;s reasoning quality is great, but Gemini&#x27;s is more than sufficient for cooking guidance — this isn&#x27;t a task that requires PhD-level reasoning, it needs good object recognition and recipe knowledge. Also, 10fps is overkill — a pot doesn&#x27;t go from fine to disaster in 100ms.</div>
        </div>
        <div class="decision-option">
          <input type="radio" name="decision_11" id="decision_11_opt2">
          <label for="decision_11_opt2">Gemini LLM (non-realtime) at 1fps with separate TTS — cheaper and still useful</label>
          <div class="decision-feedback incorrect">&#10007; Not quite. Close, but 1fps is risky for safety monitoring. A pot can go from simmering to boiling over in 2-3 seconds, and at 1fps you might miss the transition. More importantly, using non-realtime Gemini means you lose the continuous video understanding context — each frame becomes an independent analysis rather than part of a temporal stream. For cooking, temporal context matters a lot (&#x27;the onions have been browning for 4 minutes now&#x27; requires memory of previous frames). Gemini Realtime at a moderate FPS gives you that temporal reasoning built-in.</div>
        </div>
    </div>
    <div class="recap-section">
      <h2>Recap</h2>
      <div class="recap-body"><h2>What We Built (and What We Really Learned)</h2>

<p>We explored Stream&#x27;s Vision Agents framework — but more importantly, we learned a <strong>design pattern</strong> that applies way beyond this one library.</p>

<p>The core insight is the <strong>two-brain architecture</strong>: fast, cheap models handle perception (seeing things in frames) while expensive, smart models handle cognition (understanding what those things mean). This separation of concerns is what makes real-time video AI practical today. Neither YOLO nor Gemini alone gives you a good golf coach — but together, with a well-designed processor chain, they&#x27;re remarkably effective.</p>

<p>We walked through the golf coach (single processor chain: YOLO Pose → Gemini), the security camera (multi-processor chain with custom logic, TTS, and STT), and designed our own workout coach from the ground up.</p>

<p>The biggest takeaway? <strong>The instruction file is your secret weapon.</strong> The code for a Vision Agent is practically boilerplate — a few lines to wire up the pipeline. The real intelligence lives in your processor logic and your LLM instructions. That means domain experts (trainers, therapists, security professionals) can dramatically improve these systems without touching code.</p>

<p>And the whole thing is open source. Go fork it, break it, build something weird with it.</p></div>
      <ul class="takeaways-list"><li>The two-brain architecture (fast processors + reasoning LLM) is the key pattern for real-time video AI — let each model do what it&#x27;s best at</li><li>Custom processors are where YOUR domain logic lives — they bridge the gap between generic detection and domain-specific understanding</li><li>The instruction file matters more than the model choice — a great prompt with Gemini Flash will outperform a lazy prompt with the most expensive model</li><li>FPS is a design parameter, not a setting to maximize — match it to how fast meaningful changes happen in your domain</li><li>Multi-modal loops (video in → detection → reasoning → voice out → voice in) create genuinely interactive AI experiences that feel like the future</li></ul>
      
      <div class="next-steps">
        <h3>Where to go next</h3>
        <ul><li>Clone the Vision Agents repo and run the golf coach example locally — seeing YOLO pose overlays on your own video feed is genuinely cool: github.com/GetStream/Vision-Agents</li><li>Try building the workout coach we designed — start with just YOLO Pose + Gemini, add the custom rep-counting processor after</li><li>Explore custom YOLO training with Roboflow — the security example uses a custom-trained model for package detection, and training your own takes about an hour</li><li>Check out the &#x27;invisible assistant&#x27; pattern for screen-reading AI agents — same architecture but with screen capture instead of camera input</li></ul>
      </div>
    </div>
    <div class="sources-section">
      <h3>Sources</h3>
      <ul class="sources-list"><li><a href="https://github.com/GetStream/Vision-Agents" target="_blank" rel="noopener">GetStream/Vision-Agents</a> <span class="source-name">(GitHub Trending Python)</span></li></ul>
    </div>
    <div class="other-articles">
      <h3>What else was in the news</h3>
      <p class="oa-intro">These articles were also available today. Vote to help shape future sessions.</p>
      
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">666ghj/BettaFish</div>
            <div class="oa-summary">Multi-agent public opinion analysis assistant that collects, analyzes, and predicts trends from online discourse.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_0" value="up" id="vote_0_up" class="oa-toggle" data-idx="0">
            <label for="vote_0_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_0" value="down" id="vote_0_down" class="oa-toggle" data-idx="0">
            <label for="vote_0_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">anthropics/skills</div>
            <div class="oa-summary">Anthropic&#x27;s official repository of reusable agent skills for extending Claude&#x27;s capabilities in coding workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_1" value="up" id="vote_1_up" class="oa-toggle" data-idx="1">
            <label for="vote_1_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_1" value="down" id="vote_1_down" class="oa-toggle" data-idx="1">
            <label for="vote_1_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">datawhalechina/hello-agents</div>
            <div class="oa-summary">Chinese-language tutorial teaching AI agent design and implementation from scratch, covering theory through practice.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_2" value="up" id="vote_2_up" class="oa-toggle" data-idx="2">
            <label for="vote_2_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_2" value="down" id="vote_2_down" class="oa-toggle" data-idx="2">
            <label for="vote_2_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">microsoft/markitdown</div>
            <div class="oa-summary">Python tool that converts office documents and files to Markdown, now with MCP server support.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_3" value="up" id="vote_3_up" class="oa-toggle" data-idx="3">
            <label for="vote_3_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_3" value="down" id="vote_3_down" class="oa-toggle" data-idx="3">
            <label for="vote_3_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Comfy-Org/ComfyUI</div>
            <div class="oa-summary">Node-based graphical interface for building and running diffusion model image generation workflows.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_4" value="up" id="vote_4_up" class="oa-toggle" data-idx="4">
            <label for="vote_4_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_4" value="down" id="vote_4_down" class="oa-toggle" data-idx="4">
            <label for="vote_4_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">EbookFoundation/free-programming-books</div>
            <div class="oa-summary">Curated list of free programming books and learning resources available in many languages.</div>
            <div class="oa-meta">GitHub Trending Python</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_5" value="up" id="vote_5_up" class="oa-toggle" data-idx="5">
            <label for="vote_5_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_5" value="down" id="vote_5_down" class="oa-toggle" data-idx="5">
            <label for="vote_5_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">I built a demo of what AI chat will look like when it&#x27;s “free” and ad-supported</div>
            <div class="oa-summary">Article URL: https://99helpers.com/tools/ad-supported-chat Comments URL:...</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_6" value="up" id="vote_6_up" class="oa-toggle" data-idx="6">
            <label for="vote_6_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_6" value="down" id="vote_6_down" class="oa-toggle" data-idx="6">
            <label for="vote_6_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">If AI writes code, should the session be part of the commit?</div>
            <div class="oa-summary">Tool and discussion about recording AI coding sessions as metadata alongside git commits for transparency.</div>
            <div class="oa-meta">Hacker News AI · Mar 2</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_7" value="up" id="vote_7_up" class="oa-toggle" data-idx="7">
            <label for="vote_7_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_7" value="down" id="vote_7_down" class="oa-toggle" data-idx="7">
            <label for="vote_7_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">AI Made Writing Code Easier. It Made Being an Engineer Harder</div>
            <div class="oa-summary">Argues that AI coding tools lower the coding bar but raise the bar for engineering judgment and design.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_8" value="up" id="vote_8_up" class="oa-toggle" data-idx="8">
            <label for="vote_8_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_8" value="down" id="vote_8_down" class="oa-toggle" data-idx="8">
            <label for="vote_8_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
        <div class="other-article-card">
          <div class="oa-info">
            <div class="oa-title">Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster</div>
            <div class="oa-summary">AMD demonstrates running a trillion-parameter language model locally using a cluster of Ryzen AI Max+ systems.</div>
            <div class="oa-meta">Hacker News AI · Mar 1</div>
          </div>
          <div class="oa-votes">
            <input type="radio" name="vote_9" value="up" id="vote_9_up" class="oa-toggle" data-idx="9">
            <label for="vote_9_up" class="oa-toggle-label vote-up" title="More like this">&#x25B2;</label>
            <input type="radio" name="vote_9" value="down" id="vote_9_down" class="oa-toggle" data-idx="9">
            <label for="vote_9_down" class="oa-toggle-label vote-down" title="Not interested">&#x25BC;</label>
          </div>
        </div>
      <div class="oa-submit-row">
        <button id="oa-submit" class="oa-submit-btn" disabled>Submit votes</button>
        <span id="oa-hint" class="oa-submit-hint">Select at least one vote</span>
      </div>
    </div>
    <script>
    (function() {
      var articles = [{"title": "666ghj/BettaFish", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "anthropics/skills", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "datawhalechina/hello-agents", "tags": "agents", "source": "GitHub Trending Python"}, {"title": "microsoft/markitdown", "tags": "tools", "source": "GitHub Trending Python"}, {"title": "Comfy-Org/ComfyUI", "tags": "tools,vision", "source": "GitHub Trending Python"}, {"title": "EbookFoundation/free-programming-books", "tags": "coding", "source": "GitHub Trending Python"}, {"title": "I built a demo of what AI chat will look like when it's “free” and ad-supported", "tags": "tools", "source": "Hacker News AI"}, {"title": "If AI writes code, should the session be part of the commit?", "tags": "coding", "source": "Hacker News AI"}, {"title": "AI Made Writing Code Easier. It Made Being an Engineer Harder", "tags": "coding", "source": "Hacker News AI"}, {"title": "Running a One Trillion-Parameter LLM Locally on AMD Ryzen AI Max+ Cluster", "tags": "coding", "source": "Hacker News AI"}];
      var repo = "coldbrewnosugar/ai-course";
      var track = "image-gen";
      var date = "2026-03-02";

      var toggles = document.querySelectorAll('.oa-toggle');
      var btn = document.getElementById('oa-submit');
      var hint = document.getElementById('oa-hint');

      function updateBtn() {
        var any = false;
        toggles.forEach(function(t) { if (t.checked) any = true; });
        btn.disabled = !any;
        hint.textContent = any ? '' : 'Select at least one vote';
      }
      toggles.forEach(function(t) { t.addEventListener('change', updateBtn); });

      btn.addEventListener('click', function() {
        var lines = [];
        for (var i = 0; i < articles.length; i++) {
          var up = document.getElementById('vote_' + i + '_up');
          var down = document.getElementById('vote_' + i + '_down');
          var vote = '';
          if (up && up.checked) vote = 'up';
          if (down && down.checked) vote = 'down';
          if (vote) {
            lines.push(vote + ' | ' + articles[i].title + ' | tags:' + articles[i].tags + ' | source:' + articles[i].source);
          }
        }
        if (lines.length === 0) return;

        var body = 'track:' + track + '\ndate:' + date + '\n\n' + lines.join('\n');
        var title = 'Votes from ' + date + ' (' + track + ')';
        var url = 'https://github.com/' + repo + '/issues/new?labels=vote&title=' +
          encodeURIComponent(title) + '&body=' + encodeURIComponent(body);
        window.open(url, '_blank');
      });
    })();</script>
    <footer class="session-footer">
      <span>Tinker</span> &middot; Build with AI, daily
    </footer>
  </div>
  <script>
document.addEventListener('DOMContentLoaded', function() {
  // Copy-to-clipboard
  document.querySelectorAll('.copy-btn').forEach(function(btn) {
    btn.addEventListener('click', function() {
      var code = btn.closest('.code-block').querySelector('code').textContent;
      navigator.clipboard.writeText(code).then(function() {
        btn.textContent = 'COPIED';
        btn.classList.add('copied');
        setTimeout(function() {
          btn.textContent = 'COPY';
          btn.classList.remove('copied');
        }, 2000);
      });
    });
  });
});
</script>
</body>
</html>